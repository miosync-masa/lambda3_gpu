"""
GPU Memory Management System
~~~~~~~~~~~~~~~~~~~~~~~~~~~

GPUãƒ¡ãƒ¢ãƒªã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã ã‚ˆã€œï¼ğŸ’•
å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚‚ãƒãƒƒãƒå‡¦ç†ã§å‡¦ç†ã§ãã¡ã‚ƒã†ï¼

by ç’°ã¡ã‚ƒã‚“
"""

import numpy as np
import logging
import gc
import warnings
from typing import Dict, List, Tuple, Optional, Union, Any, Callable, TYPE_CHECKING
from dataclasses import dataclass
from contextlib import contextmanager
import psutil
import os

# ===============================
# Imports and Setup
# ===============================

# GPU imports
try:
    import cupy as cp
    HAS_GPU = True
    GPU_AVAILABLE = cp.cuda.is_available()
except ImportError:
    HAS_GPU = False
    GPU_AVAILABLE = False
    cp = None

# Type definitions
if TYPE_CHECKING:
    from numpy.typing import NDArray as NPArray
    if HAS_GPU:
        from cupy.typing import NDArray as CPArray
        NDArray = Union[NPArray, CPArray]
    else:
        NDArray = NPArray
else:
    # Runtime type definition
    NDArray = Union[np.ndarray, "cp.ndarray"] if HAS_GPU else np.ndarray

# Logger setup - ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§å®šç¾©
logger = logging.getLogger('lambda3_gpu.core.memory')

# ===============================
# Data Classes
# ===============================

@dataclass
class MemoryInfo:
    """ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’ä¿æŒã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    total: int  # ç·ãƒ¡ãƒ¢ãƒªï¼ˆãƒã‚¤ãƒˆï¼‰
    used: int   # ä½¿ç”¨ä¸­ãƒ¡ãƒ¢ãƒª
    free: int   # ç©ºããƒ¡ãƒ¢ãƒª
    
    @property
    def used_gb(self) -> float:
        """ä½¿ç”¨ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰"""
        return self.used / 1024**3
    
    @property
    def free_gb(self) -> float:
        """ç©ºããƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰"""
        return self.free / 1024**3
    
    @property
    def total_gb(self) -> float:
        """ç·ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰"""
        return self.total / 1024**3
    
    @property
    def usage_percent(self) -> float:
        """ä½¿ç”¨ç‡ï¼ˆ%ï¼‰"""
        return (self.used / self.total) * 100 if self.total > 0 else 0

# ===============================
# Exceptions
# ===============================

class MemoryError(Exception):
    """ãƒ¡ãƒ¢ãƒªé–¢é€£ã‚¨ãƒ©ãƒ¼"""
    pass

class GPUMemoryError(MemoryError):
    """GPU ãƒ¡ãƒ¢ãƒªé–¢é€£ã‚¨ãƒ©ãƒ¼"""
    pass

# ===============================
# GPU Memory Manager
# ===============================

class GPUMemoryManager:
    """
    GPUãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¯ãƒ©ã‚¹
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¿½è·¡ã¨æœ€é©åŒ–ã‚’è¡Œã†ã‚ˆã€œï¼
    """
    
    def __init__(self, 
                 max_memory_gb: Optional[float] = None,
                 reserve_percent: float = 10.0,
                 enable_pooling: bool = True):
        """
        Parameters
        ----------
        max_memory_gb : float, optional
            æœ€å¤§ä½¿ç”¨ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ã€‚Noneãªã‚‰åˆ©ç”¨å¯èƒ½ãªå…¨ãƒ¡ãƒ¢ãƒª
        reserve_percent : float, default=10.0
            äºˆç´„ã—ã¦ãŠããƒ¡ãƒ¢ãƒªã®å‰²åˆï¼ˆ%ï¼‰
        enable_pooling : bool, default=True
            ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
        """
        self.max_memory_gb = max_memory_gb
        self.reserve_percent = reserve_percent
        self.enable_pooling = enable_pooling
        self._allocations: Dict[str, int] = {}
        
        # Initialize device
        self._initialize_device()
    
    def _initialize_device(self):
        """ãƒ‡ãƒã‚¤ã‚¹ã®åˆæœŸåŒ–"""
        if HAS_GPU and GPU_AVAILABLE:
            self._setup_gpu_memory()
        else:
            self._setup_cpu_memory()
    
    def _setup_gpu_memory(self):
        """GPU ãƒ¡ãƒ¢ãƒªè¨­å®š"""
        self.device_type = 'gpu'
        
        try:
            # GPUæƒ…å ±å–å¾—
            free_mem, total_mem = cp.cuda.runtime.memGetInfo()
            self.total_memory = total_mem
            
            # æœ€å¤§ãƒ¡ãƒ¢ãƒªè¨­å®š
            self._set_max_memory(total_mem)
            
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š
            if self.enable_pooling:
                self._setup_memory_pool()
            
            # ãƒ­ã‚°å‡ºåŠ›
            self._log_memory_info("GPU")
            
        except Exception as e:
            logger.error(f"Failed to setup GPU memory: {e}")
            self._setup_cpu_memory()  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def _setup_cpu_memory(self):
        """CPU ãƒ¡ãƒ¢ãƒªè¨­å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        self.device_type = 'cpu'
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªæƒ…å ±
        mem = psutil.virtual_memory()
        self.total_memory = mem.total
        
        # æœ€å¤§ãƒ¡ãƒ¢ãƒªè¨­å®š
        if self.max_memory_gb is None:
            self.max_memory = int(mem.available * 0.8)
        else:
            self.max_memory = int(min(
                self.max_memory_gb * 1024**3,
                mem.available * 0.8
            ))
        
        self._log_memory_info("CPU")
    
    def _set_max_memory(self, total_mem: int):
        """æœ€å¤§ä½¿ç”¨ãƒ¡ãƒ¢ãƒªã‚’è¨­å®š"""
        if self.max_memory_gb is None:
            # äºˆç´„åˆ†ã‚’å¼•ã„ãŸåˆ†ã‚’ä½¿ç”¨
            usable_memory = total_mem * (1 - self.reserve_percent / 100)
            self.max_memory = int(usable_memory)
        else:
            self.max_memory = int(min(
                self.max_memory_gb * 1024**3,
                total_mem * (1 - self.reserve_percent / 100)
            ))
    
    def _setup_memory_pool(self):
        """ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã®è¨­å®š"""
        try:
            mempool = cp.get_default_memory_pool()
            mempool.set_limit(size=self.max_memory)
            logger.debug("Memory pool configured successfully")
        except Exception as e:
            logger.warning(f"Failed to configure memory pool: {e}")
    
    def _log_memory_info(self, device_type: str):
        """ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"{device_type} Memory initialized:")
        logger.info(f"  Total: {self.total_memory/1024**3:.1f} GB")
        logger.info(f"  Max usable: {self.max_memory/1024**3:.1f} GB")
        if device_type == "GPU":
            logger.info(f"  Reserve: {self.reserve_percent}%")
    
    def get_memory_info(self) -> MemoryInfo:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’å–å¾—"""
        if self.device_type == 'gpu' and HAS_GPU:
            try:
                free_mem, total_mem = cp.cuda.runtime.memGetInfo()
                used_mem = total_mem - free_mem
                return MemoryInfo(total=total_mem, used=used_mem, free=free_mem)
            except Exception as e:
                logger.warning(f"Failed to get GPU memory info: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # CPU ãƒ¡ãƒ¢ãƒªæƒ…å ±
        mem = psutil.virtual_memory()
        return MemoryInfo(total=mem.total, used=mem.used, free=mem.available)
    
    def allocate(self, 
                 size_bytes: int, 
                 name: Optional[str] = None,
                 dtype: type = np.float32) -> bool:
        """
        ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        Parameters
        ----------
        size_bytes : int
            å¿…è¦ãªãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
        name : str, optional
            å‰²ã‚Šå½“ã¦åï¼ˆè¿½è·¡ç”¨ï¼‰
        dtype : type
            ãƒ‡ãƒ¼ã‚¿å‹
            
        Returns
        -------
        bool
            å‰²ã‚Šå½“ã¦å¯èƒ½ãªã‚‰True
        """
        mem_info = self.get_memory_info()
        
        # æ—¢å­˜ã®å‰²ã‚Šå½“ã¦ã‚’è€ƒæ…®
        total_allocated = sum(self._allocations.values())
        available = min(mem_info.free, self.max_memory - total_allocated)
        
        if size_bytes > available:
            logger.warning(
                f"Memory allocation failed: "
                f"requested {size_bytes/1024**3:.2f} GB, "
                f"available {available/1024**3:.2f} GB"
            )
            return False
        
        if name:
            self._allocations[name] = size_bytes
            logger.debug(f"Allocated {size_bytes/1024**3:.2f} GB for '{name}'")
        
        return True
    
    def free(self, name: str):
        """ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚’è§£æ”¾"""
        if name in self._allocations:
            size = self._allocations.pop(name)
            logger.debug(f"Freed {size/1024**3:.2f} GB from '{name}'")
    
    def estimate_batch_size(self,
                           data_shape: Tuple[int, ...],
                           dtype: type = np.float32,
                           operations_multiplier: float = 3.0) -> int:
        """
        åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‹ã‚‰æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å®š
        
        Parameters
        ----------
        data_shape : tuple
            ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ï¼ˆãƒãƒƒãƒæ¬¡å…ƒã‚’å«ã‚€ï¼‰
        dtype : type
            ãƒ‡ãƒ¼ã‚¿å‹
        operations_multiplier : float
            å‡¦ç†ã«å¿…è¦ãªè¿½åŠ ãƒ¡ãƒ¢ãƒªã®å€ç‡
            
        Returns
        -------
        int
            æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        # 1è¦ç´ ã‚ãŸã‚Šã®ãƒ¡ãƒ¢ãƒª
        element_size = np.dtype(dtype).itemsize
        elements_per_batch = int(np.prod(data_shape[1:]))  # ãƒãƒƒãƒæ¬¡å…ƒä»¥å¤–
        bytes_per_batch = elements_per_batch * element_size
        
        # å‡¦ç†ç”¨ã®ä½™è£•ã‚’è€ƒæ…®
        required_per_batch = bytes_per_batch * operations_multiplier
        
        # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª
        mem_info = self.get_memory_info()
        available = min(
            mem_info.free * 0.8,  # 80%ã¾ã§ä½¿ç”¨
            self.max_memory - sum(self._allocations.values())
        )
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—
        batch_size = max(1, int(available / required_per_batch))
        
        logger.info(
            f"Estimated batch size: {batch_size} "
            f"(available: {available/1024**3:.1f} GB, "
            f"per batch: {required_per_batch/1024**3:.3f} GB)"
        )
        
        return batch_size
    
    @contextmanager
    def temporary_allocation(self, size_bytes: int, name: str = "temp"):
        """
        ä¸€æ™‚çš„ãªãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        ä½¿ã„æ–¹:
            with memory_manager.temporary_allocation(1024**3, "temp_buffer"):
                # ãƒ¡ãƒ¢ãƒªã‚’ä½¿ã†å‡¦ç†
        """
        allocated = self.allocate(size_bytes, name)
        if not allocated:
            raise MemoryError(f"Failed to allocate {size_bytes/1024**3:.2f} GB")
        
        try:
            yield
        finally:
            self.free(name)
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self.device_type == 'gpu' and HAS_GPU:
            try:
                mempool = cp.get_default_memory_pool()
                pinned_mempool = cp.get_default_pinned_memory_pool()
                mempool.free_all_blocks()
                pinned_mempool.free_all_blocks()
                logger.info("GPU cache cleared")
            except Exception as e:
                logger.warning(f"Failed to clear GPU cache: {e}")
        
        # Python GC
        gc.collect()
    
    def get_allocation_summary(self) -> str:
        """å‰²ã‚Šå½“ã¦çŠ¶æ³ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        mem_info = self.get_memory_info()
        
        summary = [
            f"\n{'='*50}",
            f"Memory Status ({self.device_type.upper()})",
            f"{'='*50}",
            f"Total Memory: {mem_info.total_gb:.1f} GB",
            f"Used Memory: {mem_info.used_gb:.1f} GB ({mem_info.usage_percent:.1f}%)",
            f"Free Memory: {mem_info.free_gb:.1f} GB",
            f"Max Allowed: {self.max_memory/1024**3:.1f} GB",
            f"\nAllocations:"
        ]
        
        if self._allocations:
            for name, size in sorted(self._allocations.items(), 
                                    key=lambda x: x[1], reverse=True):
                summary.append(f"  {name}: {size/1024**3:.2f} GB")
        else:
            summary.append("  (none)")
        
        summary.append(f"{'='*50}\n")
        
        return '\n'.join(summary)

# ===============================
# Memory Pool
# ===============================

class GPUMemoryPool:
    """
    å†åˆ©ç”¨å¯èƒ½ãªãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
    é »ç¹ãªå‰²ã‚Šå½“ã¦/è§£æ”¾ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã™ã‚‹ã‚ˆã€œï¼
    """
    
    def __init__(self, 
                 block_size: int = 1024**3,  # 1GB
                 max_blocks: int = 10):
        """
        Parameters
        ----------
        block_size : int
            ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
        max_blocks : int
            æœ€å¤§ãƒ–ãƒ­ãƒƒã‚¯æ•°
        """
        self.block_size = block_size
        self.max_blocks = max_blocks
        self._pool: List[Any] = []
        self._in_use: Dict[int, Any] = {}
        
        self.is_gpu = HAS_GPU and GPU_AVAILABLE
        self.xp = cp if self.is_gpu else np
    
    def get_block(self, size: Optional[int] = None) -> Tuple[int, Any]:
        """
        ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
        
        Returns
        -------
        block_id : int
            ãƒ–ãƒ­ãƒƒã‚¯ID
        block : array
            ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯
        """
        if size is None:
            size = self.block_size
        
        # ãƒ—ãƒ¼ãƒ«ã‹ã‚‰æ¢ã™
        for i, block in enumerate(self._pool):
            if block.size * block.itemsize >= size:
                block = self._pool.pop(i)
                block_id = id(block)
                self._in_use[block_id] = block
                logger.debug(f"Reused block {block_id} from pool")
                return block_id, block
        
        # æ–°è¦ä½œæˆ
        if len(self._pool) + len(self._in_use) < self.max_blocks:
            elements = size // np.dtype(np.float32).itemsize
            block = self.xp.empty(elements, dtype=np.float32)
            block_id = id(block)
            self._in_use[block_id] = block
            logger.debug(f"Created new block {block_id}")
            return block_id, block
        
        raise MemoryError("Memory pool exhausted")
    
    def release_block(self, block_id: int):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾ï¼ˆãƒ—ãƒ¼ãƒ«ã«æˆ»ã™ï¼‰"""
        if block_id in self._in_use:
            block = self._in_use.pop(block_id)
            if len(self._pool) < self.max_blocks:
                self._pool.append(block)
                logger.debug(f"Released block {block_id} to pool")
            else:
                logger.debug(f"Released block {block_id} (pool full)")
    
    def clear(self):
        """ãƒ—ãƒ¼ãƒ«ã‚’ã‚¯ãƒªã‚¢"""
        self._pool.clear()
        self._in_use.clear()
        logger.info("Memory pool cleared")

# ===============================
# Batch Processor
# ===============================

class BatchProcessor:
    """
    å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
    ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„ãƒ‡ãƒ¼ã‚¿ã‚‚å‡¦ç†ã§ãã‚‹ã‚ˆã€œï¼
    """
    
    def __init__(self, 
                 memory_manager: GPUMemoryManager,
                 overlap_frames: int = 0):
        """
        Parameters
        ----------
        memory_manager : GPUMemoryManager
            ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        overlap_frames : int
            ãƒãƒƒãƒé–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆå¢ƒç•Œå‡¦ç†ç”¨ï¼‰
        """
        self.memory_manager = memory_manager
        self.overlap_frames = overlap_frames
        self.is_gpu = memory_manager.device_type == 'gpu'
    
    def process_batched(self,
                       data: np.ndarray,
                       process_func: Callable,
                       axis: int = 0,
                       batch_size: Optional[int] = None,
                       dtype: type = np.float32,
                       return_type: str = 'concat',
                       progress_callback: Optional[Callable] = None,
                       **kwargs) -> Union[np.ndarray, List[np.ndarray]]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒå‡¦ç†
        
        Parameters
        ----------
        data : np.ndarray
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        process_func : callable
            å„ãƒãƒƒãƒã«é©ç”¨ã™ã‚‹é–¢æ•°
        axis : int
            ãƒãƒƒãƒåˆ†å‰²ã™ã‚‹è»¸
        batch_size : int, optional
            ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆNoneãªã‚‰è‡ªå‹•è¨ˆç®—ï¼‰
        dtype : type
            å‡¦ç†æ™‚ã®ãƒ‡ãƒ¼ã‚¿å‹
        return_type : str
            'concat': çµæœã‚’çµåˆã—ã¦è¿”ã™
            'list': ãƒãƒƒãƒã”ã¨ã®ãƒªã‚¹ãƒˆã§è¿”ã™
        progress_callback : callable, optional
            é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯(current, total)
        **kwargs
            process_funcã«æ¸¡ã™è¿½åŠ å¼•æ•°
            
        Returns
        -------
        result : array or list
            å‡¦ç†çµæœ
        """
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºæ±ºå®š
        if batch_size is None:
            batch_size = self._estimate_batch_size(data.shape, axis, dtype)
        
        # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        results = self._process_batches(
            data, process_func, axis, batch_size, dtype, 
            progress_callback, **kwargs
        )
        
        # çµæœã®è¿”å´
        return self._combine_results(results, return_type, axis)
    
    def _estimate_batch_size(self, shape: Tuple[int, ...], axis: int, dtype: type) -> int:
        """ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å®š"""
        batch_shape = list(shape)
        batch_shape[axis] = 1
        return self.memory_manager.estimate_batch_size(batch_shape, dtype=dtype)
    
    def _process_batches(self,
                        data: np.ndarray,
                        process_func: Callable,
                        axis: int,
                        batch_size: int,
                        dtype: type,
                        progress_callback: Optional[Callable],
                        **kwargs) -> List[np.ndarray]:
        """ãƒãƒƒãƒã”ã¨ã«å‡¦ç†ã‚’å®Ÿè¡Œ"""
        n_samples = data.shape[axis]
        n_batches = (n_samples + batch_size - 1) // batch_size
        
        logger.info(
            f"Batch processing: {n_samples} samples, "
            f"{n_batches} batches of size {batch_size}"
        )
        
        results = []
        
        for i in range(n_batches):
            # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—
            batch_data = self._get_batch_data(data, i, batch_size, axis, n_samples)
            
            # å‡¦ç†å®Ÿè¡Œ
            batch_result = self._process_single_batch(
                batch_data, process_func, dtype, i, **kwargs
            )
            
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—é™¤å»
            if self.overlap_frames > 0 and i < n_batches - 1:
                batch_result = self._remove_overlap(batch_result, axis)
            
            results.append(batch_result)
            
            # é€²æ—é€šçŸ¥
            if progress_callback:
                progress_callback(i + 1, n_batches)
            
            # å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if i % 10 == 0:
                self.memory_manager.clear_cache()
        
        return results
    
    def _get_batch_data(self, 
                       data: np.ndarray, 
                       batch_idx: int, 
                       batch_size: int,
                       axis: int,
                       n_samples: int) -> np.ndarray:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size + self.overlap_frames, n_samples)
        
        slices = [slice(None)] * data.ndim
        slices[axis] = slice(start_idx, end_idx)
        
        return data[tuple(slices)]
    
    def _process_single_batch(self,
                            batch_data: np.ndarray,
                            process_func: Callable,
                            dtype: type,
                            batch_idx: int,
                            **kwargs) -> np.ndarray:
        """å˜ä¸€ãƒãƒƒãƒã‚’å‡¦ç†"""
        # GPUè»¢é€ï¼ˆå¿…è¦ãªã‚‰ï¼‰
        if self.is_gpu and HAS_GPU:
            batch_data = cp.asarray(batch_data, dtype=dtype)
        
        # ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§å‡¦ç†
        with self.memory_manager.temporary_allocation(
            batch_data.nbytes, f"batch_{batch_idx}"
        ):
            batch_result = process_func(batch_data, **kwargs)
            
            # CPUè»¢é€ï¼ˆå¿…è¦ãªã‚‰ï¼‰
            if self.is_gpu and HAS_GPU and isinstance(batch_result, cp.ndarray):
                batch_result = cp.asnumpy(batch_result)
        
        return batch_result
    
    def _remove_overlap(self, 
                       batch_result: np.ndarray, 
                       axis: int) -> np.ndarray:
        """ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã‚’é™¤å»"""
        if isinstance(batch_result, np.ndarray):
            trim_slices = [slice(None)] * batch_result.ndim
            trim_slices[axis] = slice(None, -self.overlap_frames)
            return batch_result[tuple(trim_slices)]
        return batch_result
    
    def _combine_results(self,
                        results: List[np.ndarray],
                        return_type: str,
                        axis: int) -> Union[np.ndarray, List[np.ndarray]]:
        """çµæœã‚’çµåˆã¾ãŸã¯è¿”å´"""
        if return_type == 'concat' and results:
            if isinstance(results[0], np.ndarray):
                return np.concatenate(results, axis=axis)
            else:
                logger.warning("Cannot concatenate non-array results")
        
        return results

# ===============================
# Utility Functions
# ===============================

def estimate_memory_usage(shape: Tuple[int, ...], 
                         dtype: type = np.float32,
                         operations: Optional[List[str]] = None) -> Dict[str, float]:
    """
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š
    
    Parameters
    ----------
    shape : tuple
        ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
    dtype : type
        ãƒ‡ãƒ¼ã‚¿å‹
    operations : list of str, optional
        å®Ÿè¡Œã™ã‚‹æ“ä½œã®ãƒªã‚¹ãƒˆ
        
    Returns
    -------
    dict
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®šå€¤ï¼ˆGBï¼‰
    """
    # åŸºæœ¬ãƒ¡ãƒ¢ãƒª
    element_size = np.dtype(dtype).itemsize
    n_elements = int(np.prod(shape))
    base_memory = n_elements * element_size
    
    estimates = {
        'input': base_memory / 1024**3,
        'operations': 0.0,
        'output': base_memory / 1024**3,
        'total': 0.0
    }
    
    # æ“ä½œã”ã¨ã®è¿½åŠ ãƒ¡ãƒ¢ãƒª
    if operations:
        operation_multipliers = {
            'fft': 2.0,
            'conv': 3.0,
            'matmul': 2.5,
            'gradient': 1.5,
            'sort': 1.5,
            'cumsum': 1.0
        }
        
        for op in operations:
            multiplier = operation_multipliers.get(op, 1.0)
            estimates['operations'] += base_memory * multiplier / 1024**3
    
    estimates['total'] = sum(v for k, v in estimates.items() if k != 'total')
    
    return estimates

def clear_gpu_cache():
    """GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Œå…¨ã«ã‚¯ãƒªã‚¢"""
    if HAS_GPU and GPU_AVAILABLE:
        try:
            # CuPyãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            # CUDAåŒæœŸ
            cp.cuda.Stream.null.synchronize()
            
            logger.info("GPU cache cleared completely")
        except Exception as e:
            logger.warning(f"Failed to clear GPU cache: {e}")
    
    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    gc.collect()

def get_memory_summary() -> str:
    """
    ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
    """
    lines = ["\n" + "="*60, "Memory Summary", "="*60]
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª
    mem = psutil.virtual_memory()
    lines.extend([
        f"\nSystem Memory:",
        f"  Total: {mem.total/1024**3:.1f} GB",
        f"  Used: {mem.used/1024**3:.1f} GB ({mem.percent:.1f}%)",
        f"  Available: {mem.available/1024**3:.1f} GB"
    ])
    
    # GPUãƒ¡ãƒ¢ãƒª
    if HAS_GPU and GPU_AVAILABLE:
        try:
            free_mem, total_mem = cp.cuda.runtime.memGetInfo()
            used_mem = total_mem - free_mem
            lines.extend([
                f"\nGPU Memory:",
                f"  Total: {total_mem/1024**3:.1f} GB",
                f"  Used: {used_mem/1024**3:.1f} GB ({used_mem/total_mem*100:.1f}%)",
                f"  Free: {free_mem/1024**3:.1f} GB"
            ])
            
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«çŠ¶æ³
            mempool = cp.get_default_memory_pool()
            lines.extend([
                f"\nCuPy Memory Pool:",
                f"  Used blocks: {mempool.used_bytes()/1024**3:.2f} GB",
                f"  Total blocks: {mempool.total_bytes()/1024**3:.2f} GB"
            ])
        except Exception as e:
            lines.append(f"\nGPU Memory: Error getting info - {e}")
    
    lines.append("="*60 + "\n")
    
    return '\n'.join(lines)
