"""
GPU Utilities and Base Classes - Refactored Version
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

GPUã¨CPUã‚’è³¢ãåˆ‡ã‚Šæ›¿ãˆã‚‹åŸºåº•ã‚¯ãƒ©ã‚¹ã¨ä¾¿åˆ©ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é›†ã ã‚ˆã€œï¼ğŸ’•
ç’°ã¡ã‚ƒã‚“ãŒãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã€ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ã®ãƒã‚°ã‚‚ä¿®æ­£ã—ãŸã‚ˆï¼

by ç’°ã¡ã‚ƒã‚“
"""

import numpy as np
import logging
import time
from typing import Dict, List, Tuple, Optional, Union, Any, Callable, TYPE_CHECKING
from contextlib import contextmanager
from functools import wraps
import warnings

# ===============================
# GPU Availability Check
# ===============================

try:
    import cupy as cp
    from cupy import cuda
    HAS_GPU = True
    GPU_AVAILABLE = cp.cuda.is_available()
except ImportError:
    HAS_GPU = False
    GPU_AVAILABLE = False
    cp = None
    cuda = None

# ===============================
# Type Definitions
# ===============================

if TYPE_CHECKING:
    from numpy.typing import NDArray as NPArray
    if HAS_GPU:
        from cupy.typing import NDArray as CPArray
        NDArray = Union[NPArray, CPArray]
    else:
        NDArray = NPArray
else:
    NDArray = Union[np.ndarray, "cp.ndarray"] if HAS_GPU else np.ndarray

ArrayType = NDArray  # ã‚¨ã‚¤ãƒªã‚¢ã‚¹

# ===============================
# Logger Setup
# ===============================

logger = logging.getLogger('lambda3_gpu.core.utils')

# ===============================
# GPU Backend Base Class
# ===============================

class GPUBackend:
    """
    GPU/CPUè‡ªå‹•åˆ‡ã‚Šæ›¿ãˆã®åŸºåº•ã‚¯ãƒ©ã‚¹
    å…¨ã¦ã®GPUå¯¾å¿œã‚¯ãƒ©ã‚¹ã¯ã“ã‚Œã‚’ç¶™æ‰¿ã™ã‚‹ã‚ˆã€œï¼
    
    ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚‚è‡ªå‹•ã§åˆæœŸåŒ–ã•ã‚Œã‚‹ã‹ã‚‰å®‰å¿ƒï¼ğŸ’•
    """
    
    def __init__(self, 
                 device: Union[str, int] = 'auto',
                 force_cpu: bool = False,
                 mixed_precision: bool = False,
                 profile: bool = False,
                 memory_manager_config: Optional[Dict[str, Any]] = None):
        """
        Parameters
        ----------
        device : str or int, default='auto'
            'auto': è‡ªå‹•é¸æŠ
            'cpu': CPUå¼·åˆ¶
            'gpu': GPUå¼·åˆ¶
            0,1,2...: ç‰¹å®šã®GPUç•ªå·
        force_cpu : bool, default=False
            Trueæ™‚ã¯GPUãŒã‚ã£ã¦ã‚‚CPUä½¿ç”¨
        mixed_precision : bool, default=False
            FP16ä½¿ç”¨ã§é«˜é€ŸåŒ–ï¼ˆç²¾åº¦ã¯è½ã¡ã‚‹ï¼‰
        profile : bool, default=False
            ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
        memory_manager_config : dict, optional
            ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®è¨­å®š
        """
        # åŸºæœ¬è¨­å®š
        self.force_cpu = force_cpu
        self.mixed_precision = mixed_precision
        self.profile = profile
        self._timers = {}
        
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
        if force_cpu or not GPU_AVAILABLE:
            self.device = 'cpu'
            self.device_id = -1
            self.xp = np
            self.is_gpu = False
            logger.info("Using CPU backend")
        else:
            self.device_id = self._select_device(device)
            self.device = f'gpu:{self.device_id}'
            self.xp = cp
            self.is_gpu = True
            
            # GPUãƒ‡ãƒã‚¤ã‚¹ã‚’ã‚»ãƒƒãƒˆ
            cp.cuda.Device(self.device_id).use()
            logger.info(f"Using GPU backend: Device {self.device_id}")
            
            # Mixed precisionè¨­å®š
            if mixed_precision:
                self._setup_mixed_precision()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–ï¼ˆé‡è¦ï¼ï¼‰
        self._initialize_memory_manager(memory_manager_config)
        
        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._cache_device_info()
    
    def _initialize_memory_manager(self, config: Optional[Dict[str, Any]] = None):
        """ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–"""
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯å®Ÿè¡Œæ™‚ã«ï¼ˆå¾ªç’°å‚ç…§å›é¿ï¼‰
        from .gpu_memory import GPUMemoryManager
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        default_config = {
            'max_memory_gb': None,  # è‡ªå‹•
            'reserve_percent': 10.0,
            'enable_pooling': True
        }
        
        if config:
            default_config.update(config)
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆ
        self.memory_manager = GPUMemoryManager(**default_config)
        logger.debug(f"Memory manager initialized: {self.memory_manager.get_memory_info()}")
    
    def _select_device(self, device: Union[str, int]) -> int:
        """ãƒ‡ãƒã‚¤ã‚¹é¸æŠãƒ­ã‚¸ãƒƒã‚¯"""
        if device == 'auto':
            # è‡ªå‹•é¸æŠï¼ˆãƒ¡ãƒ¢ãƒªãŒä¸€ç•ªç©ºã„ã¦ã‚‹GPUï¼‰
            return auto_select_device()
        elif device == 'cpu':
            return -1
        elif device == 'gpu':
            return 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆGPU
        elif isinstance(device, int):
            # æŒ‡å®šã•ã‚ŒãŸGPUãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            n_devices = cp.cuda.runtime.getDeviceCount()
            if device >= n_devices:
                logger.warning(f"GPU {device} not found. Using GPU 0.")
                return 0
            return device
        else:
            raise ValueError(f"Invalid device: {device}")
    
    def _setup_mixed_precision(self):
        """Mixed precisionè¨­å®š"""
        if self.is_gpu and HAS_GPU:
            try:
                # TensorCoreã‚’ä½¿ã†è¨­å®š
                cp.cuda.cublas.setMathMode(cp.cuda.cublas.CUBLAS_TENSOR_OP_MATH)
                logger.info("Mixed precision (FP16) enabled")
            except Exception as e:
                logger.warning(f"Failed to enable mixed precision: {e}")
    
    def _cache_device_info(self):
        """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        self.device_info = {}
        
        if self.is_gpu and HAS_GPU:
            try:
                props = cp.cuda.runtime.getDeviceProperties(self.device_id)
                self.device_info = {
                    'name': props['name'].decode(),
                    'compute_capability': f"{props['major']}.{props['minor']}",
                    'total_memory': props['totalGlobalMem'],
                    'multiprocessor_count': props['multiProcessorCount']
                }
            except Exception as e:
                logger.warning(f"Failed to get device properties: {e}")
    
    # ===============================
    # Array Operations
    # ===============================
    
    def to_gpu(self, array: NDArray, dtype: Optional[np.dtype] = None) -> NDArray:
        """é…åˆ—ã‚’GPUã«è»¢é€ï¼ˆã¾ãŸã¯ãã®ã¾ã¾ï¼‰"""
        if self.is_gpu and HAS_GPU:
            if isinstance(array, cp.ndarray):
                return array.astype(dtype) if dtype else array
            return cp.asarray(array, dtype=dtype)
        else:
            return np.asarray(array, dtype=dtype) if dtype else np.asarray(array)
    
    def to_cpu(self, array: NDArray) -> np.ndarray:
        """é…åˆ—ã‚’CPUã«è»¢é€ï¼ˆã¾ãŸã¯ãã®ã¾ã¾ï¼‰"""
        if self.is_gpu and HAS_GPU and isinstance(array, cp.ndarray):
            return cp.asnumpy(array)
        return np.asarray(array)
    
    def zeros(self, shape: Tuple[int, ...], dtype: np.dtype = np.float32) -> NDArray:
        """ã‚¼ãƒ­é…åˆ—ã‚’ä½œæˆ"""
        return self.xp.zeros(shape, dtype=dtype)
    
    def ones(self, shape: Tuple[int, ...], dtype: np.dtype = np.float32) -> NDArray:
        """1é…åˆ—ã‚’ä½œæˆ"""
        return self.xp.ones(shape, dtype=dtype)
    
    def empty(self, shape: Tuple[int, ...], dtype: np.dtype = np.float32) -> NDArray:
        """ç©ºé…åˆ—ã‚’ä½œæˆï¼ˆé«˜é€Ÿã ã‘ã©åˆæœŸåŒ–ã•ã‚Œãªã„ï¼‰"""
        return self.xp.empty(shape, dtype=dtype)
    
    def arange(self, *args, **kwargs) -> NDArray:
        """é€£ç•ªé…åˆ—ã‚’ä½œæˆ"""
        return self.xp.arange(*args, **kwargs)
    
    # ===============================
    # Memory Management
    # ===============================
    
    def clear_memory(self):
        """ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
        if hasattr(self, 'memory_manager'):
            self.memory_manager.clear_cache()
        
        # è¿½åŠ ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if self.is_gpu and HAS_GPU:
            cp.get_default_memory_pool().free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
    
    @contextmanager
    def batch_context(self, estimated_memory: int):
        """ãƒãƒƒãƒå‡¦ç†ç”¨ã®ãƒ¡ãƒ¢ãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        if hasattr(self, 'memory_manager'):
            with self.memory_manager.batch_context(estimated_memory):
                yield
        else:
            yield
    
    # ===============================
    # Profiling
    # ===============================
    
    @contextmanager
    def timer(self, name: str):
        """ã‚¿ã‚¤ãƒãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        if self.profile:
            start = time.perf_counter()
            
            # GPUåŒæœŸï¼ˆæ­£ç¢ºãªè¨ˆæ¸¬ã®ãŸã‚ï¼‰
            if self.is_gpu and HAS_GPU:
                cp.cuda.Stream.null.synchronize()
            
            yield
            
            # GPUåŒæœŸ
            if self.is_gpu and HAS_GPU:
                cp.cuda.Stream.null.synchronize()
            
            elapsed = time.perf_counter() - start
            
            if name not in self._timers:
                self._timers[name] = []
            self._timers[name].append(elapsed)
            
            logger.debug(f"{name}: {elapsed:.3f}s")
        else:
            yield
    
    def get_profile_summary(self) -> Dict[str, Dict[str, float]]:
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœã®ã‚µãƒãƒªãƒ¼"""
        summary = {}
        for name, times in self._timers.items():
            summary[name] = {
                'total': sum(times),
                'mean': np.mean(times),
                'std': np.std(times),
                'min': min(times),
                'max': max(times),
                'count': len(times)
            }
        return summary
    
    # ===============================
    # Device Info
    # ===============================
    
    def get_device_info(self) -> Dict[str, Any]:
        """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—"""
        info = {
            'backend': 'GPU' if self.is_gpu else 'CPU',
            'device': self.device,
            'mixed_precision': self.mixed_precision
        }
        
        if self.device_info:
            info.update(self.device_info)
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚‚è¿½åŠ 
        if hasattr(self, 'memory_manager'):
            mem_info = self.memory_manager.get_memory_info()
            info['memory'] = {
                'total_gb': mem_info.total_gb,
                'used_gb': mem_info.used_gb,
                'free_gb': mem_info.free_gb
            }
        
        return info

# ===============================
# Utility Functions
# ===============================

def auto_select_device() -> int:
    """
    è‡ªå‹•çš„ã«æœ€é©ãªGPUã‚’é¸æŠ
    ãƒ¡ãƒ¢ãƒªãŒä¸€ç•ªç©ºã„ã¦ã‚‹ã‚„ã¤ã‚’é¸ã¶ã‚ˆã€œï¼
    """
    if not GPU_AVAILABLE:
        return 0
    
    n_devices = cp.cuda.runtime.getDeviceCount()
    if n_devices == 0:
        return 0
    
    best_device = 0
    max_free_memory = 0
    
    for i in range(n_devices):
        with cp.cuda.Device(i):
            free_mem, total_mem = cp.cuda.runtime.memGetInfo()
            if free_mem > max_free_memory:
                max_free_memory = free_mem
                best_device = i
                
    logger.info(f"Auto-selected GPU {best_device} with {max_free_memory/1e9:.1f}GB free")
    return best_device

def profile_gpu(func: Callable) -> Callable:
    """
    GPUé–¢æ•°ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    GPUBackendã®timerãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ©ç”¨
    """
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        # self.timerã‚’ä½¿ã†ï¼ˆGPUBackendã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        if hasattr(self, 'timer'):
            with self.timer(func.__name__):
                return func(self, *args, **kwargs)
        else:
            # timerãŒãªã„å ´åˆã¯é€šå¸¸å®Ÿè¡Œ
            return func(self, *args, **kwargs)
    
    return wrapper

def handle_gpu_errors(func: Callable) -> Callable:
    """
    GPUé–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯è‡ªå‹•ã§ãƒªãƒˆãƒ©ã‚¤ï¼
    """
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            return func(self, *args, **kwargs)
        except Exception as e:
            error_str = str(e)
            
            # GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆ
            if 'out of memory' in error_str.lower():
                logger.warning(f"GPU out of memory in {func.__name__}: {e}")
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢è©¦è¡Œ
                if hasattr(self, 'clear_memory'):
                    self.clear_memory()
                    logger.info("Cleared GPU memory, retrying...")
                    
                    # ãƒªãƒˆãƒ©ã‚¤ã¯1å›ã®ã¿
                    try:
                        return func(self, *args, **kwargs)
                    except Exception as retry_error:
                        logger.error(f"Retry failed: {retry_error}")
                        
                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ã®ææ¡ˆ
                        if hasattr(self, 'config') and hasattr(self.config, 'gpu_batch_size'):
                            suggested_size = self.config.gpu_batch_size // 2
                            logger.error(
                                f"Consider reducing batch size. "
                                f"Current: {self.config.gpu_batch_size}, "
                                f"Suggested: {suggested_size}"
                            )
                        raise
            
            # CuPy/CUDAç‰¹æœ‰ã®ã‚¨ãƒ©ãƒ¼
            elif any(keyword in error_str.lower() for keyword in ['cuda', 'cupy', 'gpu']):
                logger.error(f"GPU error in {func.__name__}: {e}")
                
                # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å‡ºåŠ›
                if hasattr(self, 'get_device_info'):
                    logger.error(f"Device info: {self.get_device_info()}")
            
            raise
    
    return wrapper

# ===============================
# Global Utility Functions
# ===============================

def get_gpu_info() -> Dict[str, Any]:
    """ã‚·ã‚¹ãƒ†ãƒ ã®GPUæƒ…å ±ã‚’å–å¾—"""
    info = {
        'gpu_available': GPU_AVAILABLE,
        'has_cupy': HAS_GPU,
        'devices': []
    }
    
    if GPU_AVAILABLE:
        n_devices = cp.cuda.runtime.getDeviceCount()
        info['device_count'] = n_devices
        
        for i in range(n_devices):
            try:
                with cp.cuda.Device(i):
                    props = cp.cuda.runtime.getDeviceProperties(i)
                    free_mem, total_mem = cp.cuda.runtime.memGetInfo()
                    
                    device_info = {
                        'id': i,
                        'name': props['name'].decode(),
                        'compute_capability': f"{props['major']}.{props['minor']}",
                        'total_memory_gb': total_mem / 1e9,
                        'free_memory_gb': free_mem / 1e9,
                        'multiprocessor_count': props['multiProcessorCount']
                    }
                    info['devices'].append(device_info)
            except Exception as e:
                logger.warning(f"Failed to get info for GPU {i}: {e}")
    
    return info

def set_gpu_device(device_id: int):
    """ä½¿ç”¨ã™ã‚‹GPUãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š"""
    if not GPU_AVAILABLE:
        raise RuntimeError("GPU not available")
    
    n_devices = cp.cuda.runtime.getDeviceCount()
    if device_id >= n_devices:
        raise ValueError(f"GPU {device_id} not found. Available: 0-{n_devices-1}")
    
    cp.cuda.Device(device_id).use()
    logger.info(f"Set active GPU to device {device_id}")

def enable_gpu_logging(level: str = 'INFO'):
    """GPUé–¢é€£ã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š"""
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    # Lambda3 GPUã®ãƒ­ã‚¬ãƒ¼
    logging.getLogger('lambda3_gpu').setLevel(log_level)
    
    # CuPyã®ãƒ­ã‚¬ãƒ¼ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    if HAS_GPU:
        logging.getLogger('cupy').setLevel(log_level)

def benchmark_gpu() -> Dict[str, float]:
    """ç°¡å˜ãªGPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    if not GPU_AVAILABLE:
        return {'error': 'GPU not available'}
    
    results = {}
    sizes = [1000, 5000, 10000]
    
    for size in sizes:
        # è¡Œåˆ—ç©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        a = cp.random.randn(size, size, dtype=np.float32)
        b = cp.random.randn(size, size, dtype=np.float32)
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = cp.dot(a, b)
        cp.cuda.Stream.null.synchronize()
        
        # è¨ˆæ¸¬
        start = time.perf_counter()
        for _ in range(5):
            c = cp.dot(a, b)
        cp.cuda.Stream.null.synchronize()
        elapsed = (time.perf_counter() - start) / 5
        
        results[f'matmul_{size}x{size}'] = elapsed
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        del a, b, c
        cp.get_default_memory_pool().free_all_blocks()
    
    return results

# ===============================
# Initialize on Import
# ===============================

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
import os
log_level = os.environ.get('LAMBDA3_LOG_LEVEL', 'INFO')
enable_gpu_logging(log_level)

# GPUæƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰
if logger.isEnabledFor(logging.DEBUG):
    gpu_info = get_gpu_info()
    logger.debug(f"GPU Info: {gpu_info}")
