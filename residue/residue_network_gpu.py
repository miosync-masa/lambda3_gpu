"""
Residue Network Analysis (GPU Version) - v4.0 Design Unified
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

æ®‹åŸºé–“ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æã®GPUå®Ÿè£… - 3ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­è¨ˆçµ±ä¸€ç‰ˆï¼
quantum_validation_v4.pyã®è¨­è¨ˆæ€æƒ³ã«å®Œå…¨æº–æ‹ ğŸ’•

Version: 4.0-unified
by ç’°ã¡ã‚ƒã‚“ - Design Unified Edition
"""

import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Union, Any, Set
from dataclasses import dataclass, field
from collections import defaultdict

# GPU imports
try:
    import cupy as cp
    from cupyx.scipy.spatial.distance import cdist as cp_cdist
    HAS_GPU = True
except ImportError:
    HAS_GPU = False
    cp = None

# Local imports
from ..types import ArrayType, NDArray
from ..core import GPUBackend, GPUMemoryManager, handle_gpu_errors

logger = logging.getLogger('lambda3_gpu.residue.network')

# ===============================
# Data Classes
# ===============================

@dataclass
class NetworkLink:
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒªãƒ³ã‚¯"""
    from_res: int
    to_res: int
    strength: float
    lag: int = 0
    distance: Optional[float] = None
    sync_rate: Optional[float] = None
    link_type: str = 'causal'  # 'causal', 'sync', 'async', 'instantaneous', 'transition', 'cascade'
    confidence: float = 1.0
    quantum_signature: Optional[str] = None  # 'entanglement', 'tunneling', 'jump', etc.

@dataclass
class NetworkAnalysisResult:
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æçµæœ"""
    causal_network: List[NetworkLink]
    sync_network: List[NetworkLink]
    async_strong_bonds: List[NetworkLink]
    spatial_constraints: Dict[Tuple[int, int], float]
    adaptive_windows: Dict[int, int]
    network_stats: Dict[str, Any]
    
    @property
    def n_causal_links(self) -> int:
        return len(self.causal_network)
    
    @property
    def n_sync_links(self) -> int:
        return len(self.sync_network)
    
    @property
    def n_async_bonds(self) -> int:
        return len(self.async_strong_bonds)

# ===============================
# CUDA Kernelsï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰
# ===============================

# é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºè¨ˆç®—ã‚«ãƒ¼ãƒãƒ«
ADAPTIVE_WINDOW_KERNEL = r'''
extern "C" __global__
void compute_adaptive_windows_kernel(
    const float* __restrict__ anomaly_scores,  // (n_residues, n_frames)
    int* __restrict__ window_sizes,          // (n_residues,)
    const int n_residues,
    const int n_frames,
    const int min_window,
    const int max_window,
    const int base_window
) {
    const int res_id = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (res_id >= n_residues) return;
    
    // è©²å½“æ®‹åŸºã®ã‚¹ã‚³ã‚¢ã‚’è§£æ
    const float* res_scores = &anomaly_scores[res_id * n_frames];
    
    // ã‚¤ãƒ™ãƒ³ãƒˆå¯†åº¦è¨ˆç®—
    int n_events = 0;
    float score_sum = 0.0f;
    float score_sq_sum = 0.0f;
    
    for (int i = 0; i < n_frames; i++) {
        float score = res_scores[i];
        if (score > 1.0f) n_events++;
        score_sum += score;
        score_sq_sum += score * score;
    }
    
    float event_density = (float)n_events / n_frames;
    float mean = score_sum / n_frames;
    float variance = (score_sq_sum / n_frames) - (mean * mean);
    float volatility = (mean > 1e-10f) ? sqrtf(variance) / mean : 0.0f;
    
    // ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—
    float scale_factor = 1.0f;
    
    if (event_density > 0.1f) {
        scale_factor *= 0.7f;
    } else if (event_density < 0.02f) {
        scale_factor *= 2.0f;
    }
    
    if (volatility > 2.0f) {
        scale_factor *= 0.8f;
    } else if (volatility < 0.5f) {
        scale_factor *= 1.3f;
    }
    
    // ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºæ±ºå®š
    int adaptive_window = (int)(base_window * scale_factor);
    window_sizes[res_id] = max(min_window, min(max_window, adaptive_window));
}
'''

# ç©ºé–“åˆ¶ç´„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ¼ãƒãƒ«
SPATIAL_FILTER_KERNEL = r'''
extern "C" __global__
void filter_by_distance_kernel(
    const float* __restrict__ distances,      // (n_pairs,)
    const int* __restrict__ pair_indices,     // (n_pairs, 2)
    bool* __restrict__ valid_mask,           // (n_pairs,)
    float* __restrict__ weights,             // (n_pairs,)
    const int n_pairs,
    const float max_distance,
    const float contact_distance,
    const float near_distance
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx >= n_pairs) return;
    
    float dist = distances[idx];
    
    // è·é›¢ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ä»˜ã‘
    if (dist < contact_distance) {
        valid_mask[idx] = true;
        weights[idx] = 1.0f;
    } else if (dist < near_distance) {
        valid_mask[idx] = true;
        weights[idx] = 0.8f;
    } else if (dist < max_distance) {
        valid_mask[idx] = true;
        weights[idx] = 0.5f;
    } else {
        valid_mask[idx] = false;
        weights[idx] = 0.0f;
    }
}
'''

# ===============================
# Residue Network GPU Class (v4.0 Unified)
# ===============================

class ResidueNetworkGPU(GPUBackend):
    """
    æ®‹åŸºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æã®GPUå®Ÿè£…
    v4.0 Design Unified - 3ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­è¨ˆçµ±ä¸€ç‰ˆï¼
    """
    
    def __init__(self,
                 max_interaction_distance: float = 15.0,
                 correlation_threshold: float = 0.15,
                 sync_threshold: float = 0.2,
                 min_causality_strength: float = 0.2,
                 max_causal_links: int = 500,
                 memory_manager: Optional[GPUMemoryManager] = None,
                 **kwargs):
        """
        Parameters
        ----------
        max_interaction_distance : float
            æœ€å¤§ç›¸äº’ä½œç”¨è·é›¢ï¼ˆÃ…ï¼‰
        correlation_threshold : float
            ç›¸é–¢é–¾å€¤
        sync_threshold : float
            åŒæœŸåˆ¤å®šé–¾å€¤
        min_causality_strength : float
            æœ€å°å› æœå¼·åº¦
        max_causal_links : int
            æœ€å¤§å› æœãƒªãƒ³ã‚¯æ•°
        """
        super().__init__(**kwargs)
        self.max_interaction_distance = max_interaction_distance
        self.correlation_threshold = correlation_threshold
        self.sync_threshold = sync_threshold
        self.min_causality_strength = min_causality_strength
        self.max_causal_links = max_causal_links
        self.memory_manager = memory_manager or GPUMemoryManager()
        
        # ã‚«ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        if self.is_gpu and HAS_GPU:
            self._compile_kernels()
    
    def _compile_kernels(self):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        try:
            self.adaptive_window_kernel = cp.RawKernel(
                ADAPTIVE_WINDOW_KERNEL, 'compute_adaptive_windows_kernel'
            )
            self.spatial_filter_kernel = cp.RawKernel(
                SPATIAL_FILTER_KERNEL, 'filter_by_distance_kernel'
            )
            logger.debug("Network analysis kernels compiled successfully")
        except Exception as e:
            logger.warning(f"Failed to compile custom kernels: {e}")
            self.adaptive_window_kernel = None
            self.spatial_filter_kernel = None
    
    @handle_gpu_errors
    def analyze_network(self,
                   residue_anomaly_scores: Dict[int, np.ndarray],
                   residue_coupling: np.ndarray,
                   residue_coms: Optional[np.ndarray] = None,
                   lag_window: int = 200) -> NetworkAnalysisResult:
        """
        æ®‹åŸºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è§£æï¼ˆv4.0 ä¿®æ­£ç‰ˆï¼‰
        anomaly_scoresãŒç©ºã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«æ”¹å–„ï¼
        """
        with self.timer('analyze_network'):
            logger.info("ğŸ¯ Analyzing residue interaction network (v4.0 Unified)")
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ç¢ºèª
            if not residue_anomaly_scores:
                logger.warning("No anomaly scores provided - using default analysis")
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è§£æã‚’æä¾›
                n_residues = residue_coupling.shape[1] if residue_coupling is not None else 10
                n_frames = residue_coupling.shape[0] if residue_coupling is not None else 1
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®anomaly_scoresã‚’ç”Ÿæˆ
                residue_anomaly_scores = {}
                for res_id in range(n_residues):
                    residue_anomaly_scores[res_id] = np.random.uniform(0.5, 1.5, n_frames)
                
                logger.info(f"   Generated default scores for {n_residues} residues")
            
            first_score = next(iter(residue_anomaly_scores.values()))
            n_frames = len(first_score)
            
            if n_frames <= 0:
                logger.warning("No frames to analyze")
                return self._create_empty_result()
            
            # ========================================
            # v4.0 3ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š
            # ========================================
            
            # 1. CASCADEåˆ¤å®šï¼ˆasync_bondsã®æ½œåœ¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
            has_cascade_potential = self._check_cascade_potential(
                residue_anomaly_scores, residue_coupling, n_frames
            )
            
            if has_cascade_potential and n_frames >= 2:
                logger.info("   ğŸŒ CASCADE pattern detected - Network cascade analysis")
                return self._analyze_cascade_pattern(
                    residue_anomaly_scores, residue_coupling, residue_coms, lag_window
                )
            
            # 2. INSTANTANEOUSåˆ¤å®šï¼ˆå˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
            elif n_frames == 1:
                logger.info("   âš¡ INSTANTANEOUS pattern detected - Quantum-like analysis")
                return self._analyze_instantaneous_pattern(
                    residue_anomaly_scores, residue_coupling, residue_coms
                )
            
            # 3. TRANSITIONåˆ¤å®šï¼ˆé€šå¸¸ã®æ™‚ç³»åˆ—ï¼‰
            else:
                logger.info(f"   ğŸ“ˆ TRANSITION pattern detected - {n_frames} frames analysis")
                return self._analyze_transition_pattern(
                    residue_anomaly_scores, residue_coupling, residue_coms, 
                    lag_window, n_frames
                )
    
    def _analyze_transition_pattern(self,
                                  residue_anomaly_scores: Dict[int, np.ndarray],
                                  residue_coupling: np.ndarray,
                                  residue_coms: Optional[np.ndarray],
                                  lag_window: int,
                                  n_frames: int) -> NetworkAnalysisResult:
        """TRANSITIONãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šæ™‚ç³»åˆ—é·ç§»ã®è§£æï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        residue_ids = sorted(residue_anomaly_scores.keys())
        
        # è§£æãƒ¢ãƒ¼ãƒ‰ã®æ±ºå®šï¼ˆä¿®æ­£ï¼šã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        if n_frames < 10:
            analysis_mode = 'short_timeseries'
            logger.info(f"      Analysis mode: {analysis_mode}")
            quantum_signature = 'short_transition'
        elif n_frames < 50:
            analysis_mode = 'medium_timeseries'
            quantum_signature = 'medium_transition'
        else:
            analysis_mode = 'long_timeseries'
            quantum_signature = 'extended_transition'
        
        # é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆä¿®æ­£ï¼šæœ€å°å€¤ã‚’ä¿è¨¼ï¼‰
        if self.adaptive_window_kernel and n_frames >= 10:
            adaptive_windows = self._compute_adaptive_windows_gpu(
                residue_anomaly_scores, n_frames
            )
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            default_window = min(100, max(10, n_frames // 2))
            adaptive_windows = {res_id: default_window for res_id in residue_ids}
            logger.debug(f"Using default adaptive window: {default_window}")
        
        # ç©ºé–“åˆ¶ç´„ï¼ˆä¿®æ­£ï¼šNone ãƒã‚§ãƒƒã‚¯ï¼‰
        if residue_coms is not None and len(residue_coms) > 0:
            spatial_constraints = self._compute_spatial_constraints(
                residue_ids, residue_coms
            )
        else:
            # å…¨ãƒšã‚¢ã‚’æœ‰åŠ¹ã¨ã™ã‚‹
            spatial_constraints = {}
            for i, res_i in enumerate(residue_ids):
                for j, res_j in enumerate(residue_ids[i+1:], i+1):
                    spatial_constraints[(res_i, res_j)] = 10.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè·é›¢
            logger.debug("Using default spatial constraints (all pairs valid)")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆä¿®æ­£ï¼šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰
        try:
            if analysis_mode == 'short_timeseries':
                # çŸ­ã„æ™‚ç³»åˆ—ã§ã¯åŒæœŸæ€§ã‚’é‡è¦–
                networks = self._build_short_timeseries_network(
                    residue_anomaly_scores,
                    residue_coupling,
                    spatial_constraints,
                    n_frames
                )
            else:
                # é€šå¸¸ã®å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ
                networks = self._build_classical_transition_network(
                    residue_anomaly_scores,
                    residue_coupling,
                    spatial_constraints,
                    adaptive_windows,
                    lag_window
                )
        except Exception as e:
            logger.error(f"Network construction failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç©ºã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            networks = {
                'causal': [],
                'sync': [],
                'async': []
            }
        
        # çµ±è¨ˆæƒ…å ±ï¼ˆä¿®æ­£ï¼šå®‰å…¨ãªè¨ˆç®—ï¼‰
        network_stats = self._compute_network_stats(networks, spatial_constraints)
        network_stats.update({
            'event_type': 'TRANSITION',
            'pattern': 'transition',
            'quantum_signature': quantum_signature,
            'analysis_mode': analysis_mode,
            'n_frames': n_frames,
            'n_residues': len(residue_ids)
        })
        
        result = NetworkAnalysisResult(
            causal_network=networks['causal'],
            sync_network=networks['sync'],
            async_strong_bonds=networks['async'],
            spatial_constraints=spatial_constraints,
            adaptive_windows=adaptive_windows,
            network_stats=network_stats
        )
        
        self._print_summary(result)
        return result
    
    def _build_short_timeseries_network(self,
                                       residue_anomaly_scores: Dict[int, np.ndarray],
                                       residue_coupling: np.ndarray,
                                       spatial_constraints: Dict,
                                       n_frames: int) -> Dict[str, List[NetworkLink]]:
        """çŸ­ã„æ™‚ç³»åˆ—ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆæ–°è¦è¿½åŠ ï¼‰"""
        networks = {'causal': [], 'sync': [], 'async': []}
        residue_ids = sorted(residue_anomaly_scores.keys())
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºç›¸é–¢ã‚’è¨ˆç®—
        for i, res_i in enumerate(residue_ids):
            for j, res_j in enumerate(residue_ids[i+1:], i+1):
                if (res_i, res_j) not in spatial_constraints:
                    continue
                
                scores_i = residue_anomaly_scores[res_i]
                scores_j = residue_anomaly_scores[res_j]
                
                # ç›¸é–¢è¨ˆç®—ï¼ˆçŸ­ã„ç³»åˆ—ã§ã‚‚å®‰å…¨ã«ï¼‰
                if len(scores_i) > 1 and len(scores_j) > 1:
                    try:
                        corr = np.corrcoef(scores_i, scores_j)[0, 1]
                        if not np.isnan(corr) and abs(corr) > 0.3:  # é–¾å€¤ã‚’ç·©å’Œ
                            link = NetworkLink(
                                from_res=res_i,
                                to_res=res_j,
                                strength=abs(corr),
                                lag=0,
                                correlation=corr,
                                distance=spatial_constraints[(res_i, res_j)]
                            )
                            
                            if corr > 0.5:
                                networks['sync'].append(link)
                            else:
                                networks['async'].append(link)
                    except:
                        continue
        
        return networks
    
    # ========================================
    # CASCADEæ½œåœ¨æ€§ãƒã‚§ãƒƒã‚¯
    # ========================================
    
    def _check_cascade_potential(self,
                                residue_anomaly_scores: Dict[int, np.ndarray],
                                residue_coupling: np.ndarray,
                                n_frames: int) -> bool:
        """CASCADEãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ½œåœ¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if n_frames < 2:
            return False
        
        # è¤‡æ•°æ®‹åŸºãŒåŒæ™‚ã«é«˜ã„ç•°å¸¸ã‚’ç¤ºã™ã‹ãƒã‚§ãƒƒã‚¯
        high_anomaly_count = 0
        threshold = 2.0  # ç•°å¸¸ã‚¹ã‚³ã‚¢é–¾å€¤
        
        for scores in residue_anomaly_scores.values():
            if np.any(scores > threshold):
                high_anomaly_count += 1
        
        # 3æ®‹åŸºä»¥ä¸ŠãŒç•°å¸¸ â†’ CASCADEå¯èƒ½æ€§
        if high_anomaly_count >= 3:
            # ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°ã®å¤‰å‹•ã‚‚ãƒã‚§ãƒƒã‚¯
            if residue_coupling.ndim == 3 and residue_coupling.shape[0] >= 2:
                coupling_std = np.std(residue_coupling, axis=0)
                if np.max(coupling_std) > 0.3:  # é«˜ã„å¤‰å‹•
                    return True
        
        return False
    
    # ========================================
    # INSTANTANEOUS ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
    # ========================================
    
    def _analyze_instantaneous_pattern(self,
                                  residue_anomaly_scores: Dict[int, np.ndarray],
                                  residue_coupling: np.ndarray,
                                  residue_coms: Optional[np.ndarray]) -> NetworkAnalysisResult:
        """INSTANTANEOUSãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šåŒä¸€ãƒ•ãƒ¬ãƒ¼ãƒ å†…ä¸¦åˆ—å”èª¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        
        residue_ids = sorted(residue_anomaly_scores.keys())
        
        # å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç•°å¸¸ã‚¹ã‚³ã‚¢å–å¾—
        frame_scores = {}
        for res_id, scores in residue_anomaly_scores.items():
            frame_scores[res_id] = float(scores[0] if hasattr(scores, '__len__') else scores)
        
        # é«˜ç•°å¸¸æ®‹åŸºã‚’ç‰¹å®š
        mean_score = np.mean(list(frame_scores.values()))
        std_score = np.std(list(frame_scores.values()))
        anomaly_threshold = mean_score + 2 * std_score
        high_anomaly_residues = [r for r, s in frame_scores.items() if s > anomaly_threshold]
        
        logger.info(f"   Found {len(high_anomaly_residues)} residues with simultaneous anomalies")
        
        # ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°è¡Œåˆ—ã®æº–å‚™
        if residue_coupling.ndim == 3 and residue_coupling.shape[0] > 0:
            coupling = residue_coupling[0]
        elif residue_coupling.ndim == 2:
            coupling = residue_coupling
        else:
            coupling = None
        
        # ä¸¦åˆ—å”èª¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
        sync_links = []
        async_bonds = []
        
        # é«˜ç•°å¸¸æ®‹åŸºé–“ã®ãƒšã‚¢
        for i, res_i in enumerate(high_anomaly_residues):
            for j, res_j in enumerate(high_anomaly_residues[i+1:], i+1):
                # å”èª¿å¼·åº¦ = ç•°å¸¸åº¦ã®å¹¾ä½•å¹³å‡
                strength = np.sqrt(frame_scores[res_i] * frame_scores[res_j])
                
                # ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°ã§é‡ã¿ä»˜ã‘ï¼ˆã‚ã‚Œã°ï¼‰
                if coupling is not None and res_i < coupling.shape[0] and res_j < coupling.shape[1]:
                    coupling_factor = coupling[res_i, res_j] / (np.mean(coupling) + 1e-10)
                    strength *= np.clip(coupling_factor, 0.5, 2.0)
                
                # ç©ºé–“è·é›¢ï¼ˆã‚ã‚Œã°ï¼‰
                distance = None
                if residue_coms is not None and residue_coms.size > 0:
                    if residue_coms.ndim >= 2 and res_i < residue_coms.shape[-2] and res_j < residue_coms.shape[-2]:
                        if residue_coms.ndim == 3:
                            com_i = residue_coms[0, res_i]
                            com_j = residue_coms[0, res_j]
                        else:
                            com_i = residue_coms[res_i]
                            com_j = residue_coms[res_j]
                        distance = float(np.linalg.norm(com_i - com_j))
                
                link = NetworkLink(
                    from_res=res_i,
                    to_res=res_j,
                    strength=float(strength),
                    lag=0,  # æ™‚é–“å·®ãªã—
                    distance=distance,
                    sync_rate=1.0,  # å®Œå…¨åŒæœŸ
                    link_type='instantaneous',
                    confidence=1.0,
                    quantum_signature='parallel_coordination'  # ä¸¦åˆ—å”èª¿
                )
                sync_links.append(link)
                
                # ç‰¹ã«å¼·ã„å”èª¿
                if strength > anomaly_threshold * 1.5:
                    async_bonds.append(link)
        
        # ç©ºé–“åˆ¶ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        spatial_constraints = {}
        if residue_coms is not None and residue_coms.shape[0] > 0:
            spatial_constraints = self._compute_spatial_constraints(residue_ids, residue_coms)
        
        return NetworkAnalysisResult(
            causal_network=[],  # å› æœãªã—ï¼ˆæ™‚é–“å·®ã‚¼ãƒ­ï¼‰
            sync_network=sync_links,  # ä¸¦åˆ—å”èª¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            async_strong_bonds=async_bonds,
            spatial_constraints=spatial_constraints,
            adaptive_windows={res_id: 1 for res_id in residue_ids},
            network_stats={
                'n_causal': 0,
                'n_sync': len(sync_links),
                'n_async': len(async_bonds),
                'event_type': 'INSTANTANEOUS',
                'pattern': 'parallel_network',  # ä¸¦åˆ—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                'quantum_signature': 'parallel_coordination',
                'n_frames': 1,
                'n_high_anomaly': len(high_anomaly_residues)
            }
        )
        
    # ========================================
    # TRANSITION ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
    # ========================================
    
    def _analyze_transition_pattern(self,
                                  residue_anomaly_scores: Dict[int, np.ndarray],
                                  residue_coupling: np.ndarray,
                                  residue_coms: Optional[np.ndarray],
                                  lag_window: int,
                                  n_frames: int) -> NetworkAnalysisResult:
        """TRANSITIONãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šé·ç§»éç¨‹ã®è§£æ"""
        
        residue_ids = sorted(residue_anomaly_scores.keys())
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«ã‚ˆã‚‹ã‚µãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
        if n_frames == 2:
            quantum_signature = 'tunneling'
            analysis_mode = 'quantum_tunneling'
        elif n_frames == 3:
            quantum_signature = 'jump'
            analysis_mode = 'quantum_jump'
        elif n_frames < 10:
            quantum_signature = 'short_transition'
            analysis_mode = 'short_timeseries'
        else:
            quantum_signature = None
            analysis_mode = 'classical'
        
        logger.info(f"      Analysis mode: {analysis_mode}")
        
        # é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºè¨ˆç®—
        with self.timer('adaptive_windows'):
            adaptive_windows = self._compute_adaptive_windows(residue_anomaly_scores)
        
        # ç©ºé–“åˆ¶ç´„è¨ˆç®—
        with self.timer('spatial_constraints'):
            if residue_coms is not None:
                spatial_constraints = self._compute_spatial_constraints(
                    residue_ids, residue_coms
                )
            else:
                spatial_constraints = self._create_all_pairs(residue_ids)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
        with self.timer('build_network'):
            if analysis_mode in ['quantum_tunneling', 'quantum_jump']:
                networks = self._build_quantum_transition_network(
                    residue_anomaly_scores,
                    residue_coupling,
                    spatial_constraints,
                    n_frames,
                    quantum_signature
                )
            elif analysis_mode == 'short_timeseries':
                networks = self._build_short_transition_network(
                    residue_anomaly_scores,
                    residue_coupling,
                    spatial_constraints,
                    n_frames
                )
            else:
                networks = self._build_classical_transition_network(
                    residue_anomaly_scores,
                    residue_coupling,
                    spatial_constraints,
                    adaptive_windows,
                    lag_window
                )
        
        # çµ±è¨ˆæƒ…å ±
        network_stats = self._compute_network_stats(networks, spatial_constraints)
        network_stats.update({
            'event_type': 'TRANSITION',
            'pattern': 'transition',
            'quantum_signature': quantum_signature,
            'analysis_mode': analysis_mode,
            'n_frames': n_frames
        })
        
        result = NetworkAnalysisResult(
            causal_network=networks['causal'],
            sync_network=networks['sync'],
            async_strong_bonds=networks['async'],
            spatial_constraints=spatial_constraints,
            adaptive_windows=adaptive_windows,
            network_stats=network_stats
        )
        
        self._print_summary(result)
        return result
    
    # ========================================
    # CASCADE ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
    # ========================================
    
    def _analyze_cascade_pattern(self,
                               residue_anomaly_scores: Dict[int, np.ndarray],
                               residue_coupling: np.ndarray,
                               residue_coms: Optional[np.ndarray],
                               lag_window: int) -> NetworkAnalysisResult:
        """CASCADEãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®è§£æ"""
        
        residue_ids = sorted(residue_anomaly_scores.keys())
        n_frames = len(next(iter(residue_anomaly_scores.values())))
        
        logger.info("      Detecting cascade propagation paths...")
        
        # é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ç”¨ã«çŸ­ãï¼‰
        adaptive_windows = {res_id: min(50, n_frames//2) for res_id in residue_ids}
        
        # ç©ºé–“åˆ¶ç´„
        if residue_coms is not None:
            spatial_constraints = self._compute_spatial_constraints(
                residue_ids, residue_coms
            )
        else:
            spatial_constraints = self._create_all_pairs(residue_ids)
        
        # ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ç‰¹æœ‰ã®è§£æ
        causal_links = []
        sync_links = []
        async_bonds = []
        
        # é–‹å§‹ç‚¹ã®æ¤œå‡ºï¼ˆæœ€åˆã«ç•°å¸¸ã‚’ç¤ºã—ãŸæ®‹åŸºï¼‰
        initiators = self._find_cascade_initiators(residue_anomaly_scores)
        
        # ä¼æ’­çµŒè·¯ã®æ§‹ç¯‰
        for initiator in initiators:
            paths = self._trace_cascade_paths(
                initiator,
                residue_anomaly_scores,
                residue_coupling,
                spatial_constraints,
                max_depth=5
            )
            
            for path in paths:
                for i in range(len(path) - 1):
                    from_res = path[i]
                    to_res = path[i + 1]
                    
                    # ä¼æ’­å¼·åº¦ã®è¨ˆç®—
                    strength = self._calculate_cascade_strength(
                        from_res, to_res,
                        residue_anomaly_scores,
                        residue_coupling
                    )
                    
                    if strength > self.min_causality_strength:
                        link = NetworkLink(
                            from_res=from_res,
                            to_res=to_res,
                            strength=strength,
                            lag=i + 1,  # ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®ã‚¹ãƒ†ãƒƒãƒ—
                            sync_rate=0.0,  # éåŒæœŸ
                            link_type='cascade',
                            confidence=0.8,
                            quantum_signature='information_transfer'
                        )
                        causal_links.append(link)
                        async_bonds.append(link)
        
        # åŒæœŸçš„ãªå‰¯æ¬¡åŠ¹æœã‚‚æ¤œå‡º
        sync_links = self._detect_cascade_synchrony(
            residue_anomaly_scores,
            residue_coupling,
            spatial_constraints
        )
        
        # çµ±è¨ˆæƒ…å ±
        network_stats = {
            'n_causal': len(causal_links),
            'n_sync': len(sync_links),
            'n_async': len(async_bonds),
            'event_type': 'CASCADE',
            'pattern': 'cascade',
            'quantum_signature': 'information_transfer',
            'n_initiators': len(initiators),
            'cascade_depth': max([link.lag for link in causal_links]) if causal_links else 0,
            'n_frames': n_frames
        }
        
        result = NetworkAnalysisResult(
            causal_network=causal_links,
            sync_network=sync_links,
            async_strong_bonds=async_bonds,
            spatial_constraints=spatial_constraints,
            adaptive_windows=adaptive_windows,
            network_stats=network_stats
        )
        
        self._print_summary(result)
        return result
    
    # ========================================
    # TRANSITION ã‚µãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    # ========================================
    
    def _build_quantum_transition_network(self,
                                        anomaly_scores: Dict[int, np.ndarray],
                                        residue_coupling: np.ndarray,
                                        spatial_constraints: Dict,
                                        n_frames: int,
                                        quantum_signature: str) -> Dict:
        """é‡å­é·ç§»ï¼ˆ2-3ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        causal_links = []
        sync_links = []
        async_bonds = []
        
        residue_ids = sorted(anomaly_scores.keys())
        
        for i, res_i in enumerate(residue_ids):
            for j, res_j in enumerate(residue_ids[i+1:], i+1):
                if (res_i, res_j) not in spatial_constraints:
                    continue
                
                scores_i = anomaly_scores[res_i]
                scores_j = anomaly_scores[res_j]
                
                # å¤‰åŒ–é‡è¨ˆç®—
                if n_frames == 2:
                    delta_i = scores_i[1] - scores_i[0]
                    delta_j = scores_j[1] - scores_j[0]
                    
                    # åŒã˜æ–¹å‘ã¸ã®å¤‰åŒ– = ãƒˆãƒ³ãƒãƒªãƒ³ã‚°ãƒšã‚¢
                    if delta_i * delta_j > 0 and abs(delta_i) > 0.5 and abs(delta_j) > 0.5:
                        link = NetworkLink(
                            from_res=res_i,
                            to_res=res_j,
                            strength=float(np.sqrt(abs(delta_i * delta_j))),
                            lag=0,
                            sync_rate=0.8,
                            link_type='transition',
                            quantum_signature=quantum_signature,
                            distance=spatial_constraints[(res_i, res_j)]
                        )
                        sync_links.append(link)
                        if link.strength > 0.7:
                            async_bonds.append(link)
                
                elif n_frames == 3:
                    # ã‚¸ãƒ£ãƒ³ãƒ—æ¤œå‡º
                    jump_i = abs(scores_i[1] - scores_i[0]) + abs(scores_i[2] - scores_i[1])
                    jump_j = abs(scores_j[1] - scores_j[0]) + abs(scores_j[2] - scores_j[1])
                    
                    if jump_i > 0.3 and jump_j > 0.3:
                        link = NetworkLink(
                            from_res=res_i,
                            to_res=res_j,
                            strength=float(np.sqrt(jump_i * jump_j)),
                            lag=1,
                            sync_rate=0.5,
                            link_type='transition',
                            quantum_signature=quantum_signature,
                            distance=spatial_constraints[(res_i, res_j)]
                        )
                        sync_links.append(link)
                        if link.strength > 0.5:
                            async_bonds.append(link)
        
        return {
            'causal': causal_links,
            'sync': sync_links,
            'async': async_bonds
        }
    
    def _build_short_transition_network(self,
                                       anomaly_scores: Dict[int, np.ndarray],
                                       residue_coupling: np.ndarray,
                                       spatial_constraints: Dict,
                                       n_frames: int) -> Dict:
        """çŸ­æœŸé·ç§»ï¼ˆ4-9ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        causal_links = []
        sync_links = []
        async_bonds = []
        
        residue_ids = sorted(anomaly_scores.keys())
        
        for i, res_i in enumerate(residue_ids):
            for j, res_j in enumerate(residue_ids[i+1:], i+1):
                if (res_i, res_j) not in spatial_constraints:
                    continue
                
                scores_i = anomaly_scores[res_i]
                scores_j = anomaly_scores[res_j]
                
                # å‰åŠã¨å¾ŒåŠã®æ¯”è¼ƒ
                mid_point = n_frames // 2
                first_half_i = np.mean(scores_i[:mid_point])
                second_half_i = np.mean(scores_i[mid_point:])
                first_half_j = np.mean(scores_j[:mid_point])
                second_half_j = np.mean(scores_j[mid_point:])
                
                change_i = second_half_i - first_half_i
                change_j = second_half_j - first_half_j
                
                threshold = 0.2
                
                if abs(change_i) > threshold and abs(change_j) > threshold:
                    if change_i * change_j > 0:
                        # åŒæœŸçš„å¤‰åŒ–
                        link = NetworkLink(
                            from_res=res_i,
                            to_res=res_j,
                            strength=float(np.sqrt(abs(change_i * change_j))),
                            lag=0,
                            sync_rate=0.6,
                            link_type='transition',
                            distance=spatial_constraints[(res_i, res_j)]
                        )
                        sync_links.append(link)
                    else:
                        # å› æœçš„å¤‰åŒ–
                        if abs(change_i) > abs(change_j):
                            from_res, to_res = res_i, res_j
                        else:
                            from_res, to_res = res_j, res_i
                        
                        link = NetworkLink(
                            from_res=from_res,
                            to_res=to_res,
                            strength=float(max(abs(change_i), abs(change_j))),
                            lag=mid_point,
                            sync_rate=0.2,
                            link_type='transition',
                            distance=spatial_constraints.get((from_res, to_res), 0.0)
                        )
                        causal_links.append(link)
                        
                        if link.strength > 0.5:
                            async_bonds.append(link)
        
        return {
            'causal': causal_links,
            'sync': sync_links,
            'async': async_bonds
        }
    
    def _build_classical_transition_network(self,
                                          anomaly_scores: Dict[int, np.ndarray],
                                          residue_coupling: np.ndarray,
                                          spatial_constraints: Dict,
                                          adaptive_windows: Dict[int, int],
                                          lag_window: int) -> Dict:
        """å¤å…¸çš„é·ç§»ï¼ˆ10+ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        # æ—¢å­˜ã®_build_networksãƒ¡ã‚½ãƒƒãƒ‰ã®å†…å®¹ã‚’ä½¿ç”¨
        return self._build_networks(
            anomaly_scores,
            residue_coupling,
            spatial_constraints,
            adaptive_windows,
            lag_window
        )
    
    # ========================================
    # CASCADEç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # ========================================
    
    def _find_cascade_initiators(self,
                                anomaly_scores: Dict[int, np.ndarray]) -> List[int]:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®é–‹å§‹ç‚¹ã‚’æ¤œå‡º"""
        initiators = []
        
        # æœ€åˆã«ç•°å¸¸ã‚’ç¤ºã—ãŸæ®‹åŸºã‚’æ¢ã™
        for res_id, scores in anomaly_scores.items():
            # æœ€åˆã®é«˜ç•°å¸¸ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¢ã™
            first_anomaly = np.where(scores > 2.0)[0]
            if len(first_anomaly) > 0:
                initiators.append((res_id, first_anomaly[0]))
        
        # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        initiators.sort(key=lambda x: x[1])
        
        # ä¸Šä½5å€‹ã®é–‹å§‹ç‚¹
        return [res_id for res_id, _ in initiators[:5]]
    
    def _trace_cascade_paths(self,
                           initiator: int,
                           anomaly_scores: Dict[int, np.ndarray],
                           residue_coupling: np.ndarray,
                           spatial_constraints: Dict,
                           max_depth: int = 5) -> List[List[int]]:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ä¼æ’­çµŒè·¯ã‚’è¿½è·¡"""
        paths = []
        
        def dfs(current: int, path: List[int], depth: int):
            if depth >= max_depth:
                paths.append(path.copy())
                return
            
            # éš£æ¥æ®‹åŸºã‚’æ¢ã™
            neighbors = []
            for (res_i, res_j) in spatial_constraints.keys():
                if res_i == current:
                    neighbors.append(res_j)
                elif res_j == current:
                    neighbors.append(res_i)
            
            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            neighbors = sorted(neighbors, 
                             key=lambda x: np.max(anomaly_scores.get(x, [0])),
                             reverse=True)
            
            for neighbor in neighbors[:3]:  # ä¸Šä½3çµŒè·¯
                if neighbor not in path:
                    path.append(neighbor)
                    dfs(neighbor, path, depth + 1)
                    path.pop()
            
            if len(neighbors) == 0:
                paths.append(path.copy())
        
        dfs(initiator, [initiator], 0)
        return paths
    
    def _calculate_cascade_strength(self,
                                  from_res: int,
                                  to_res: int,
                                  anomaly_scores: Dict[int, np.ndarray],
                                  residue_coupling: np.ndarray) -> float:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ä¼æ’­ã®å¼·åº¦ã‚’è¨ˆç®—"""
        # ç°¡æ˜“å®Ÿè£…
        scores_from = anomaly_scores.get(from_res, np.array([0]))
        scores_to = anomaly_scores.get(to_res, np.array([0]))
        
        # ç›¸é–¢è¨ˆç®—
        if len(scores_from) > 1 and len(scores_to) > 1:
            try:
                corr = np.corrcoef(scores_from, scores_to)[0, 1]
                return abs(corr)
            except:
                return 0.0
        return 0.0
    
    def _detect_cascade_synchrony(self,
                                 anomaly_scores: Dict[int, np.ndarray],
                                 residue_coupling: np.ndarray,
                                 spatial_constraints: Dict) -> List[NetworkLink]:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã«ä¼´ã†åŒæœŸçš„å¤‰åŒ–ã‚’æ¤œå‡º"""
        sync_links = []
        
        # ç°¡æ˜“å®Ÿè£…ï¼šé«˜ç•°å¸¸ã‚’åŒæ™‚ã«ç¤ºã™ãƒšã‚¢ã‚’æ¢ã™
        residue_ids = sorted(anomaly_scores.keys())
        
        for i, res_i in enumerate(residue_ids):
            for j, res_j in enumerate(residue_ids[i+1:], i+1):
                if (res_i, res_j) not in spatial_constraints:
                    continue
                
                scores_i = anomaly_scores[res_i]
                scores_j = anomaly_scores[res_j]
                
                # åŒæ™‚ã«é«˜ã„ç•°å¸¸
                high_i = scores_i > 2.0
                high_j = scores_j > 2.0
                
                overlap = np.sum(high_i & high_j)
                
                if overlap > 0:
                    link = NetworkLink(
                        from_res=res_i,
                        to_res=res_j,
                        strength=float(overlap) / len(scores_i),
                        lag=0,
                        sync_rate=1.0,
                        link_type='cascade',
                        quantum_signature='synchronous_cascade',
                        distance=spatial_constraints[(res_i, res_j)]
                    )
                    sync_links.append(link)
        
        return sync_links
    
    # ========================================
    # æ—¢å­˜ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå¤‰æ›´æœ€å°é™ï¼‰
    # ========================================
    
    def _compute_adaptive_windows(self,
                                anomaly_scores: Dict[int, np.ndarray]) -> Dict[int, int]:
        """é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰"""
        n_residues = len(anomaly_scores)
        residue_ids = sorted(anomaly_scores.keys())
        
        # ã‚¹ã‚³ã‚¢ã‚’é…åˆ—ã«æ•´ç†
        n_frames = len(next(iter(anomaly_scores.values())))
        scores_array = np.zeros((n_residues, n_frames), dtype=np.float32)
        
        for i, res_id in enumerate(residue_ids):
            scores_array[i] = anomaly_scores[res_id]
        
        if self.is_gpu and self.adaptive_window_kernel is not None:
            # GPUç‰ˆ
            scores_gpu = self.to_gpu(scores_array)
            window_sizes_gpu = self.zeros(n_residues, dtype=cp.int32)
            
            # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
            block_size = 256
            grid_size = (n_residues + block_size - 1) // block_size
            
            self.adaptive_window_kernel(
                (grid_size,), (block_size,),
                (scores_gpu.ravel(), window_sizes_gpu, n_residues, n_frames,
                 30, 300, 100)  # min, max, base window
            )
            
            window_sizes = self.to_cpu(window_sizes_gpu)
        else:
            # CPUç‰ˆ
            window_sizes = np.zeros(n_residues, dtype=np.int32)
            
            for i, scores in enumerate(scores_array):
                # ã‚¤ãƒ™ãƒ³ãƒˆå¯†åº¦
                n_events = np.sum(scores > 1.0)
                event_density = n_events / n_frames
                
                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                if np.mean(scores) > 1e-10:
                    volatility = np.std(scores) / np.mean(scores)
                else:
                    volatility = 0.0
                
                # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
                scale_factor = 1.0
                if event_density > 0.1:
                    scale_factor *= 0.7
                elif event_density < 0.02:
                    scale_factor *= 2.0
                
                if volatility > 2.0:
                    scale_factor *= 0.8
                elif volatility < 0.5:
                    scale_factor *= 1.3
                
                window_sizes[i] = int(np.clip(100 * scale_factor, 30, 300))
        
        # è¾æ›¸ã«å¤‰æ›
        return {res_id: int(window_sizes[i]) for i, res_id in enumerate(residue_ids)}
    
    def _compute_spatial_constraints(self,
                                   residue_ids: List[int],
                                   residue_coms: np.ndarray) -> Dict[Tuple[int, int], float]:
        """ç©ºé–“åˆ¶ç´„è¨ˆç®—ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰"""
        n_frames, n_all_residues, _ = residue_coms.shape
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ ã§å¹³å‡è·é›¢è¨ˆç®—
        sample_frames = np.linspace(0, n_frames-1, min(10, n_frames), dtype=int)
        
        if self.is_gpu:
            coms_gpu = self.to_gpu(residue_coms[sample_frames])
            
            # å„ãƒ•ãƒ¬ãƒ¼ãƒ ã§è·é›¢è¨ˆç®—
            avg_distances = self.zeros((n_all_residues, n_all_residues))
            
            for frame_coms in coms_gpu:
                distances = cp_cdist(frame_coms, frame_coms)
                avg_distances += distances / len(sample_frames)
        else:
            # CPUç‰ˆ
            from scipy.spatial.distance import cdist
            avg_distances = np.zeros((n_all_residues, n_all_residues))
            
            for frame_idx in sample_frames:
                distances = cdist(residue_coms[frame_idx], residue_coms[frame_idx])
                avg_distances += distances / len(sample_frames)
        
        # æœ‰åŠ¹ãªãƒšã‚¢ã‚’æŠ½å‡º
        spatial_constraints = {}
        
        for i, res_i in enumerate(residue_ids):
            for j, res_j in enumerate(residue_ids):
                if i < j and res_i < n_all_residues and res_j < n_all_residues:
                    dist = float(avg_distances[res_i, res_j])
                    
                    if dist < self.max_interaction_distance:
                        spatial_constraints[(res_i, res_j)] = dist
        
        logger.info(f"   Found {len(spatial_constraints)} spatially valid pairs "
                   f"(< {self.max_interaction_distance} Ã…)")
        
        return spatial_constraints
    
    def _create_all_pairs(self, residue_ids: List[int]) -> Dict[Tuple[int, int], float]:
        """å…¨ãƒšã‚¢ã‚’ä½œæˆï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰"""
        pairs = {}
        for i, res_i in enumerate(residue_ids):
            for j, res_j in enumerate(residue_ids[i+1:], i+1):
                pairs[(res_i, res_j)] = 0.0  # è·é›¢ä¸æ˜
        return pairs
    
    def _build_networks(self,
                       anomaly_scores: Dict[int, np.ndarray],
                       residue_coupling: np.ndarray,
                       spatial_constraints: Dict[Tuple[int, int], float],
                       adaptive_windows: Dict[int, int],
                       lag_window: int) -> Dict[str, List[NetworkLink]]:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆæ—¢å­˜ã®å®Ÿè£…ã‚’æ´»ç”¨ï¼‰"""
        causal_links = []
        sync_links = []
        async_bonds = []
        
        # GPUã§ä¸¦åˆ—å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ãƒšã‚¢ã‚’ãƒãƒƒãƒåŒ–
        pairs_list = list(spatial_constraints.keys())
        n_pairs = len(pairs_list)
        batch_size = min(1000, n_pairs)
        
        logger.info(f"   Analyzing {n_pairs} residue pairs in batches of {batch_size}")
        
        for batch_start in range(0, n_pairs, batch_size):
            batch_end = min(batch_start + batch_size, n_pairs)
            batch_pairs = pairs_list[batch_start:batch_end]
            
            # ãƒãƒƒãƒå‡¦ç†
            batch_results = self._analyze_pair_batch(
                batch_pairs,
                anomaly_scores,
                residue_coupling,
                spatial_constraints,
                adaptive_windows,
                lag_window
            )
            
            # çµæœã‚’åˆ†é¡
            for result in batch_results:
                if result['type'] == 'causal':
                    causal_links.append(result['link'])
                elif result['type'] == 'sync':
                    sync_links.append(result['link'])
                
                # éåŒæœŸå¼·çµåˆãƒã‚§ãƒƒã‚¯
                if (result.get('has_causality', False) and 
                    abs(result.get('sync_rate', 0)) <= self.sync_threshold):
                    async_bonds.append(result['link'])
        
        # å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        causal_links = self._filter_causal_network(causal_links)
        
        return {
            'causal': causal_links,
            'sync': sync_links,
            'async': async_bonds
        }
    
    def _analyze_pair_batch(self,
                          pairs: List[Tuple[int, int]],
                          anomaly_scores: Dict[int, np.ndarray],
                          residue_coupling: np.ndarray,
                          spatial_constraints: Dict[Tuple[int, int], float],
                          adaptive_windows: Dict[int, int],
                          lag_window: int) -> List[Dict]:
        """ãƒšã‚¢ã®ãƒãƒƒãƒè§£æï¼ˆlink_typeã‚’'transition'ã«çµ±ä¸€ï¼‰"""
        results = []
        
        for res_i, res_j in pairs:
            if res_i not in anomaly_scores or res_j not in anomaly_scores:
                continue
            
            scores_i = anomaly_scores[res_i]
            scores_j = anomaly_scores[res_j]
            
            n_frames = len(scores_i)
            if n_frames < 10:
                continue
            
            # æœ€é©ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            window = (adaptive_windows.get(res_i, 100) + 
                     adaptive_windows.get(res_j, 100)) // 2
            
            # å› æœæ€§è§£æ
            max_correlation = 0.0
            optimal_lag = 0
            
            # GPUä¸Šã§ç›¸é–¢è¨ˆç®—
            if self.is_gpu:
                scores_i_gpu = self.to_gpu(scores_i)
                scores_j_gpu = self.to_gpu(scores_j)
                
                for lag in range(0, min(lag_window, len(scores_i)//2), 10):
                    if lag < len(scores_i):
                        try:
                            # å‰æ–¹å‘
                            corr = float(self.xp.corrcoef(
                                scores_i_gpu[:-lag] if lag > 0 else scores_i_gpu,
                                scores_j_gpu[lag:] if lag > 0 else scores_j_gpu
                            )[0, 1])
                            
                            if abs(corr) > abs(max_correlation):
                                max_correlation = corr
                                optimal_lag = lag
                            
                            # å¾Œæ–¹å‘
                            if lag > 0:
                                corr = float(self.xp.corrcoef(
                                    scores_i_gpu[lag:],
                                    scores_j_gpu[:-lag]
                                )[0, 1])
                                
                                if abs(corr) > abs(max_correlation):
                                    max_correlation = corr
                                    optimal_lag = -lag
                        except:
                            continue
            else:
                # CPUç‰ˆ
                for lag in range(0, min(lag_window, len(scores_i)//2), 10):
                    if lag < len(scores_i):
                        try:
                            corr = np.corrcoef(
                                scores_i[:-lag] if lag > 0 else scores_i,
                                scores_j[lag:] if lag > 0 else scores_j
                            )[0, 1]
                            
                            if abs(corr) > abs(max_correlation):
                                max_correlation = corr
                                optimal_lag = lag
                        except:
                            continue
            
            # åŒæœŸç‡
            try:
                sync_rate = float(np.corrcoef(scores_i, scores_j)[0, 1])
            except:
                sync_rate = 0.0
            
            # ãƒªãƒ³ã‚¯ä½œæˆ
            distance = spatial_constraints.get((res_i, res_j), 0.0)
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            if abs(max_correlation) > self.correlation_threshold:
                # å› æœæ–¹å‘æ±ºå®š
                if optimal_lag >= 0:
                    from_res, to_res = res_i, res_j
                else:
                    from_res, to_res = res_j, res_i
                    optimal_lag = -optimal_lag
                
                link = NetworkLink(
                    from_res=from_res,
                    to_res=to_res,
                    strength=abs(max_correlation),
                    lag=optimal_lag,
                    distance=distance,
                    sync_rate=sync_rate,
                    link_type='transition'  # v4.0: 'causal'ã‹ã‚‰å¤‰æ›´
                )
                
                results.append({
                    'type': 'causal',
                    'link': link,
                    'has_causality': True,
                    'sync_rate': sync_rate
                })
            
            # åŒæœŸãƒã‚§ãƒƒã‚¯
            if abs(sync_rate) > self.sync_threshold:
                link = NetworkLink(
                    from_res=res_i,
                    to_res=res_j,
                    strength=abs(sync_rate),
                    lag=0,
                    distance=distance,
                    sync_rate=sync_rate,
                    link_type='transition'  # v4.0: 'sync'ã‹ã‚‰å¤‰æ›´
                )
                
                results.append({
                    'type': 'sync',
                    'link': link
                })
        
        return results
    
    def _filter_causal_network(self,
                             causal_links: List[NetworkLink]) -> List[NetworkLink]:
        """å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰"""
        # å¼·åº¦ã§ã‚½ãƒ¼ãƒˆ
        causal_links.sort(key=lambda x: x.strength, reverse=True)
        
        # ä¸Šä½Nå€‹ã‚’é¸æŠ
        if len(causal_links) > self.max_causal_links:
            logger.info(f"   Filtering causal network: {len(causal_links)} â†’ "
                       f"{self.max_causal_links} links")
            causal_links = causal_links[:self.max_causal_links]
        
        # æœ€å°å¼·åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿
        causal_links = [
            link for link in causal_links 
            if link.strength >= self.min_causality_strength
        ]
        
        return causal_links
    
    def _compute_network_stats(self,
                             networks: Dict[str, List[NetworkLink]],
                             spatial_constraints: Dict) -> Dict[str, Any]:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆè¨ˆç®—ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰"""
        # æ¬¡æ•°åˆ†å¸ƒ
        in_degree = defaultdict(int)
        out_degree = defaultdict(int)
        
        for link in networks['causal']:
            out_degree[link.from_res] += 1
            in_degree[link.to_res] += 1
        
        # ãƒãƒ–æ®‹åŸº
        hubs = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:5]
        sinks = sorted(in_degree.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # å¹³å‡è·é›¢
        distances = [link.distance for link in networks['causal'] if link.distance]
        avg_distance = np.mean(distances) if distances else 0.0
        
        # ãƒ©ã‚°åˆ†å¸ƒ
        lags = [link.lag for link in networks['causal']]
        avg_lag = np.mean(lags) if lags else 0.0
        
        return {
            'n_causal': len(networks['causal']),
            'n_sync': len(networks['sync']),
            'n_async': len(networks['async']),
            'hub_residues': hubs,
            'sink_residues': sinks,
            'avg_interaction_distance': avg_distance,
            'avg_causal_lag': avg_lag,
            'n_spatial_pairs': len(spatial_constraints)
        }
    
    def _print_summary(self, result: NetworkAnalysisResult):
        """çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›ï¼ˆv4.0å¯¾å¿œï¼‰"""
        logger.info("\nğŸŒ Network Analysis Summary (v4.0):")
        logger.info(f"   Pattern: {result.network_stats.get('pattern', 'unknown')}")
        logger.info(f"   Event type: {result.network_stats.get('event_type', 'unknown')}")
        
        if result.network_stats.get('quantum_signature'):
            logger.info(f"   Quantum signature: {result.network_stats['quantum_signature']}")
        
        logger.info(f"   Causal links: {result.n_causal_links}")
        logger.info(f"   Synchronous links: {result.n_sync_links}")
        logger.info(f"   Async strong bonds: {result.n_async_bonds}")
        
        stats = result.network_stats
        if stats.get('hub_residues'):
            logger.info(f"\n   Top hub residues:")
            for res_id, degree in stats['hub_residues']:
                logger.info(f"     Residue {res_id}: {degree} outgoing links")
    
    def _create_empty_result(self) -> NetworkAnalysisResult:
        """ç©ºã®çµæœã‚’ç”Ÿæˆ"""
        return NetworkAnalysisResult(
            causal_network=[],
            sync_network=[],
            async_strong_bonds=[],
            spatial_constraints={},
            adaptive_windows={},
            network_stats={
                'error': 'No data to analyze',
                'event_type': 'NONE',
                'pattern': 'none'
            }
        )

# ===============================
# Standalone Functionsï¼ˆv4.0å¯¾å¿œï¼‰
# ===============================

def analyze_residue_network_gpu(residue_anomaly_scores: Dict[int, np.ndarray],
                              residue_coupling: np.ndarray,
                              residue_coms: Optional[np.ndarray] = None,
                              max_interaction_distance: float = 15.0,
                              **kwargs) -> NetworkAnalysisResult:
    """æ®‹åŸºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°ï¼ˆv4.0ï¼‰"""
    analyzer = ResidueNetworkGPU(
        max_interaction_distance=max_interaction_distance,
        **kwargs
    )
    return analyzer.analyze_network(
        residue_anomaly_scores, residue_coupling, residue_coms
    )

def compute_spatial_constraints_gpu(residue_ids: List[int],
                                  residue_coms: np.ndarray,
                                  max_distance: float = 15.0,
                                  backend: Optional[GPUBackend] = None) -> Dict[Tuple[int, int], float]:
    """ç©ºé–“åˆ¶ç´„è¨ˆç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    analyzer = ResidueNetworkGPU(max_interaction_distance=max_distance)
    if backend:
        analyzer.device = backend.device
        analyzer.is_gpu = backend.is_gpu
        analyzer.xp = backend.xp
    
    return analyzer._compute_spatial_constraints(residue_ids, residue_coms)

def filter_causal_network_gpu(causal_links: List[NetworkLink],
                            max_links: int = 500,
                            min_strength: float = 0.2) -> List[NetworkLink]:
    """å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    # å¼·åº¦ã§ã‚½ãƒ¼ãƒˆ
    causal_links.sort(key=lambda x: x.strength, reverse=True)
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered = []
    for link in causal_links[:max_links]:
        if link.strength >= min_strength:
            filtered.append(link)
    
    return filtered

def build_propagation_paths_gpu(initiators: List[int],
                              causal_links: List[NetworkLink],
                              max_depth: int = 5) -> List[List[int]]:
    """ä¼æ’­çµŒè·¯æ§‹ç¯‰ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
    graph = defaultdict(list)
    for link in causal_links:
        graph[link.from_res].append((link.to_res, link.strength))
    
    paths = []
    
    def dfs(current: int, path: List[int], depth: int):
        if depth >= max_depth:
            paths.append(path.copy())
            return
        
        if current in graph:
            # å¼·åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦æ¢ç´¢
            neighbors = sorted(graph[current], key=lambda x: x[1], reverse=True)
            
            for neighbor, weight in neighbors[:3]:  # ä¸Šä½3çµŒè·¯
                if neighbor not in path:
                    path.append(neighbor)
                    dfs(neighbor, path, depth + 1)
                    path.pop()
        else:
            paths.append(path.copy())
    
    # å„é–‹å§‹ç‚¹ã‹ã‚‰æ¢ç´¢
    for initiator in initiators:
        dfs(initiator, [initiator], 0)
    
    # é‡è¤‡é™¤å»ã¨é•·ã•ã§ã‚½ãƒ¼ãƒˆ
    unique_paths = []
    seen = set()
    
    for path in sorted(paths, key=len, reverse=True):
        path_tuple = tuple(path)
        if path_tuple not in seen and len(path) > 1:
            seen.add(path_tuple)
            unique_paths.append(path)
    
    return unique_paths
