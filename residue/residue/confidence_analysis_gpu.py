"""
Statistical Confidence Analysis (GPU Version)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

çµ±è¨ˆçš„ä¿¡é ¼æ€§ã‚’GPUã§é«˜é€Ÿã«è©•ä¾¡ï¼
ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã€é †åˆ—æ¤œå®šã€ä¿¡é ¼åŒºé–“ã¨ã‹å…¨éƒ¨é€Ÿã„ã‚ˆã€œï¼ğŸ’•

by ç’°ã¡ã‚ƒã‚“
"""

import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass

# GPU imports
try:
    import cupy as cp
    HAS_GPU = True
except ImportError:
    HAS_GPU = False
    cp = None

# Local imports
from ..core import GPUBackend, GPUMemoryManager, handle_gpu_errors, profile_gpu

logger = logging.getLogger('lambda3_gpu.residue.confidence')

# ===============================
# Data Classes
# ===============================

@dataclass
class ConfidenceResult:
    """ä¿¡é ¼æ€§è§£æã®çµæœ"""
    statistic_name: str           # çµ±è¨ˆé‡ã®åå‰
    point_estimate: float         # ç‚¹æ¨å®šå€¤
    ci_lower: float              # ä¿¡é ¼åŒºé–“ä¸‹é™
    ci_upper: float              # ä¿¡é ¼åŒºé–“ä¸Šé™
    confidence_level: float       # ä¿¡é ¼æ°´æº–
    n_bootstrap: int             # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—å›æ•°
    p_value: Optional[float] = None     # på€¤
    standard_error: Optional[float] = None  # æ¨™æº–èª¤å·®
    bias: Optional[float] = None         # ãƒã‚¤ã‚¢ã‚¹
    
    @property
    def ci_width(self) -> float:
        """ä¿¡é ¼åŒºé–“ã®å¹…"""
        return self.ci_upper - self.ci_lower
    
    @property
    def is_significant(self) -> bool:
        """çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆ0ã‚’å«ã¾ãªã„ï¼‰"""
        return self.ci_lower > 0 or self.ci_upper < 0

# ===============================
# CUDA Kernels
# ===============================

# ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚«ãƒ¼ãƒãƒ«
BOOTSTRAP_RESAMPLE_KERNEL = r'''
extern "C" __global__
void bootstrap_resample_kernel(
    const float* __restrict__ data,        // (n_samples,)
    float* __restrict__ resampled,        // (n_bootstrap, n_samples)
    const int* __restrict__ indices,       // (n_bootstrap, n_samples)
    const int n_samples,
    const int n_bootstrap
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = n_bootstrap * n_samples;
    
    if (idx >= total_elements) return;
    
    const int bootstrap_idx = idx / n_samples;
    const int sample_idx = idx % n_samples;
    
    // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    int source_idx = indices[idx];
    resampled[idx] = data[source_idx];
}
'''

# ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—çµ±è¨ˆé‡è¨ˆç®—ã‚«ãƒ¼ãƒãƒ«
BOOTSTRAP_STATISTIC_KERNEL = r'''
extern "C" __global__
void compute_bootstrap_statistics_kernel(
    const float* __restrict__ resampled_x,  // (n_bootstrap, n_samples)
    const float* __restrict__ resampled_y,  // (n_bootstrap, n_samples)  
    float* __restrict__ statistics,         // (n_bootstrap,)
    const int n_samples,
    const int n_bootstrap,
    const int statistic_type  // 0: correlation, 1: mean_diff, 2: regression_slope
) {
    const int b = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (b >= n_bootstrap) return;
    
    const float* x = &resampled_x[b * n_samples];
    const float* y = &resampled_y[b * n_samples];
    
    if (statistic_type == 0) {
        // ç›¸é–¢ä¿‚æ•°
        float mean_x = 0.0f, mean_y = 0.0f;
        
        // å¹³å‡
        for (int i = 0; i < n_samples; i++) {
            mean_x += x[i];
            mean_y += y[i];
        }
        mean_x /= n_samples;
        mean_y /= n_samples;
        
        // å…±åˆ†æ•£ã¨åˆ†æ•£
        float cov = 0.0f, var_x = 0.0f, var_y = 0.0f;
        for (int i = 0; i < n_samples; i++) {
            float dx = x[i] - mean_x;
            float dy = y[i] - mean_y;
            cov += dx * dy;
            var_x += dx * dx;
            var_y += dy * dy;
        }
        
        float denominator = sqrtf(var_x * var_y);
        statistics[b] = (denominator > 1e-10f) ? cov / denominator : 0.0f;
        
    } else if (statistic_type == 1) {
        // å¹³å‡å·®
        float mean_x = 0.0f, mean_y = 0.0f;
        for (int i = 0; i < n_samples; i++) {
            mean_x += x[i];
            mean_y += y[i];
        }
        statistics[b] = mean_x / n_samples - mean_y / n_samples;
        
    } else if (statistic_type == 2) {
        // å›å¸°ä¿‚æ•°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        float mean_x = 0.0f, mean_y = 0.0f;
        for (int i = 0; i < n_samples; i++) {
            mean_x += x[i];
            mean_y += y[i];
        }
        mean_x /= n_samples;
        mean_y /= n_samples;
        
        float num = 0.0f, denom = 0.0f;
        for (int i = 0; i < n_samples; i++) {
            float dx = x[i] - mean_x;
            float dy = y[i] - mean_y;
            num += dx * dy;
            denom += dx * dx;
        }
        
        statistics[b] = (denom > 1e-10f) ? num / denom : 0.0f;
    }
}
'''

# é †åˆ—æ¤œå®šã‚«ãƒ¼ãƒãƒ«
PERMUTATION_TEST_KERNEL = r'''
extern "C" __global__
void permutation_test_kernel(
    const float* __restrict__ group1,       // (n1,)
    const float* __restrict__ group2,       // (n2,)
    const int* __restrict__ perm_indices,   // (n_permutations, n1+n2)
    float* __restrict__ test_statistics,    // (n_permutations,)
    const int n1,
    const int n2,
    const int n_permutations
) {
    const int perm = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (perm >= n_permutations) return;
    
    const int n_total = n1 + n2;
    const int* indices = &perm_indices[perm * n_total];
    
    // çµåˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é †åˆ—ã«å¾“ã£ã¦ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
    float sum1 = 0.0f, sum2 = 0.0f;
    
    for (int i = 0; i < n1; i++) {
        int idx = indices[i];
        if (idx < n1) {
            sum1 += group1[idx];
        } else {
            sum1 += group2[idx - n1];
        }
    }
    
    for (int i = n1; i < n_total; i++) {
        int idx = indices[i];
        if (idx < n1) {
            sum2 += group1[idx];
        } else {
            sum2 += group2[idx - n1];
        }
    }
    
    // å¹³å‡å·®ã‚’çµ±è¨ˆé‡ã¨ã—ã¦ä½¿ç”¨
    test_statistics[perm] = sum1 / n1 - sum2 / n2;
}
'''

# ===============================
# Confidence Analyzer GPU Class
# ===============================

class ConfidenceAnalyzerGPU(GPUBackend):
    """
    çµ±è¨ˆçš„ä¿¡é ¼æ€§è§£æã®GPUå®Ÿè£…
    
    ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ã§ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã—ãŸã‚Šã€
    é †åˆ—æ¤œå®šã§på€¤ã‚’å‡ºã—ãŸã‚Šã€å…¨éƒ¨é«˜é€Ÿã«ã§ãã‚‹ã‚ˆã€œï¼
    """
    
    def __init__(self,
                 n_bootstrap: int = 1000,
                 confidence_level: float = 0.95,
                 random_seed: int = 42,
                 memory_manager: Optional[GPUMemoryManager] = None,
                 **kwargs):
        """
        Parameters
        ----------
        n_bootstrap : int
            ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«æ•°
        confidence_level : float
            ä¿¡é ¼æ°´æº–ï¼ˆ0.95 = 95%ï¼‰
        random_seed : int
            ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        super().__init__(**kwargs)
        self.n_bootstrap = n_bootstrap
        self.confidence_level = confidence_level
        self.random_seed = random_seed
        self.memory_manager = memory_manager or GPUMemoryManager()
        
        # ä¹±æ•°ç”Ÿæˆå™¨
        if self.is_gpu:
            self.rng = cp.random.RandomState(seed=random_seed)
        else:
            self.rng = np.random.RandomState(seed=random_seed)
        
        # ã‚«ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        if self.is_gpu and HAS_GPU:
            self._compile_kernels()
    
    def _compile_kernels(self):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        try:
            self.resample_kernel = cp.RawKernel(
                BOOTSTRAP_RESAMPLE_KERNEL, 'bootstrap_resample_kernel'
            )
            self.statistic_kernel = cp.RawKernel(
                BOOTSTRAP_STATISTIC_KERNEL, 'compute_bootstrap_statistics_kernel'
            )
            self.permutation_kernel = cp.RawKernel(
                PERMUTATION_TEST_KERNEL, 'permutation_test_kernel'
            )
            logger.debug("Confidence analysis kernels compiled successfully")
        except Exception as e:
            logger.warning(f"Failed to compile custom kernels: {e}")
            self.resample_kernel = None
            self.statistic_kernel = None
            self.permutation_kernel = None
    
    @handle_gpu_errors
    @profile_gpu
    def bootstrap_correlation_confidence(self,
                                       series_x: np.ndarray,
                                       series_y: np.ndarray,
                                       n_bootstrap: Optional[int] = None) -> ConfidenceResult:
        """
        ç›¸é–¢ä¿‚æ•°ã®ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ä¿¡é ¼åŒºé–“
        
        Parameters
        ----------
        series_x : np.ndarray
            ç³»åˆ—X
        series_y : np.ndarray
            ç³»åˆ—Y
        n_bootstrap : int, optional
            ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—å›æ•°
            
        Returns
        -------
        ConfidenceResult
            ä¿¡é ¼åŒºé–“ã®çµæœ
        """
        if n_bootstrap is None:
            n_bootstrap = self.n_bootstrap
        
        n_samples = len(series_x)
        
        # GPUè»¢é€
        x_gpu = self.to_gpu(series_x)
        y_gpu = self.to_gpu(series_y)
        
        # å…ƒã®ç›¸é–¢ä¿‚æ•°
        original_corr = float(self.xp.corrcoef(x_gpu, y_gpu)[0, 1])
        
        # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—
        with self.timer('bootstrap'):
            bootstrap_correlations = self._bootstrap_statistic(
                x_gpu, y_gpu, 
                lambda x, y: self.xp.corrcoef(x, y)[0, 1],
                n_bootstrap
            )
        
        # ä¿¡é ¼åŒºé–“è¨ˆç®—
        ci_lower, ci_upper = self._compute_confidence_interval(
            bootstrap_correlations, self.confidence_level
        )
        
        # ãƒã‚¤ã‚¢ã‚¹è£œæ­£
        bias = float(self.xp.mean(bootstrap_correlations)) - original_corr
        
        # æ¨™æº–èª¤å·®
        se = float(self.xp.std(bootstrap_correlations))
        
        result = ConfidenceResult(
            statistic_name='correlation',
            point_estimate=original_corr,
            ci_lower=float(ci_lower),
            ci_upper=float(ci_upper),
            confidence_level=self.confidence_level,
            n_bootstrap=n_bootstrap,
            standard_error=se,
            bias=bias
        )
        
        return result
    
    def _bootstrap_statistic(self,
                           x_gpu: Union[np.ndarray, cp.ndarray],
                           y_gpu: Union[np.ndarray, cp.ndarray],
                           statistic_func: Callable,
                           n_bootstrap: int) -> Union[np.ndarray, cp.ndarray]:
        """æ±ç”¨ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—"""
        n_samples = len(x_gpu)
        bootstrap_stats = self.zeros(n_bootstrap)
        
        # ãƒãƒƒãƒå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        batch_size = min(100, n_bootstrap)
        
        for i in range(0, n_bootstrap, batch_size):
            batch_end = min(i + batch_size, n_bootstrap)
            batch_n = batch_end - i
            
            # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆ
            indices = self.rng.randint(0, n_samples, size=(batch_n, n_samples))
            
            # ãƒãƒƒãƒã§çµ±è¨ˆé‡è¨ˆç®—
            for j in range(batch_n):
                x_resampled = x_gpu[indices[j]]
                y_resampled = y_gpu[indices[j]]
                bootstrap_stats[i + j] = statistic_func(x_resampled, y_resampled)
        
        return bootstrap_stats
    
    def _compute_confidence_interval(self,
                                   bootstrap_stats: Union[np.ndarray, cp.ndarray],
                                   confidence_level: float) -> Tuple[float, float]:
        """ä¿¡é ¼åŒºé–“è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«æ³•ï¼‰"""
        alpha = 1 - confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = self.xp.percentile(bootstrap_stats, lower_percentile)
        ci_upper = self.xp.percentile(bootstrap_stats, upper_percentile)
        
        return ci_lower, ci_upper
    
    @handle_gpu_errors
    @profile_gpu
    def permutation_test(self,
                       group1: np.ndarray,
                       group2: np.ndarray,
                       n_permutations: int = 10000,
                       alternative: str = 'two-sided') -> float:
        """
        é †åˆ—æ¤œå®šã§på€¤ã‚’è¨ˆç®—
        
        Parameters
        ----------
        group1 : np.ndarray
            ã‚°ãƒ«ãƒ¼ãƒ—1ã®ãƒ‡ãƒ¼ã‚¿
        group2 : np.ndarray
            ã‚°ãƒ«ãƒ¼ãƒ—2ã®ãƒ‡ãƒ¼ã‚¿
        n_permutations : int
            é †åˆ—æ•°
        alternative : str
            å¯¾ç«‹ä»®èª¬ï¼ˆ'two-sided', 'greater', 'less'ï¼‰
            
        Returns
        -------
        float
            på€¤
        """
        # GPUè»¢é€
        group1_gpu = self.to_gpu(group1)
        group2_gpu = self.to_gpu(group2)
        
        n1 = len(group1_gpu)
        n2 = len(group2_gpu)
        
        # è¦³æ¸¬çµ±è¨ˆé‡ï¼ˆå¹³å‡å·®ï¼‰
        observed_stat = float(self.xp.mean(group1_gpu) - self.xp.mean(group2_gpu))
        
        # çµåˆãƒ‡ãƒ¼ã‚¿
        combined = self.xp.concatenate([group1_gpu, group2_gpu])
        n_total = n1 + n2
        
        # é †åˆ—çµ±è¨ˆé‡
        perm_stats = self.zeros(n_permutations)
        
        # ãƒãƒƒãƒå‡¦ç†
        batch_size = min(1000, n_permutations)
        
        for i in range(0, n_permutations, batch_size):
            batch_end = min(i + batch_size, n_permutations)
            batch_n = batch_end - i
            
            for j in range(batch_n):
                # ãƒ©ãƒ³ãƒ€ãƒ é †åˆ—
                perm = self.rng.permutation(n_total)
                perm_group1 = combined[perm[:n1]]
                perm_group2 = combined[perm[n1:]]
                
                # çµ±è¨ˆé‡è¨ˆç®—
                perm_stats[i + j] = self.xp.mean(perm_group1) - self.xp.mean(perm_group2)
        
        # på€¤è¨ˆç®—
        if alternative == 'two-sided':
            p_value = self.xp.mean(self.xp.abs(perm_stats) >= abs(observed_stat))
        elif alternative == 'greater':
            p_value = self.xp.mean(perm_stats >= observed_stat)
        else:  # less
            p_value = self.xp.mean(perm_stats <= observed_stat)
        
        return float(p_value)
    
    @profile_gpu
    def compute_confidence_intervals(self,
                                   data: np.ndarray,
                                   statistics: List[str] = ['mean', 'std', 'median'],
                                   n_bootstrap: Optional[int] = None) -> Dict[str, ConfidenceResult]:
        """
        è¤‡æ•°çµ±è¨ˆé‡ã®ä¿¡é ¼åŒºé–“ã‚’ä¸€åº¦ã«è¨ˆç®—
        
        Parameters
        ----------
        data : np.ndarray
            ãƒ‡ãƒ¼ã‚¿
        statistics : list of str
            è¨ˆç®—ã™ã‚‹çµ±è¨ˆé‡
        n_bootstrap : int, optional
            ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—å›æ•°
            
        Returns
        -------
        dict
            çµ±è¨ˆé‡å -> ConfidenceResult
        """
        if n_bootstrap is None:
            n_bootstrap = self.n_bootstrap
        
        # GPUè»¢é€
        data_gpu = self.to_gpu(data)
        n_samples = len(data_gpu)
        
        results = {}
        
        # çµ±è¨ˆé‡é–¢æ•°ãƒãƒƒãƒ—
        stat_funcs = {
            'mean': self.xp.mean,
            'std': self.xp.std,
            'median': self.xp.median,
            'var': self.xp.var,
            'min': self.xp.min,
            'max': self.xp.max
        }
        
        for stat_name in statistics:
            if stat_name not in stat_funcs:
                logger.warning(f"Unknown statistic: {stat_name}")
                continue
            
            stat_func = stat_funcs[stat_name]
            
            # å…ƒã®çµ±è¨ˆé‡
            original_stat = float(stat_func(data_gpu))
            
            # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—
            bootstrap_stats = self.zeros(n_bootstrap)
            
            for i in range(n_bootstrap):
                # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                indices = self.rng.randint(0, n_samples, size=n_samples)
                resampled = data_gpu[indices]
                bootstrap_stats[i] = stat_func(resampled)
            
            # ä¿¡é ¼åŒºé–“
            ci_lower, ci_upper = self._compute_confidence_interval(
                bootstrap_stats, self.confidence_level
            )
            
            # çµæœä½œæˆ
            results[stat_name] = ConfidenceResult(
                statistic_name=stat_name,
                point_estimate=original_stat,
                ci_lower=float(ci_lower),
                ci_upper=float(ci_upper),
                confidence_level=self.confidence_level,
                n_bootstrap=n_bootstrap,
                standard_error=float(self.xp.std(bootstrap_stats)),
                bias=float(self.xp.mean(bootstrap_stats)) - original_stat
            )
        
        return results
    
    def evaluate_statistical_significance(self,
                                        test_statistic: float,
                                        null_distribution: np.ndarray,
                                        alternative: str = 'two-sided') -> Tuple[float, bool]:
        """
        çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’è©•ä¾¡
        
        Parameters
        ----------
        test_statistic : float
            æ¤œå®šçµ±è¨ˆé‡
        null_distribution : np.ndarray
            å¸°ç„¡åˆ†å¸ƒ
        alternative : str
            å¯¾ç«‹ä»®èª¬
            
        Returns
        -------
        p_value : float
            på€¤
        is_significant : bool
            æœ‰æ„ã‹ã©ã†ã‹ï¼ˆÎ±=0.05ï¼‰
        """
        # GPUè»¢é€
        null_dist_gpu = self.to_gpu(null_distribution)
        
        # på€¤è¨ˆç®—
        if alternative == 'two-sided':
            p_value = self.xp.mean(self.xp.abs(null_dist_gpu) >= abs(test_statistic))
        elif alternative == 'greater':
            p_value = self.xp.mean(null_dist_gpu >= test_statistic)
        else:  # less
            p_value = self.xp.mean(null_dist_gpu <= test_statistic)
        
        p_value = float(p_value)
        is_significant = p_value < 0.05
        
        return p_value, is_significant

# ===============================
# Standalone Functions
# ===============================

def bootstrap_correlation_confidence_gpu(series_x: np.ndarray,
                                       series_y: np.ndarray,
                                       n_bootstrap: int = 1000,
                                       confidence_level: float = 0.95,
                                       **kwargs) -> ConfidenceResult:
    """ç›¸é–¢ä¿‚æ•°ã®ä¿¡é ¼åŒºé–“è¨ˆç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    analyzer = ConfidenceAnalyzerGPU(
        n_bootstrap=n_bootstrap,
        confidence_level=confidence_level,
        **kwargs
    )
    return analyzer.bootstrap_correlation_confidence(series_x, series_y)

def permutation_test_gpu(group1: np.ndarray,
                       group2: np.ndarray,
                       n_permutations: int = 10000,
                       alternative: str = 'two-sided',
                       **kwargs) -> float:
    """é †åˆ—æ¤œå®šã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    analyzer = ConfidenceAnalyzerGPU(**kwargs)
    return analyzer.permutation_test(group1, group2, n_permutations, alternative)

def compute_confidence_intervals_gpu(data: np.ndarray,
                                   statistics: List[str] = ['mean', 'std'],
                                   n_bootstrap: int = 1000,
                                   confidence_level: float = 0.95,
                                   **kwargs) -> Dict[str, ConfidenceResult]:
    """è¤‡æ•°çµ±è¨ˆé‡ã®ä¿¡é ¼åŒºé–“è¨ˆç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    analyzer = ConfidenceAnalyzerGPU(
        n_bootstrap=n_bootstrap,
        confidence_level=confidence_level,
        **kwargs
    )
    return analyzer.compute_confidence_intervals(data, statistics)

def evaluate_statistical_significance_gpu(test_statistic: float,
                                        null_distribution: np.ndarray,
                                        alternative: str = 'two-sided',
                                        backend: Optional[GPUBackend] = None) -> Tuple[float, bool]:
    """çµ±è¨ˆçš„æœ‰æ„æ€§è©•ä¾¡ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    analyzer = ConfidenceAnalyzerGPU()
    if backend:
        analyzer.device = backend.device
        analyzer.is_gpu = backend.is_gpu
        analyzer.xp = backend.xp
    
    return analyzer.evaluate_statistical_significance(
        test_statistic, null_distribution, alternative
    )

# ===============================
# Utility Functions
# ===============================

def create_null_distribution_gpu(data: np.ndarray,
                               statistic_func: Callable,
                               n_samples: int = 10000,
                               backend: Optional[GPUBackend] = None) -> np.ndarray:
    """
    å¸°ç„¡åˆ†å¸ƒã‚’ç”Ÿæˆï¼ˆãƒ©ãƒ³ãƒ€ãƒ åŒ–ã§ï¼‰
    
    Parameters
    ----------
    data : np.ndarray
        å…ƒãƒ‡ãƒ¼ã‚¿
    statistic_func : callable
        çµ±è¨ˆé‡é–¢æ•°
    n_samples : int
        ã‚µãƒ³ãƒ—ãƒ«æ•°
        
    Returns
    -------
    np.ndarray
        å¸°ç„¡åˆ†å¸ƒ
    """
    backend = backend or GPUBackend()
    data_gpu = backend.to_gpu(data)
    null_dist = backend.zeros(n_samples)
    
    rng = cp.random.RandomState(seed=42) if backend.is_gpu else np.random.RandomState(seed=42)
    
    for i in range(n_samples):
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        shuffled = data_gpu.copy()
        rng.shuffle(shuffled)
        
        # çµ±è¨ˆé‡è¨ˆç®—
        null_dist[i] = statistic_func(shuffled)
    
    return backend.to_cpu(null_dist)
