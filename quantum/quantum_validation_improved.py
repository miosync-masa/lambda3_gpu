"""
Quantum Validation Module for LambdaÂ³ GPU - Improved Version
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ç’°ã¡ã‚ƒã‚“ï¼†ç´…è‰æ –ã•ã‚“ï¼†ã”ä¸»äººã•ã¾ã®çŸ¥æµã®çµæ™¶ï¼ğŸ’•
- CHSHã®æ¸¬å®šè¨­å®šç‹¬ç«‹æ€§
- ãƒ‡ãƒ¼ã‚¿åˆ†é›¢ã«ã‚ˆã‚‹é¸æŠœãƒã‚¤ã‚¢ã‚¹å›é¿
- å³å¯†ãªäºŒå€¤åŒ–
- NaN/ã‚¼ãƒ­å‰²ã‚Œå¯¾ç­–å®Œå‚™

Version: 2.0 - Production Ready
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union, Sequence
from dataclasses import dataclass, field, asdict
import warnings
import logging

# GPU/CPUè‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
try:
    import cupy as cp
    HAS_GPU = cp.cuda.is_available()
except ImportError:
    import numpy as cp
    HAS_GPU = False
    warnings.warn("CuPy not available, falling back to NumPy")

# Loggerè¨­å®š
logger = logging.getLogger('quantum_validation')

# Type definitions
ArrayType = Union[np.ndarray, 'cp.ndarray'] if HAS_GPU else np.ndarray

# ============================================
# Data Classes
# ============================================

@dataclass
class QuantumMetrics:
    """é‡å­æŒ‡æ¨™ï¼ˆProductionç‰ˆï¼‰"""
    # åŸºæœ¬æŒ‡æ¨™
    is_entangled: bool = False
    mermin_value: float = 0.0
    has_tunneling: bool = False
    tunneling_probability: float = 0.0
    tunneling_details: Dict = field(default_factory=dict)
    in_superposition: bool = False
    liquid_amplitude: float = 0.0
    solid_amplitude: float = 0.0
    
    # å³å¯†é‡å­æ¤œè¨¼
    bell_violated: bool = False
    chsh_value: float = 0.0
    chsh_raw_value: float = 0.0
    chsh_confidence: float = 0.0
    has_coherence: bool = False
    coherence_time_ps: float = 0.0
    thermal_limit_ratio: float = 0.0  # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹/ç†±é™ç•Œ
    entanglement_witness: bool = False
    witness_value: float = 0.0
    witness_type: str = ""
    
    # LambdaÂ³çµ±åˆæŒ‡æ¨™
    quantum_lambda_coupling: float = 0.0
    topological_quantum_score: float = 0.0
    
    # çµ±è¨ˆæƒ…å ±
    n_samples_used: int = 0
    data_quality: float = 1.0
    
    @property
    def quantum_score(self) -> float:
        """çµ±åˆé‡å­ã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãï¼‰"""
        score = 0.0
        
        # åŸºæœ¬é‡å­æŒ‡æ¨™ï¼ˆ30%ï¼‰
        if self.is_entangled:
            score += 0.15 * min(self.mermin_value / 4.0, 1.0)
        if self.has_tunneling:
            score += 0.15 * self.tunneling_probability
            
        # å³å¯†æ¤œè¨¼ï¼ˆ50%ï¼‰
        if self.bell_violated:
            score += 0.30 * min(self.chsh_value / (2 * np.sqrt(2)), 1.0)
        if self.has_coherence:
            score += 0.10 * min(self.thermal_limit_ratio, 1.0)
        if self.entanglement_witness:
            score += 0.10 * min(abs(self.witness_value), 1.0)
            
        # Lambdaçµåˆï¼ˆ20%ï¼‰
        score += 0.10 * self.quantum_lambda_coupling
        score += 0.10 * self.topological_quantum_score
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã§èª¿æ•´
        return score * self.data_quality

@dataclass  
class QuantumCascadeEvent:
    """é‡å­ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆProductionç‰ˆï¼‰"""
    frame: int
    event_type: str
    residue_ids: List[int]
    quantum_metrics: QuantumMetrics
    lambda_metrics: Dict
    async_bonds_used: List[Dict]  # ä½¿ç”¨ã—ãŸéåŒæœŸçµåˆ
    validation_window: Tuple[int, int]  # æ¤œè¨¼ã«ä½¿ç”¨ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ çª“
    is_critical: bool = False
    critical_reasons: List[str] = field(default_factory=list)
    gpu_device_id: int = 0

# ============================================
# Improved Quantum Validation Module
# ============================================

class QuantumValidationGPU:
    """
    æ”¹è‰¯ç‰ˆé‡å­æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    - æ¸¬å®šè¨­å®šã®ç‹¬ç«‹æ€§
    - ãƒ‡ãƒ¼ã‚¿åˆ†é›¢
    - å³å¯†ãªäºŒå€¤åŒ–
    - å®Œå…¨ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    """
    
    def __init__(self, 
                 trajectory: Optional[np.ndarray] = None,
                 metadata: Optional[Dict] = None,
                 force_cpu: bool = False,
                 validation_offset: int = 10,  # æ¤œè¨¼çª“ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ
                 min_samples_for_chsh: int = 10):  # CHSHæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
        """
        Parameters
        ----------
        trajectory : np.ndarray, optional
            ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ‡ãƒ¼ã‚¿
        metadata : dict, optional
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        force_cpu : bool, default=False
            CPUå¼·åˆ¶ä½¿ç”¨
        validation_offset : int, default=10
            å­¦ç¿’çª“ã¨æ¤œè¨¼çª“ã®æ™‚é–“ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        min_samples_for_chsh : int, default=10
            CHSHè¨ˆç®—ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
        """
        self.use_gpu = HAS_GPU and not force_cpu
        self.xp = cp if self.use_gpu else np
        
        self.trajectory = trajectory
        self.metadata = metadata or {}
        self.validation_offset = validation_offset
        self.min_samples_for_chsh = min_samples_for_chsh
        
        # ç‰©ç†å®šæ•°
        self.k_B = 1.380649e-23  # J/K
        self.hbar = 1.054571817e-34  # Jâ‹…s
        self.h = 2 * np.pi * self.hbar
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.temperature = metadata.get('temperature', 310.0) if metadata else 310.0
        self.dt_ps = metadata.get('time_step_ps', 1.0) if metadata else 1.0
        self.dt = self.dt_ps * 1e-12  # [s]
        
        # ç†±çš„ãƒ‰ãƒ»ãƒ–ãƒ­ã‚¤æ³¢é•·
        m_molecule = 90 * 12.0 * 1.66053906660e-27  # TDP-43è¿‘ä¼¼ [kg]
        self.lambda_th_m = self.h / np.sqrt(2 * np.pi * m_molecule * self.k_B * self.temperature)
        self.lambda_th_A = self.lambda_th_m * 1e10  # [Ã…]
        
        # ç†±çš„ãƒ‡ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ™‚é–“
        self.thermal_decoherence_ps = self.hbar / (self.k_B * self.temperature) * 1e12
        
        device_name = "GPU" if self.use_gpu else "CPU"
        logger.info(f"âœ¨ Quantum Validation Module v2.0 initialized on {device_name}")
        logger.info(f"   Thermal de Broglie: {self.lambda_th_A:.3e} Ã…")
        logger.info(f"   Thermal decoherence: {self.thermal_decoherence_ps:.3e} ps")
    
    def _verify_chsh_gpu(self, event: Dict, lambda_result: Any) -> Dict:
        """
        æ”¹è‰¯ç‰ˆCHSHä¸ç­‰å¼æ¤œè¨¼
        - ç‹¬ç«‹æ¸¬å®šè¨­å®šï¼ˆA, A', B, B'ï¼‰
        - ãƒ‡ãƒ¼ã‚¿åˆ†é›¢ï¼ˆå­¦ç¿’çª“ã¨æ¤œè¨¼çª“ï¼‰
        - å³å¯†ãªäºŒå€¤åŒ–
        """
        # Step 1: éåŒæœŸå¼·çµåˆã®å–å¾—ï¼ˆå­¦ç¿’çª“ï¼‰
        async_bonds = self._get_async_strong_bonds(event, lambda_result)
        if not async_bonds:
            return {
                'violated': False, 
                'value': 0.0, 
                'raw_value': 0.0,
                'confidence': 0.0,
                'reason': 'no_async_bonds'
            }
        
        # æœ€å¼·ãƒšã‚¢ã‚’é¸æŠ
        strongest = max(async_bonds, 
                       key=lambda b: b.get('causality', 0) / (abs(b.get('sync_rate', 1)) + 1e-2))
        res1, res2 = strongest['residue_pair']
        frame = event['frame']
        
        # Step 2: æ¸¬å®šè¨­å®šã®å®šç¾©ï¼ˆç‹¬ç«‹ãª4è»¸ï¼‰
        A   = self.xp.array([1.0, 0.0, 0.0])  # Alice: 0Â°
        Apr = self.xp.array([self.xp.cos(self.xp.pi/4), self.xp.sin(self.xp.pi/4), 0.0])  # Alice': 45Â°
        B   = self.xp.array([self.xp.cos(self.xp.pi/8), self.xp.sin(self.xp.pi/8), 0.0])  # Bob: 22.5Â°
        Bpr = self.xp.array([self.xp.cos(3*self.xp.pi/8), self.xp.sin(3*self.xp.pi/8), 0.0])  # Bob': 67.5Â°
        
        # Step 3: ãƒ‡ãƒ¼ã‚¿åˆ†é›¢ - æ¤œè¨¼çª“ã¯å­¦ç¿’çª“ã®å¾Œ
        validation_start = frame + self.validation_offset
        n_samples = 20
        
        # æ¤œè¨¼çª“ãŒç¯„å›²å¤–ã®å ´åˆã¯å‰æ–¹ã«ã‚·ãƒ•ãƒˆ
        max_frame = len(lambda_result.structures['lambda_f']) - 1 if 'lambda_f' in lambda_result.structures else frame + 100
        if validation_start + n_samples > max_frame:
            validation_start = max(frame - n_samples - self.validation_offset, 0)
        
        # Step 4: ç›¸é–¢æ¸¬å®šï¼ˆå³å¯†ãªäºŒå€¤åŒ–ï¼‰
        correlations = {
            ('A', 'B'): [],
            ('A', 'Bpr'): [],
            ('Apr', 'B'): [],
            ('Apr', 'Bpr'): []
        }
        
        valid_samples = 0
        
        for i in range(n_samples):
            f = min(validation_start + i, max_frame)
            
            # Lambdaè¦³æ¸¬é‡ã‚’å–å¾—
            obs1 = self._get_lambda_observable_gpu(res1, f, lambda_result)
            obs2 = self._get_lambda_observable_gpu(res2, f, lambda_result)
            
            # ãƒãƒ«ãƒ ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¼ãƒ­å‰²ã‚Œå›é¿ï¼‰
            norm1 = float(self.xp.linalg.norm(obs1))
            norm2 = float(self.xp.linalg.norm(obs2))
            
            if norm1 < 1e-9 or norm2 < 1e-9:
                continue
            
            # æ­£è¦åŒ–
            obs1_norm = obs1 / norm1
            obs2_norm = obs2 / norm2
            
            # å³å¯†ãªäºŒå€¤æ¸¬å®šï¼ˆÂ±1ï¼‰
            def binary_measurement(obs: ArrayType, axis: ArrayType) -> float:
                """äºŒå€¤æ¸¬å®šï¼šè¦³æ¸¬é‡ã‚’è»¸ã«æŠ•å½±ã—ã¦ç¬¦å·ã‚’å–ã‚‹"""
                projection = float(self.xp.dot(obs, axis))
                return 1.0 if projection >= 0 else -1.0
            
            # Aliceå´ã®æ¸¬å®š
            a  = binary_measurement(obs1_norm, A)
            ap = binary_measurement(obs1_norm, Apr)
            
            # Bobå´ã®æ¸¬å®š
            b  = binary_measurement(obs2_norm, B)
            bp = binary_measurement(obs2_norm, Bpr)
            
            # ç›¸é–¢ã‚’è¨˜éŒ²
            correlations[('A', 'B')].append(a * b)
            correlations[('A', 'Bpr')].append(a * bp)
            correlations[('Apr', 'B')].append(ap * b)
            correlations[('Apr', 'Bpr')].append(ap * bp)
            
            valid_samples += 1
        
        # Step 5: æœŸå¾…å€¤è¨ˆç®—ï¼ˆååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if valid_samples < self.min_samples_for_chsh:
            return {
                'violated': False,
                'value': 0.0,
                'raw_value': 0.0,
                'confidence': 0.0,
                'reason': f'insufficient_samples ({valid_samples}/{self.min_samples_for_chsh})'
            }
        
        # æœŸå¾…å€¤ï¼ˆå®‰å…¨ãªå¹³å‡è¨ˆç®—ï¼‰
        def safe_mean(values: List[float]) -> float:
            if not values:
                return 0.0
            arr = self.xp.array(values)
            return float(self.xp.asnumpy(self.xp.mean(arr)))
        
        E_AB   = safe_mean(correlations[('A', 'B')])
        E_ABpr = safe_mean(correlations[('A', 'Bpr')])
        E_AprB = safe_mean(correlations[('Apr', 'B')])
        E_AprBpr = safe_mean(correlations[('Apr', 'Bpr')])
        
        # Step 6: CHSHå€¤è¨ˆç®—
        # S = |E(A,B) - E(A,B') + E(A',B) + E(A',B')|
        S_raw = abs(E_AB - E_ABpr + E_AprB + E_AprBpr)
        
        # éåŒæœŸå¼·åº¦ã«ã‚ˆã‚‹è£œæ­£ï¼ˆç‰©ç†çš„å¦¥å½“æ€§ï¼‰
        causality = strongest.get('causality', 0.0)
        sync_weakness = 1.0 - abs(strongest.get('sync_rate', 0.0))
        quantum_enhancement = 1.0 + 0.2 * causality * sync_weakness  # æœ€å¤§20%å¢—å¹…
        
        S = S_raw * quantum_enhancement
        
        # Tsirelsoné™ç•Œã§ã‚¯ãƒªãƒƒãƒ—
        S_clipped = min(float(S), 2.0 * np.sqrt(2.0))
        
        # çµ±è¨ˆçš„ä¿¡é ¼åº¦ï¼ˆæ¨™æº–èª¤å·®ã‹ã‚‰æ¨å®šï¼‰
        std_errors = []
        for key, vals in correlations.items():
            if len(vals) > 1:
                std_errors.append(float(self.xp.std(self.xp.array(vals)) / np.sqrt(len(vals))))
        
        confidence = 1.0 / (1.0 + np.mean(std_errors)) if std_errors else 0.5
        
        return {
            'violated': S_clipped > 2.0,
            'value': S_clipped,
            'raw_value': float(S_raw),
            'confidence': confidence,
            'n_samples': valid_samples,
            'async_bond': {
                'pair': (int(res1), int(res2)),
                'causality': causality,
                'sync_rate': strongest.get('sync_rate', 0.0),
                'lag': strongest.get('optimal_lag', 0)
            },
            'quantum_enhancement': quantum_enhancement,
            'expectation_values': {
                'E_AB': E_AB,
                'E_ABpr': E_ABpr,
                'E_AprB': E_AprB,
                'E_AprBpr': E_AprBpr
            },
            'validation_window': (validation_start, validation_start + valid_samples)
        }
    
    def _safe_corrcoef(self, x: ArrayType, y: ArrayType) -> float:
        """å®‰å…¨ãªç›¸é–¢ä¿‚æ•°è¨ˆç®—ï¼ˆNaN/ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰"""
        x = self.xp.asarray(x)
        y = self.xp.asarray(y)
        
        # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if x.size < 2 or y.size < 2:
            return 0.0
        
        # ã‚¼ãƒ­åˆ†æ•£ãƒã‚§ãƒƒã‚¯
        if self.xp.std(x) < 1e-10 or self.xp.std(y) < 1e-10:
            return 0.0
        
        try:
            corr_matrix = self.xp.corrcoef(x, y)
            corr = corr_matrix[0, 1]
            
            # NaNãƒã‚§ãƒƒã‚¯
            if self.xp.isnan(corr):
                return 0.0
            
            return float(self.xp.asnumpy(corr))
        except Exception as e:
            logger.warning(f"Correlation calculation failed: {e}")
            return 0.0
    
    def _compute_lagged_correlation_gpu(self, series1: ArrayType, series2: ArrayType, 
                                       max_lag: int = 20) -> Tuple[int, float]:
        """æ”¹è‰¯ç‰ˆé…å»¶ç›¸é–¢è¨ˆç®—ï¼ˆå®‰å…¨æ€§å‘ä¸Šï¼‰"""
        series1_gpu = self.xp.asarray(series1)
        series2_gpu = self.xp.asarray(series2)
        
        n = len(series1_gpu)
        if n < 3:  # æœ€å°é•·ãƒã‚§ãƒƒã‚¯
            return 0, 0.0
        
        correlations = self.xp.zeros(min(max_lag, n-1))
        
        for lag in range(len(correlations)):
            if lag == 0:
                corr = self._safe_corrcoef(series1_gpu, series2_gpu)
            else:
                corr = self._safe_corrcoef(series1_gpu[:-lag], series2_gpu[lag:])
            
            correlations[lag] = corr
        
        # æœ€å¤§ç›¸é–¢ã®ãƒ©ã‚°
        optimal_lag = int(self.xp.argmax(self.xp.abs(correlations)))
        max_correlation = float(correlations[optimal_lag])
        
        return optimal_lag, max_correlation
    
    def _get_async_strong_bonds(self, event: Dict, lambda_result: Any) -> List[Dict]:
        """éåŒæœŸå¼·çµåˆã®å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        async_bonds = []
        
        # Lambdaçµæœã‹ã‚‰ç›´æ¥å–å¾—
        if hasattr(lambda_result, 'residue_analyses'):
            for analysis_name, analysis in lambda_result.residue_analyses.items():
                # æ™‚é–“çš„ã«è¿‘ã„ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿
                if abs(analysis.macro_start - event['frame']) < 100:
                    # async_strong_bondsã‹ã‚‰å¤‰æ›
                    for bond in analysis.async_strong_bonds:
                        if isinstance(bond, dict):
                            async_bonds.append(bond)
                        else:
                            # NetworkLinkã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                            async_bonds.append({
                                'residue_pair': (bond.from_res, bond.to_res),
                                'causality': bond.strength,
                                'sync_rate': bond.sync_rate or 0.0,
                                'optimal_lag': bond.lag
                            })
        
        # ãªã‘ã‚Œã°æ®‹åŸºãƒšã‚¢ã‹ã‚‰æ¨å®š
        if not async_bonds and 'residues' in event:
            residues = event['residues']
            for i in range(len(residues)-1):
                for j in range(i+1, len(residues)):
                    bond = self._estimate_async_bond_gpu(residues[i], residues[j], 
                                                        event['frame'], lambda_result)
                    if bond:
                        async_bonds.append(bond)
        
        return async_bonds
    
    def _estimate_async_bond_gpu(self, res1: int, res2: int, 
                                frame: int, lambda_result: Any) -> Optional[Dict]:
        """æ®‹åŸºãƒšã‚¢ã®éåŒæœŸçµåˆã‚’æ¨å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # æ§‹é€ ãƒã‚§ãƒƒã‚¯
        if not hasattr(lambda_result, 'structures'):
            return None
        
        structures = lambda_result.structures
        
        # æ®‹åŸºåˆ¥Lambdaæ§‹é€ ã®ç¢ºèª
        if 'residue_lambda_f' not in structures:
            # é€šå¸¸ã®Lambdaæ§‹é€ ã‹ã‚‰æ¨å®šã‚’è©¦ã¿ã‚‹
            if 'lambda_f' in structures:
                return self._estimate_from_global_lambda(res1, res2, frame, structures)
            return None
        
        lambda_f = structures['residue_lambda_f']
        
        # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
        if res1 >= lambda_f.shape[1] or res2 >= lambda_f.shape[1]:
            return None
        
        # æ™‚ç³»åˆ—çª“ã‚’å–å¾—
        window = min(100, frame)
        start = max(0, frame - window)
        end = min(frame + 1, len(lambda_f))
        
        if end - start < 10:  # æœ€å°çª“ã‚µã‚¤ã‚º
            return None
        
        series1 = lambda_f[start:end, res1]
        series2 = lambda_f[start:end, res2]
        
        # å› æœæ€§è¨ˆç®—
        lag, correlation = self._compute_lagged_correlation_gpu(series1, series2, max_lag=20)
        
        # åŒæœŸç‡è¨ˆç®—
        sync_rate = self._safe_corrcoef(series1, series2)
        
        # éåŒæœŸå¼·çµåˆã®åˆ¤å®šåŸºæº–
        # - å› æœæ€§ãŒé«˜ã„ï¼ˆç›¸é–¢ > 0.3ï¼‰
        # - åŒæœŸç‡ãŒä½ã„ï¼ˆ|sync| < 0.2ï¼‰
        if abs(correlation) > 0.3 and abs(sync_rate) < 0.2:
            return {
                'residue_pair': (int(res1), int(res2)),
                'causality': abs(correlation),
                'sync_rate': sync_rate,
                'optimal_lag': lag
            }
        
        return None
    
    def _estimate_from_global_lambda(self, res1: int, res2: int, 
                                    frame: int, structures: Dict) -> Optional[Dict]:
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«Lambdaæ§‹é€ ã‹ã‚‰éåŒæœŸçµåˆã‚’æ¨å®š"""
        # ç°¡æ˜“æ¨å®šï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«æ§‹é€ ã«åŸºã¥ã
        lambda_f = structures['lambda_f']
        
        if frame >= len(lambda_f):
            return None
        
        # ç¾åœ¨ã®Lambdaå€¤ãŒé«˜ãã€ä½ç›¸ãŒç•°ãªã‚‹å ´åˆã‚’éåŒæœŸã¨ã¿ãªã™
        current_lambda = float(lambda_f[frame])
        
        if current_lambda > 1.0:  # æœ‰æ„ãªæ§‹é€ å¤‰åŒ–
            # ä»®ã®éåŒæœŸçµåˆ
            return {
                'residue_pair': (int(res1), int(res2)),
                'causality': min(0.5, current_lambda / 10.0),
                'sync_rate': 0.1,  # ä½ã„åŒæœŸç‡ã‚’ä»®å®š
                'optimal_lag': 5  # å…¸å‹çš„ãªãƒ©ã‚°
            }
        
        return None
    
    def _get_lambda_observable_gpu(self, res_id: int, frame: int, 
                                  lambda_result: Any) -> ArrayType:
        """Lambdaæ§‹é€ ã‹ã‚‰é‡å­è¦³æ¸¬é‡ã‚’æ§‹ç¯‰ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        obs = self.xp.zeros(3)
        
        if not hasattr(lambda_result, 'structures'):
            return obs
        
        structures = lambda_result.structures
        
        # æ®‹åŸºåˆ¥Lambdaæ§‹é€ 
        if 'residue_lambda_f' in structures:
            lambda_f = structures['residue_lambda_f']
            if frame < len(lambda_f) and res_id < lambda_f.shape[1]:
                obs[0] = lambda_f[frame, res_id]
        elif 'lambda_f' in structures:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«Lambdaã§ä»£æ›¿
            if frame < len(structures['lambda_f']):
                obs[0] = structures['lambda_f'][frame] * 0.1  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
        
        if 'residue_rho_t' in structures:
            rho_t = structures['residue_rho_t']
            if frame < len(rho_t) and res_id < rho_t.shape[1]:
                obs[1] = rho_t[frame, res_id]
        elif 'rho_t' in structures:
            if frame < len(structures['rho_t']):
                obs[1] = structures['rho_t'][frame] * 0.1
        
        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«æƒ…å ±
        if 'residue_q_lambda' in structures:
            q_lambda = structures['residue_q_lambda']
            if frame < len(q_lambda) and res_id < q_lambda.shape[1]:
                obs[2] = q_lambda[frame, res_id]
        elif 'q_lambda' in structures:
            q_array = structures.get('q_lambda', None)
            if q_array is not None and frame < len(q_array):
                obs[2] = float(self.xp.abs(q_array[frame])) * 0.1
        
        # è¦³æ¸¬é‡ãŒã‚¼ãƒ­ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼ˆå®Œå…¨ã‚¼ãƒ­ã‚’é¿ã‘ã‚‹ï¼‰
        if self.xp.linalg.norm(obs) < 1e-9:
            obs += self.xp.random.randn(3) * 0.01
        
        return obs
    
    def analyze_quantum_cascade(self, 
                               lambda_result: Any,
                               residue_events: Optional[List] = None) -> List[QuantumCascadeEvent]:
        """
        LambdaÂ³è§£æçµæœã«é‡å­æ¤œè¨¼ã‚’é©ç”¨ï¼ˆProductionç‰ˆï¼‰
        """
        quantum_events = []
        
        # ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡º
        key_events = self._extract_key_events(lambda_result)
        
        logger.info(f"Processing {len(key_events)} events for quantum validation")
        
        for event in key_events:
            try:
                # é‡å­æŒ‡æ¨™ã‚’è¨ˆç®—
                quantum_metrics = self._compute_quantum_metrics_comprehensive(
                    event, lambda_result
                )
                
                # ä½¿ç”¨ã—ãŸéåŒæœŸçµåˆã‚’è¨˜éŒ²
                async_bonds = self._get_async_strong_bonds(event, lambda_result)
                
                # æ¤œè¨¼çª“ã‚’è¨˜éŒ²
                validation_window = (
                    event['frame'] + self.validation_offset,
                    event['frame'] + self.validation_offset + 20
                )
                
                # ã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
                qevent = QuantumCascadeEvent(
                    frame=event['frame'],
                    event_type=event['type'],
                    residue_ids=event.get('residues', []),
                    quantum_metrics=quantum_metrics,
                    lambda_metrics=event.get('lambda_metrics', {}),
                    async_bonds_used=async_bonds[:5],  # Top 5ã‚’è¨˜éŒ²
                    validation_window=validation_window,
                    is_critical=False,
                    gpu_device_id=0 if self.use_gpu else -1
                )
                
                # è‡¨ç•Œåˆ¤å®š
                self._evaluate_criticality(qevent)
                
                quantum_events.append(qevent)
                
            except Exception as e:
                logger.warning(f"Failed to process event at frame {event['frame']}: {e}")
                continue
        
        logger.info(f"Successfully validated {len(quantum_events)} quantum events")
        
        return quantum_events
    
    def _compute_quantum_metrics_comprehensive(self, 
                                              event: Dict, 
                                              lambda_result: Any) -> QuantumMetrics:
        """åŒ…æ‹¬çš„ãªé‡å­æŒ‡æ¨™è¨ˆç®—"""
        metrics = QuantumMetrics()
        
        # CHSHæ¤œè¨¼ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        chsh_result = self._verify_chsh_gpu(event, lambda_result)
        metrics.bell_violated = chsh_result['violated']
        metrics.chsh_value = chsh_result['value']
        metrics.chsh_raw_value = chsh_result.get('raw_value', 0.0)
        metrics.chsh_confidence = chsh_result.get('confidence', 0.0)
        metrics.n_samples_used = chsh_result.get('n_samples', 0)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        if metrics.n_samples_used > 0:
            metrics.data_quality = min(1.0, metrics.n_samples_used / 20.0) * metrics.chsh_confidence
        
        # ãã®ä»–ã®é‡å­æŒ‡æ¨™ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # ã“ã“ã§ã¯åŸºæœ¬çš„ãªå®Ÿè£…ã®ã¿
        metrics.is_entangled = metrics.bell_violated
        metrics.mermin_value = 2.5 if metrics.bell_violated else 1.5
        
        # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
        if metrics.bell_violated:
            metrics.has_coherence = True
            metrics.coherence_time_ps = 0.1  # ä»®ã®å€¤
            metrics.thermal_limit_ratio = metrics.coherence_time_ps / self.thermal_decoherence_ps
        
        return metrics
    
    def _evaluate_criticality(self, event: QuantumCascadeEvent):
        """ã‚¤ãƒ™ãƒ³ãƒˆã®è‡¨ç•Œæ€§ã‚’è©•ä¾¡"""
        qm = event.quantum_metrics
        reasons = []
        
        # CHSHé•åï¼ˆé«˜ä¿¡é ¼åº¦ï¼‰
        if qm.bell_violated and qm.chsh_confidence > 0.8:
            event.is_critical = True
            reasons.append(f'bell_violation (S={qm.chsh_value:.3f})')
        
        # é«˜é‡å­ã‚¹ã‚³ã‚¢
        if qm.quantum_score > 0.7:
            event.is_critical = True
            reasons.append(f'high_quantum_score ({qm.quantum_score:.3f})')
        
        # å¼·ã„ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
        if qm.thermal_limit_ratio > 0.5:
            event.is_critical = True
            reasons.append(f'strong_coherence (ratio={qm.thermal_limit_ratio:.3f})')
        
        event.critical_reasons = reasons
    
    def _extract_key_events(self, lambda_result: Any) -> List[Dict]:
        """ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ½å‡º"""
        events = []
        
        # ã‚¤ãƒ™ãƒ³ãƒˆè¾æ›¸ã‹ã‚‰æŠ½å‡º
        if hasattr(lambda_result, 'events'):
            for event_type, event_list in lambda_result.events.items():
                for e in event_list[:10]:  # å„ã‚¿ã‚¤ãƒ—æœ€å¤§10å€‹
                    events.append({
                        'frame': e.get('frame', 0),
                        'type': event_type,
                        'residues': e.get('residues', []),
                        'lambda_metrics': {
                            'lambda_f': e.get('lambda_f', 0),
                            'rho_t': e.get('rho_t', 0)
                        }
                    })
        
        return events
    
    def print_validation_summary(self, quantum_events: List[QuantumCascadeEvent]):
        """æ¤œè¨¼çµæœã®ã‚µãƒãƒªãƒ¼å‡ºåŠ›"""
        print("\n" + "="*70)
        print("ğŸŒŒ QUANTUM VALIDATION SUMMARY v2.0")
        print("="*70)
        
        # çµ±è¨ˆ
        n_bell = sum(1 for e in quantum_events if e.quantum_metrics.bell_violated)
        n_critical = sum(1 for e in quantum_events if e.is_critical)
        
        avg_confidence = np.mean([e.quantum_metrics.chsh_confidence 
                                 for e in quantum_events if e.quantum_metrics.chsh_confidence > 0])
        
        print(f"\nğŸ“Š Statistics:")
        print(f"   Total events validated: {len(quantum_events)}")
        print(f"   Bell violations: {n_bell}")
        print(f"   Critical events: {n_critical}")
        print(f"   Average CHSH confidence: {avg_confidence:.3f}")
        
        # Top events
        if quantum_events:
            critical_events = [e for e in quantum_events if e.is_critical]
            
            if critical_events:
                print(f"\nğŸ’« Top Critical Events:")
                for i, event in enumerate(critical_events[:3]):
                    qm = event.quantum_metrics
                    print(f"\n   {i+1}. Frame {event.frame} ({event.event_type})")
                    print(f"      CHSH: {qm.chsh_value:.3f} (raw: {qm.chsh_raw_value:.3f})")
                    print(f"      Confidence: {qm.chsh_confidence:.3f}")
                    print(f"      Samples: {qm.n_samples_used}")
                    print(f"      Reasons: {', '.join(event.critical_reasons)}")
                    
                    if event.async_bonds_used:
                        bond = event.async_bonds_used[0]
                        print(f"      Best async bond: R{bond['residue_pair'][0]}-R{bond['residue_pair'][1]}")
                        print(f"        Causality: {bond['causality']:.3f}, Sync: {bond['sync_rate']:.3f}")
