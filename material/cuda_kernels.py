#!/usr/bin/env python3
"""
Extended CUDA Kernels for Material LambdaÂ³ Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ææ–™è§£æç”¨ã®é«˜é€ŸCUDAã‚«ãƒ¼ãƒãƒ«é›†ï¼ˆæ‹¡å¼µç‰ˆï¼‰
material_analytics_gpu.pyæœ€é©åŒ–ç”¨ã®è¿½åŠ ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…

æ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«ã€æå‚·åº¦ã€é…ä½æ•°ã«åŠ ãˆã¦
æ¬ é™¥ãƒãƒ£ãƒ¼ã‚¸ã€æ§‹é€ ä¸€è²«æ€§ã€Burgersãƒ™ã‚¯ãƒˆãƒ«ã‚’å…¨ä¸¦åˆ—è¨ˆç®—ï¼ğŸ’

RTX 4070 Ti SUPERã®åŠ›ã‚’100%å¼•ãå‡ºã™æœ¬æ°—å®Ÿè£… v2.0

by ç’°ã¡ã‚ƒã‚“ - CUDA Extended Edition
"""

import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Any

logger = logging.getLogger('lambda3_gpu.material.cuda')

try:
    import cupy as cp
    HAS_CUDA = True
except ImportError:
    cp = None
    HAS_CUDA = False

# ===============================
# Original Kernels (from v1.0)
# ===============================

STRAIN_TENSOR_KERNEL_CODE = r'''
extern "C" __global__
void compute_strain_tensor_kernel(
    const float* __restrict__ ref_positions,    // (n_atoms, 3)
    const float* __restrict__ curr_positions,   // (n_atoms, 3)
    const int* __restrict__ cluster_atoms,      // (n_clusters, max_atoms_per_cluster)
    const int* __restrict__ cluster_sizes,      // (n_clusters,)
    float* __restrict__ strain_tensor,          // (n_clusters, 6) Voigtè¨˜æ³•
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size < 4) return;
    
    // å…±æœ‰ãƒ¡ãƒ¢ãƒªã§é«˜é€ŸåŒ–
    __shared__ float ref_centroid[3];
    __shared__ float curr_centroid[3];
    __shared__ float covariance[9];  // 3x3è¡Œåˆ—
    __shared__ float ref_matrix[9];  // ref^T * ref
    
    // åˆæœŸåŒ–
    if (tid < 3) {
        ref_centroid[tid] = 0.0f;
        curr_centroid[tid] = 0.0f;
    }
    if (tid < 9) {
        covariance[tid] = 0.0f;
        ref_matrix[tid] = 0.0f;
    }
    __syncthreads();
    
    // é‡å¿ƒè¨ˆç®—ï¼ˆä¸¦åˆ—ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
    int atoms_per_thread = (cluster_size + blockDim.x - 1) / blockDim.x;
    int start_atom = tid * atoms_per_thread;
    int end_atom = min(start_atom + atoms_per_thread, cluster_size);
    
    float local_ref[3] = {0.0f, 0.0f, 0.0f};
    float local_curr[3] = {0.0f, 0.0f, 0.0f};
    
    for (int i = start_atom; i < end_atom; i++) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + i];
        if (atom_idx < 0 || atom_idx >= n_atoms) continue;
        
        for (int d = 0; d < 3; d++) {
            local_ref[d] += ref_positions[atom_idx * 3 + d];
            local_curr[d] += curr_positions[atom_idx * 3 + d];
        }
    }
    
    // å…±æœ‰ãƒ¡ãƒ¢ãƒªã«é›†ç´„
    for (int d = 0; d < 3; d++) {
        atomicAdd(&ref_centroid[d], local_ref[d]);
        atomicAdd(&curr_centroid[d], local_curr[d]);
    }
    __syncthreads();
    
    if (tid < 3) {
        ref_centroid[tid] /= cluster_size;
        curr_centroid[tid] /= cluster_size;
    }
    __syncthreads();
    
    // å¤‰å½¢å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«Fè¨ˆç®—
    for (int i = start_atom; i < end_atom; i++) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + i];
        if (atom_idx < 0 || atom_idx >= n_atoms) continue;
        
        float ref[3], curr[3];
        for (int d = 0; d < 3; d++) {
            ref[d] = ref_positions[atom_idx * 3 + d] - ref_centroid[d];
            curr[d] = curr_positions[atom_idx * 3 + d] - curr_centroid[d];
        }
        
        // F = curr * ref^T ã®è¦ç´ ã‚’è¨ˆç®—
        for (int i = 0; i < 3; i++) {
            for (int j = 0; j < 3; j++) {
                atomicAdd(&covariance[i * 3 + j], curr[i] * ref[j]);
                atomicAdd(&ref_matrix[j * 3 + i], ref[j] * ref[i]);
            }
        }
    }
    __syncthreads();
    
    // Green-Lagrangeæ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«è¨ˆç®—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰0ãŒä»£è¡¨ï¼‰
    if (tid == 0) {
        // ç°¡æ˜“é€†è¡Œåˆ—ï¼ˆ3x3ã®å ´åˆã¯è§£æçš„ã«è¨ˆç®—å¯èƒ½ï¼‰
        float det = ref_matrix[0] * (ref_matrix[4] * ref_matrix[8] - ref_matrix[5] * ref_matrix[7])
                  - ref_matrix[1] * (ref_matrix[3] * ref_matrix[8] - ref_matrix[5] * ref_matrix[6])
                  + ref_matrix[2] * (ref_matrix[3] * ref_matrix[7] - ref_matrix[4] * ref_matrix[6]);
        
        if (fabsf(det) < 1e-10f) {
            for (int i = 0; i < 6; i++) {
                strain_tensor[cluster_id * 6 + i] = 0.0f;
            }
            return;
        }
        
        float F[9];
        float inv_det = 1.0f / det;
        
        for (int i = 0; i < 9; i++) {
            F[i] = covariance[i] * inv_det;
        }
        
        // Green-Lagrangeæ­ªã¿ E = 0.5 * (F^T * F - I)
        float E[6];  // Voigtè¨˜æ³•
        
        E[0] = 0.5f * (F[0]*F[0] + F[3]*F[3] + F[6]*F[6] - 1.0f);  // E_xx
        E[1] = 0.5f * (F[1]*F[1] + F[4]*F[4] + F[7]*F[7] - 1.0f);  // E_yy
        E[2] = 0.5f * (F[2]*F[2] + F[5]*F[5] + F[8]*F[8] - 1.0f);  // E_zz
        E[3] = F[0]*F[1] + F[3]*F[4] + F[6]*F[7];  // E_xy
        E[4] = F[0]*F[2] + F[3]*F[5] + F[6]*F[8];  // E_xz
        E[5] = F[1]*F[2] + F[4]*F[5] + F[7]*F[8];  // E_yz
        
        for (int i = 0; i < 6; i++) {
            strain_tensor[cluster_id * 6 + i] = E[i];
        }
    }
}
'''

COORDINATION_NUMBER_KERNEL_CODE = r'''
extern "C" __global__
void compute_coordination_kernel(
    const float* __restrict__ positions,        // (n_atoms, 3)
    const int* __restrict__ cluster_atoms,      // (n_clusters, max_atoms)
    const int* __restrict__ cluster_sizes,
    float* __restrict__ coordination_numbers,   // (n_clusters,)
    const float cutoff_squared,
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size == 0) return;
    
    __shared__ float total_coordination;
    if (tid == 0) total_coordination = 0.0f;
    __syncthreads();
    
    int n_samples = min(cluster_size, 50);
    int atoms_per_thread = (n_samples + blockDim.x - 1) / blockDim.x;
    int start_sample = tid * atoms_per_thread;
    int end_sample = min(start_sample + atoms_per_thread, n_samples);
    
    float local_coord = 0.0f;
    
    for (int s = start_sample; s < end_sample; s++) {
        int atom_i = cluster_atoms[cluster_id * max_atoms_per_cluster + s];
        if (atom_i < 0 || atom_i >= n_atoms) continue;
        
        float xi = positions[atom_i * 3 + 0];
        float yi = positions[atom_i * 3 + 1];
        float zi = positions[atom_i * 3 + 2];
        
        int neighbors = 0;
        
        for (int j = 0; j < n_atoms; j++) {
            if (j == atom_i) continue;
            
            float dx = positions[j * 3 + 0] - xi;
            float dy = positions[j * 3 + 1] - yi;
            float dz = positions[j * 3 + 2] - zi;
            
            float dist_sq = dx*dx + dy*dy + dz*dz;
            if (dist_sq < cutoff_squared) {
                neighbors++;
            }
        }
        
        local_coord += neighbors;
    }
    
    atomicAdd(&total_coordination, local_coord);
    __syncthreads();
    
    if (tid == 0) {
        coordination_numbers[cluster_id] = total_coordination / n_samples;
    }
}
'''

DAMAGE_SCORE_KERNEL_CODE = r'''
extern "C" __global__
void compute_damage_kernel(
    const float* __restrict__ ref_positions,
    const float* __restrict__ curr_positions,
    const int* __restrict__ cluster_atoms,
    const int* __restrict__ cluster_sizes,
    float* __restrict__ damage_scores,          // (n_clusters,)
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size == 0) return;
    
    __shared__ float total_rmsd;
    if (tid == 0) total_rmsd = 0.0f;
    __syncthreads();
    
    int atoms_per_thread = (cluster_size + blockDim.x - 1) / blockDim.x;
    int start_atom = tid * atoms_per_thread;
    int end_atom = min(start_atom + atoms_per_thread, cluster_size);
    
    float local_rmsd = 0.0f;
    
    for (int i = start_atom; i < end_atom; i++) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + i];
        if (atom_idx < 0 || atom_idx >= n_atoms) continue;
        
        float dx = curr_positions[atom_idx * 3 + 0] - ref_positions[atom_idx * 3 + 0];
        float dy = curr_positions[atom_idx * 3 + 1] - ref_positions[atom_idx * 3 + 1];
        float dz = curr_positions[atom_idx * 3 + 2] - ref_positions[atom_idx * 3 + 2];
        
        local_rmsd += dx*dx + dy*dy + dz*dz;
    }
    
    atomicAdd(&total_rmsd, local_rmsd);
    __syncthreads();
    
    if (tid == 0) {
        float rmsd = sqrtf(total_rmsd / cluster_size);
        damage_scores[cluster_id] = tanhf(rmsd / 2.0f);  // 0-1ã«æ­£è¦åŒ–
    }
}
'''

# ===============================
# NEW: Extended Kernels for MaterialAnalyticsGPU
# ===============================

DEFECT_CHARGE_KERNEL_CODE = r'''
extern "C" __global__
void compute_defect_charge_kernel(
    const float* __restrict__ curr_positions,   // (n_atoms, 3)
    const float* __restrict__ next_positions,   // (n_atoms, 3)
    const int* __restrict__ cluster_atoms,      // (n_clusters, max_atoms)
    const int* __restrict__ cluster_sizes,
    float* __restrict__ defect_charge,          // (n_clusters,)
    const float lattice_constant,
    const int ideal_coordination,
    const float cutoff_sq,
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size < 5) {
        if (tid == 0) defect_charge[cluster_id] = 0.0f;
        return;
    }
    
    __shared__ float burgers_magnitude;
    __shared__ float coord_change_total;
    __shared__ int sample_count;
    
    if (tid == 0) {
        burgers_magnitude = 0.0f;
        coord_change_total = 0.0f;
        sample_count = 0;
    }
    __syncthreads();
    
    // 1. Burgersãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    if (tid < cluster_size) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + tid];
        if (atom_idx >= 0 && atom_idx < n_atoms) {
            float disp[3];
            for (int d = 0; d < 3; d++) {
                disp[d] = next_positions[atom_idx * 3 + d] - curr_positions[atom_idx * 3 + d];
            }
            
            // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¤‰ä½ã®å¯„ä¸ã‚’åŠ ç®—
            float disp_mag = sqrtf(disp[0]*disp[0] + disp[1]*disp[1] + disp[2]*disp[2]);
            atomicAdd(&burgers_magnitude, disp_mag);
        }
    }
    __syncthreads();
    
    // 2. é…ä½æ•°å¤‰åŒ–ã®è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    int n_samples = min(10, cluster_size);
    if (tid < n_samples) {
        int atom_i = cluster_atoms[cluster_id * max_atoms_per_cluster + tid];
        if (atom_i >= 0 && atom_i < n_atoms) {
            // ç¾åœ¨ãƒ•ãƒ¬ãƒ¼ãƒ ã®é…ä½æ•°
            int coord_curr = 0;
            float xi_c = curr_positions[atom_i * 3 + 0];
            float yi_c = curr_positions[atom_i * 3 + 1];
            float zi_c = curr_positions[atom_i * 3 + 2];
            
            for (int j = 0; j < n_atoms; j++) {
                if (j == atom_i) continue;
                float dx = curr_positions[j * 3 + 0] - xi_c;
                float dy = curr_positions[j * 3 + 1] - yi_c;
                float dz = curr_positions[j * 3 + 2] - zi_c;
                if (dx*dx + dy*dy + dz*dz < cutoff_sq) coord_curr++;
            }
            
            // æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ ã®é…ä½æ•°
            int coord_next = 0;
            float xi_n = next_positions[atom_i * 3 + 0];
            float yi_n = next_positions[atom_i * 3 + 1];
            float zi_n = next_positions[atom_i * 3 + 2];
            
            for (int j = 0; j < n_atoms; j++) {
                if (j == atom_i) continue;
                float dx = next_positions[j * 3 + 0] - xi_n;
                float dy = next_positions[j * 3 + 1] - yi_n;
                float dz = next_positions[j * 3 + 2] - zi_n;
                if (dx*dx + dy*dy + dz*dz < cutoff_sq) coord_next++;
            }
            
            float coord_change = fabsf(coord_next - coord_curr) / (float)ideal_coordination;
            atomicAdd(&coord_change_total, coord_change);
            atomicAdd(&sample_count, 1);
        }
    }
    __syncthreads();
    
    // 3. æ¬ é™¥ãƒãƒ£ãƒ¼ã‚¸è¨ˆç®—ï¼ˆæ­£è¦åŒ–ï¼‰
    if (tid == 0) {
        float burgers_normalized = burgers_magnitude / (lattice_constant * cluster_size);
        float coord_change_normalized = (sample_count > 0) ? 
            coord_change_total / sample_count : 0.0f;
        
        // tanhã§é£½å’Œã•ã›ã¦çˆ†ç™ºã‚’é˜²ã
        float raw_charge = 0.3f * burgers_normalized + 0.2f * coord_change_normalized;
        defect_charge[cluster_id] = tanhf(raw_charge);
    }
}
'''

STRUCTURAL_COHERENCE_KERNEL_CODE = r'''
extern "C" __global__
void compute_coherence_kernel(
    const float* __restrict__ coordination,     // (n_frames, n_clusters)
    const float* __restrict__ strain,          // (n_frames, n_clusters)  
    float* __restrict__ coherence,             // (n_frames,)
    const int window,
    const int ideal_coord,
    const int n_frames,
    const int n_clusters
) {
    const int frame = blockIdx.x * blockDim.x + threadIdx.x;
    if (frame >= n_frames) return;
    
    // ã‚¨ãƒƒã‚¸å‡¦ç†
    if (frame < window || frame >= n_frames - window) {
        if (frame < window) coherence[frame] = coherence[window];
        else coherence[frame] = coherence[n_frames - window - 1];
        return;
    }
    
    float total_coherence = 0.0f;
    
    for (int c = 0; c < n_clusters; c++) {
        // ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®çµ±è¨ˆé‡è¨ˆç®—
        float coord_mean = 0.0f;
        float coord_sq_mean = 0.0f;
        
        for (int w = -window; w <= window; w++) {
            int idx = (frame + w) * n_clusters + c;
            float coord = coordination[idx];
            coord_mean += coord;
            coord_sq_mean += coord * coord;
        }
        
        int window_size = 2 * window + 1;
        coord_mean /= window_size;
        coord_sq_mean /= window_size;
        
        // æ¨™æº–åå·®
        float coord_std = sqrtf(fmaxf(0.0f, coord_sq_mean - coord_mean * coord_mean));
        
        // æ§‹é€ å®‰å®šæ€§è©•ä¾¡
        float cluster_coherence = 1.0f;
        
        if (coord_std < 0.5f && fabsf(coord_mean - ideal_coord) < 1.0f) {
            // å®‰å®šã—ã¦ç†æƒ³æ§‹é€ ã‚’ä¿æŒ
            cluster_coherence = 1.0f;
        } else if (coord_std > 2.0f) {
            // æ¿€ã—ãæºã‚‰ã„ã§ã„ã‚‹
            cluster_coherence = 0.3f;
        } else {
            // æ§‹é€ å¤‰åŒ–ä¸­
            cluster_coherence = 1.0f - fminf(coord_std / 3.0f, 1.0f);
        }
        
        // æ­ªã¿ã«ã‚ˆã‚‹è£œæ­£
        float strain_val = strain[frame * n_clusters + c];
        if (strain_val > 0.05f) {  // 5%ä»¥ä¸Šã®æ­ªã¿
            cluster_coherence *= (1.0f - fminf(strain_val, 1.0f));
        }
        
        total_coherence += cluster_coherence;
    }
    
    coherence[frame] = total_coherence / n_clusters;
}
'''

BURGERS_VECTOR_KERNEL_CODE = r'''
extern "C" __global__
void compute_burgers_kernel(
    const float* __restrict__ positions,        // (n_atoms, 3)
    const int* __restrict__ cluster_atoms,      // (n_clusters, max_atoms)
    const int* __restrict__ cluster_sizes,
    float* __restrict__ burgers_vectors,        // (n_clusters, 3)
    const float cutoff,
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size < 10) {
        if (tid == 0) {
            burgers_vectors[cluster_id * 3 + 0] = 0.0f;
            burgers_vectors[cluster_id * 3 + 1] = 0.0f;
            burgers_vectors[cluster_id * 3 + 2] = 0.0f;
        }
        return;
    }
    
    __shared__ float burgers[3];
    
    if (tid < 3) {
        burgers[tid] = 0.0f;
    }
    __syncthreads();
    
    // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒåŸå­
    int center_idx = cluster_size / 2;
    int center_atom = cluster_atoms[cluster_id * max_atoms_per_cluster + center_idx];
    
    if (center_atom < 0 || center_atom >= n_atoms) {
        if (tid == 0) {
            burgers_vectors[cluster_id * 3 + 0] = 0.0f;
            burgers_vectors[cluster_id * 3 + 1] = 0.0f;
            burgers_vectors[cluster_id * 3 + 2] = 0.0f;
        }
        return;
    }
    
    float center_pos[3];
    if (tid == 0) {
        center_pos[0] = positions[center_atom * 3 + 0];
        center_pos[1] = positions[center_atom * 3 + 1];
        center_pos[2] = positions[center_atom * 3 + 2];
    }
    __syncthreads();
    
    // æœ€è¿‘å‚6åŸå­ã§é–‰å›è·¯ã‚’æ§‹æˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
    if (tid < 6 && tid < cluster_size) {
        int atom_i = cluster_atoms[cluster_id * max_atoms_per_cluster + tid];
        int atom_j = cluster_atoms[cluster_id * max_atoms_per_cluster + ((tid + 1) % 6)];
        
        if (atom_i >= 0 && atom_i < n_atoms && atom_j >= 0 && atom_j < n_atoms) {
            for (int d = 0; d < 3; d++) {
                float delta = positions[atom_j * 3 + d] - positions[atom_i * 3 + d];
                atomicAdd(&burgers[d], delta);
            }
        }
    }
    __syncthreads();
    
    // çµæœã‚’å‡ºåŠ›
    if (tid == 0) {
        burgers_vectors[cluster_id * 3 + 0] = burgers[0];
        burgers_vectors[cluster_id * 3 + 1] = burgers[1];
        burgers_vectors[cluster_id * 3 + 2] = burgers[2];
    }
}
'''

ROLLING_STATS_KERNEL_CODE = r'''
extern "C" __global__
void compute_rolling_stats_kernel(
    const float* __restrict__ data,             // (n_frames, n_features)
    float* __restrict__ mean,                   // (n_frames, n_features)
    float* __restrict__ std,                    // (n_frames, n_features)
    const int window,
    const int n_frames,
    const int n_features
) {
    const int frame = blockIdx.x;
    const int feature = blockIdx.y * blockDim.x + threadIdx.x;
    
    if (frame >= n_frames || feature >= n_features) return;
    
    int start = max(0, frame - window);
    int end = min(n_frames - 1, frame + window);
    int window_size = end - start + 1;
    
    float sum = 0.0f;
    float sum_sq = 0.0f;
    
    for (int f = start; f <= end; f++) {
        float val = data[f * n_features + feature];
        sum += val;
        sum_sq += val * val;
    }
    
    float mean_val = sum / window_size;
    float std_val = sqrtf(fmaxf(0.0f, sum_sq / window_size - mean_val * mean_val));
    
    mean[frame * n_features + feature] = mean_val;
    std[frame * n_features + feature] = std_val;
}
'''

# ===============================
# Extended Kernel Manager Class
# ===============================

class MaterialCUDAKernels:
    """ææ–™è§£æç”¨CUDAã‚«ãƒ¼ãƒãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    
    def __init__(self):
        # Original kernels
        self.strain_kernel = None
        self.coord_kernel = None
        self.damage_kernel = None
        
        # Extended kernels for MaterialAnalyticsGPU
        self.defect_charge_kernel = None
        self.coherence_kernel = None
        self.burgers_kernel = None
        self.rolling_stats_kernel = None
        
        self.compiled = False
        
        if HAS_CUDA:
            self._compile_kernels()
    
    def _compile_kernels(self):
        """å…¨CUDAã‚«ãƒ¼ãƒãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        try:
            # === Original 3 kernels ===
            self.strain_kernel = cp.RawKernel(
                STRAIN_TENSOR_KERNEL_CODE,
                'compute_strain_tensor_kernel',
                options=('--use_fast_math',)
            )
            
            self.coord_kernel = cp.RawKernel(
                COORDINATION_NUMBER_KERNEL_CODE,
                'compute_coordination_kernel',
                options=('--use_fast_math',)
            )
            
            self.damage_kernel = cp.RawKernel(
                DAMAGE_SCORE_KERNEL_CODE,
                'compute_damage_kernel',
                options=('--use_fast_math',)
            )
            
            # === NEW: Extended kernels ===
            self.defect_charge_kernel = cp.RawKernel(
                DEFECT_CHARGE_KERNEL_CODE,
                'compute_defect_charge_kernel',
                options=('--use_fast_math',)
            )
            
            self.coherence_kernel = cp.RawKernel(
                STRUCTURAL_COHERENCE_KERNEL_CODE,
                'compute_coherence_kernel',
                options=('--use_fast_math',)
            )
            
            self.burgers_kernel = cp.RawKernel(
                BURGERS_VECTOR_KERNEL_CODE,
                'compute_burgers_kernel',
                options=('--use_fast_math',)
            )
            
            self.rolling_stats_kernel = cp.RawKernel(
                ROLLING_STATS_KERNEL_CODE,
                'compute_rolling_stats_kernel',
                options=('--use_fast_math',)
            )
            
            self.compiled = True
            logger.info("âœ… All 7 CUDA kernels compiled successfully!")
            logger.info("   - Original: strain, coordination, damage")
            logger.info("   - Extended: defect_charge, coherence, burgers, rolling_stats")
            
        except Exception as e:
            logger.warning(f"âš ï¸ CUDA kernel compilation failed: {e}")
            logger.warning("   Falling back to standard GPU computation")
            self.compiled = False
    
    # ===============================
    # Original Methods (unchanged)
    # ===============================
    
    def compute_strain_tensors_cuda(self,
                                   ref_positions: cp.ndarray,
                                   curr_positions: cp.ndarray,
                                   cluster_atoms: Dict,
                                   n_frames: int = None) -> cp.ndarray:
        """CUDAã‚«ãƒ¼ãƒãƒ«ã§æ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«è¨ˆç®—"""
        if not self.compiled or self.strain_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = ref_positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        ref_pos_flat = ref_positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        strain_result = cp.zeros((n_clusters, 6), dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.strain_kernel(
            (blocks,), (threads_per_block,),
            (ref_pos_flat, curr_positions.flatten().astype(cp.float32),
             cluster_array_gpu, cluster_sizes_gpu,
             strain_result, n_atoms, n_clusters, max_atoms)
        )
        
        return strain_result
    
    def compute_coordination_cuda(self,
                                 positions: cp.ndarray,
                                 cluster_atoms: Dict,
                                 cutoff: float) -> cp.ndarray:
        """CUDAã‚«ãƒ¼ãƒãƒ«ã§é…ä½æ•°è¨ˆç®—"""
        if not self.compiled or self.coord_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        positions_flat = positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        coord_result = cp.zeros(n_clusters, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.coord_kernel(
            (blocks,), (threads_per_block,),
            (positions_flat, cluster_array_gpu, cluster_sizes_gpu,
             coord_result, cutoff * cutoff, n_atoms, n_clusters, max_atoms)
        )
        
        return coord_result
    
    def compute_damage_cuda(self,
                          ref_positions: cp.ndarray,
                          curr_positions: cp.ndarray,
                          cluster_atoms: Dict) -> cp.ndarray:
        """CUDAã‚«ãƒ¼ãƒãƒ«ã§æå‚·åº¦è¨ˆç®—"""
        if not self.compiled or self.damage_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = ref_positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        ref_pos_flat = ref_positions.flatten().astype(cp.float32)
        curr_pos_flat = curr_positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        damage_result = cp.zeros(n_clusters, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.damage_kernel(
            (blocks,), (threads_per_block,),
            (ref_pos_flat, curr_pos_flat, cluster_array_gpu, cluster_sizes_gpu,
             damage_result, n_atoms, n_clusters, max_atoms)
        )
        
        return damage_result
    
    # ===============================
    # NEW: Extended Methods for MaterialAnalyticsGPU
    # ===============================
    
    def compute_defect_charge_cuda(self,
                                  curr_positions: cp.ndarray,
                                  next_positions: cp.ndarray,
                                  cluster_atoms: Dict,
                                  lattice_constant: float,
                                  ideal_coordination: int,
                                  cutoff: float) -> cp.ndarray:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§æ¬ é™¥ãƒãƒ£ãƒ¼ã‚¸è¨ˆç®—ï¼ˆMaterialAnalyticsGPUç”¨ï¼‰
        
        Returns
        -------
        cp.ndarray
            (n_clusters,) ã®æ¬ é™¥ãƒãƒ£ãƒ¼ã‚¸ï¼ˆ0-1æ­£è¦åŒ–æ¸ˆã¿ï¼‰
        """
        if not self.compiled or self.defect_charge_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = curr_positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        curr_pos_flat = curr_positions.flatten().astype(cp.float32)
        next_pos_flat = next_positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        defect_charge = cp.zeros(n_clusters, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.defect_charge_kernel(
            (blocks,), (threads_per_block,),
            (curr_pos_flat, next_pos_flat, cluster_array_gpu, cluster_sizes_gpu,
             defect_charge, lattice_constant, ideal_coordination, 
             cutoff * cutoff, n_atoms, n_clusters, max_atoms)
        )
        
        return defect_charge
    
    def compute_structural_coherence_cuda(self,
                                         coordination: cp.ndarray,
                                         strain: cp.ndarray,
                                         window: int,
                                         ideal_coord: int) -> cp.ndarray:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§æ§‹é€ ä¸€è²«æ€§è¨ˆç®—
        
        Returns
        -------
        cp.ndarray
            (n_frames,) ã®æ§‹é€ ä¸€è²«æ€§
        """
        if not self.compiled or self.coherence_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_frames, n_clusters = coordination.shape
        coherence = cp.zeros(n_frames, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = (n_frames + threads_per_block - 1) // threads_per_block
        
        self.coherence_kernel(
            (blocks,), (threads_per_block,),
            (coordination.astype(cp.float32), strain.astype(cp.float32),
             coherence, window, ideal_coord, n_frames, n_clusters)
        )
        
        return coherence
    
    def compute_burgers_vectors_cuda(self,
                                    positions: cp.ndarray,
                                    cluster_atoms: Dict,
                                    cutoff: float) -> cp.ndarray:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§Burgersãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—
        
        Returns
        -------
        cp.ndarray
            (n_clusters, 3) ã®Burgersãƒ™ã‚¯ãƒˆãƒ«
        """
        if not self.compiled or self.burgers_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        positions_flat = positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        burgers_vectors = cp.zeros((n_clusters, 3), dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.burgers_kernel(
            (blocks,), (threads_per_block,),
            (positions_flat, cluster_array_gpu, cluster_sizes_gpu,
             burgers_vectors.ravel(), cutoff, n_atoms, n_clusters, max_atoms)
        )
        
        return burgers_vectors
    
    def compute_rolling_stats_cuda(self,
                                  data: cp.ndarray,
                                  window: int) -> Tuple[cp.ndarray, cp.ndarray]:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§ç§»å‹•å¹³å‡ãƒ»æ¨™æº–åå·®è¨ˆç®—
        
        Returns
        -------
        tuple
            (mean, std) ã®ä¸¡æ–¹ (n_frames, n_features)
        """
        if not self.compiled or self.rolling_stats_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_frames, n_features = data.shape
        mean = cp.zeros_like(data, dtype=cp.float32)
        std = cp.zeros_like(data, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œï¼ˆ2Dã‚°ãƒªãƒƒãƒ‰ï¼‰
        threads_per_block = (16, 16)
        blocks_x = (n_frames + threads_per_block[0] - 1) // threads_per_block[0]
        blocks_y = (n_features + threads_per_block[1] - 1) // threads_per_block[1]
        
        self.rolling_stats_kernel(
            (blocks_x, blocks_y), threads_per_block,
            (data.astype(cp.float32), mean, std,
             window, n_frames, n_features)
        )
        
        return mean, std
    
    def get_kernel_status(self) -> Dict[str, bool]:
        """å„ã‚«ãƒ¼ãƒãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            'strain_tensor': self.strain_kernel is not None,
            'coordination': self.coord_kernel is not None,
            'damage_score': self.damage_kernel is not None,
            'defect_charge': self.defect_charge_kernel is not None,
            'structural_coherence': self.coherence_kernel is not None,
            'burgers_vector': self.burgers_kernel is not None,
            'rolling_stats': self.rolling_stats_kernel is not None,
        }

# ===============================
# Export
# ===============================

__all__ = [
    'MaterialCUDAKernels',
    # Original kernel codes
    'STRAIN_TENSOR_KERNEL_CODE',
    'COORDINATION_NUMBER_KERNEL_CODE',
    'DAMAGE_SCORE_KERNEL_CODE',
    # Extended kernel codes
    'DEFECT_CHARGE_KERNEL_CODE',
    'STRUCTURAL_COHERENCE_KERNEL_CODE',
    'BURGERS_VECTOR_KERNEL_CODE',
    'ROLLING_STATS_KERNEL_CODE',
]

# ===============================
# Test/Demo
# ===============================

if __name__ == "__main__":
    print("ğŸ’ Extended Material CUDA Kernels Module v2.0")
    print("   RTX 4070 Ti SUPER optimized!")
    print("   7 kernels for ultimate performance!")
    
    if HAS_CUDA:
        kernels = MaterialCUDAKernels()
        if kernels.compiled:
            print("\nâœ… Compilation status:")
            for name, status in kernels.get_kernel_status().items():
                symbol = "âœ“" if status else "âœ—"
                print(f"   {symbol} {name}")
        else:
            print("   âš ï¸ Compilation failed")
    else:
        print("   âŒ CUDA not available")
        
