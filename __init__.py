"""
LambdaÂ³ GPU-Accelerated MD Analysis Framework
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ç’°ã¡ã‚ƒã‚“ãŒä½œã£ãŸè¶…é«˜é€ŸGPUç‰ˆLambdaÂ³ã ã‚ˆã€œï¼ğŸ’•
NO TIME, NO PHYSICS, ONLY STRUCTURE... but FASTER! ğŸš€

Basic usage:
    >>> from lambda3_gpu import MDLambda3DetectorGPU, MDConfig
    >>> config = MDConfig()
    >>> detector = MDLambda3DetectorGPU(config)
    >>> result = detector.analyze(trajectory)

Full documentation at https://github.com/your-repo/lambda3-gpu
"""

__version__ = '3.0.0-gpu'
__author__ = 'LambdaÂ³ Project (GPU Edition by Tamaki)'
__email__ = 'tamaki@miosync.inc'
__license__ = 'MIT'

import warnings
import logging
import sys
import os
from typing import Optional, Dict, Any, Tuple

# ===============================
# Constants
# ===============================

LOG_FORMAT = '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
DATE_FORMAT = '%Y-%m-%d %H:%M:%S'

REQUIRED_PACKAGES = {
    'numpy': '1.19.0',
    'scipy': '1.5.0',
    'numba': '0.50.0',
    'matplotlib': '3.2.0'
}

OPTIONAL_PACKAGES = {
    'cupy': '8.0.0',
    'cupyx': None,
    'joblib': '0.16.0',
    'tqdm': '4.50.0'
}

# Default values for A100
DEFAULT_COMPUTE_CAPABILITY = "8.0"
DEFAULT_GPU_NAME = "NVIDIA A100"

# ===============================
# Logging Setup
# ===============================

logging.basicConfig(
    level=logging.INFO,
    format=LOG_FORMAT,
    datefmt=DATE_FORMAT
)

logger = logging.getLogger('lambda3_gpu')
logger.setLevel(logging.INFO)

# ===============================
# GPU Detection and Setup
# ===============================

class GPUEnvironment:
    """GPUç’°å¢ƒæƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.has_cupy = False
        self.gpu_available = False
        self.gpu_name = None
        self.gpu_memory = 0
        self.compute_capability = None
        self.cuda_version = None
        self.device_id = 0
        
        self._detect_gpu()
    
    def _detect_gpu(self):
        """GPUç’°å¢ƒã‚’æ¤œå‡º"""
        try:
            import cupy as cp
            self.has_cupy = True
            
            if cp.cuda.is_available():
                self.gpu_available = True
                self._get_gpu_info(cp)
            else:
                logger.warning("CuPy is installed but no GPU is available")
                
        except ImportError:
            warnings.warn(
                "CuPy not installed! GPU acceleration disabled. "
                "Install with: pip install cupy-cuda12x",
                ImportWarning
            )
        except Exception as e:
            self.has_cupy = True
            warnings.warn(
                f"GPU initialization error: {e}. Running in CPU mode.",
                RuntimeWarning
            )
    
    def _get_gpu_info(self, cp):
        """GPUè©³ç´°æƒ…å ±ã‚’å–å¾—"""
        try:
            # ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ
            cp.cuda.Device(self.device_id).use()
            
            # ãƒ‡ãƒã‚¤ã‚¹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å–å¾—
            props = self._get_device_properties(cp)
            
            # GPUå
            self.gpu_name = self._extract_gpu_name(props)
            
            # ãƒ¡ãƒ¢ãƒªæƒ…å ±
            self.gpu_memory = self._get_gpu_memory(cp)
            
            # Compute Capability
            self.compute_capability = self._get_compute_capability(cp, props)
            
            # CUDA version
            self.cuda_version = self._get_cuda_version(cp)
            
        except Exception as e:
            logger.error(f"Failed to get GPU info: {e}")
            self.gpu_available = False
    
    def _get_device_properties(self, cp):
        """ãƒ‡ãƒã‚¤ã‚¹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å®‰å…¨ã«å–å¾—"""
        try:
            return cp.cuda.runtime.getDeviceProperties(self.device_id)
        except Exception as e:
            logger.warning(f"Could not get device properties: {e}")
            return {}
    
    def _extract_gpu_name(self, props) -> str:
        """GPUåã‚’æŠ½å‡º"""
        try:
            if isinstance(props, dict):
                name = props.get('name', DEFAULT_GPU_NAME)
            else:
                name = getattr(props, 'name', DEFAULT_GPU_NAME)
            
            # ãƒã‚¤ãƒˆæ–‡å­—åˆ—ã®å ´åˆã¯ãƒ‡ã‚³ãƒ¼ãƒ‰
            if isinstance(name, bytes):
                return name.decode('utf-8')
            return str(name)
        except:
            return DEFAULT_GPU_NAME
    
    def _get_gpu_memory(self, cp) -> float:
        """GPUãƒ¡ãƒ¢ãƒªå®¹é‡ã‚’å–å¾—ï¼ˆGBï¼‰"""
        try:
            _, total_memory = cp.cuda.runtime.memGetInfo()
            return total_memory / 1024**3
        except:
            return 0.0
    
    def _get_compute_capability(self, cp, props) -> str:
        """Compute Capabilityã‚’å–å¾—"""
        try:
            # æ–¹æ³•1: propsã‹ã‚‰å–å¾—
            if isinstance(props, dict):
                major = props.get('major', 0)
                minor = props.get('minor', 0)
            else:
                major = getattr(props, 'major', 0)
                minor = getattr(props, 'minor', 0)
            
            # å–å¾—ã§ããŸå ´åˆ
            if major > 0:
                return f"{major}.{minor}"
            
            # æ–¹æ³•2: ãƒ‡ãƒã‚¤ã‚¹å±æ€§ã‹ã‚‰å–å¾—ï¼ˆå¤ã„æ–¹æ³•ï¼‰
            try:
                device = cp.cuda.Device(self.device_id)
                attrs = device.attributes
                if hasattr(attrs, 'get') and 'ComputeCapability' in attrs:
                    return attrs['ComputeCapability']
            except:
                pass
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            logger.warning(f"Could not detect compute capability, using default: {DEFAULT_COMPUTE_CAPABILITY}")
            return DEFAULT_COMPUTE_CAPABILITY
            
        except Exception as e:
            logger.warning(f"Error getting compute capability: {e}")
            return DEFAULT_COMPUTE_CAPABILITY
    
    def _get_cuda_version(self, cp) -> str:
        """CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—"""
        try:
            version = cp.cuda.runtime.runtimeGetVersion()
            major = version // 1000
            minor = (version % 1000) // 10
            return f"{major}.{minor}"
        except:
            return "Unknown"
    
    def get_info_dict(self) -> Dict[str, Any]:
        """ç’°å¢ƒæƒ…å ±ã‚’è¾æ›¸ã§è¿”ã™"""
        return {
            'available': self.gpu_available,
            'name': self.gpu_name,
            'memory_gb': self.gpu_memory,
            'cuda_version': self.cuda_version,
            'compute_capability': self.compute_capability,
            'has_cupy': self.has_cupy
        }

# ===============================
# Initialize GPU Environment
# ===============================

gpu_env = GPUEnvironment()

# Export global variables for backward compatibility
HAS_CUPY = gpu_env.has_cupy
GPU_AVAILABLE = gpu_env.gpu_available
GPU_NAME = gpu_env.gpu_name
GPU_MEMORY = gpu_env.gpu_memory
GPU_COMPUTE_CAPABILITY = gpu_env.compute_capability
CUDA_VERSION_STR = gpu_env.cuda_version

# ===============================
# Dependency Checking
# ===============================

def check_dependencies():
    """ä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯"""
    missing_required = []
    missing_optional = []
    
    for package, min_version in REQUIRED_PACKAGES.items():
        try:
            __import__(package)
        except ImportError:
            missing_required.append(package)
    
    for package, min_version in OPTIONAL_PACKAGES.items():
        try:
            __import__(package)
        except ImportError:
            missing_optional.append(package)
    
    if missing_required:
        raise ImportError(
            f"Missing required packages: {', '.join(missing_required)}. "
            f"Please install with: pip install {' '.join(missing_required)}"
        )
    
    if missing_optional:
        logger.debug(f"Optional packages not installed: {', '.join(missing_optional)}")

# Run dependency check
check_dependencies()

# ===============================
# Log GPU Status
# ===============================

if GPU_AVAILABLE:
    logger.info(f"ğŸš€ GPU Enabled: {GPU_NAME}")
    logger.info(f"   Memory: {GPU_MEMORY:.1f} GB")
    logger.info(f"   CUDA: {CUDA_VERSION_STR}")
    logger.info(f"   Compute Capability: {GPU_COMPUTE_CAPABILITY}")
else:
    logger.warning("ğŸ’» Running in CPU mode (GPU not available)")

# ===============================
# Utility Functions
# ===============================

def get_gpu_info() -> Dict[str, Any]:
    """GPUæƒ…å ±ã‚’å–å¾—"""
    return gpu_env.get_info_dict()

def set_gpu_device(device_id: int = 0) -> bool:
    """ä½¿ç”¨ã™ã‚‹GPUãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š"""
    if not GPU_AVAILABLE:
        logger.warning("GPU not available, cannot set device")
        return False
    
    try:
        import cupy as cp
        cp.cuda.Device(device_id).use()
        logger.info(f"Set GPU device to: {device_id}")
        return True
    except Exception as e:
        logger.error(f"Failed to set GPU device: {e}")
        return False

def enable_gpu_logging(level: str = 'DEBUG') -> None:
    """GPUé–¢é€£ã®è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–"""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    
    logger.setLevel(numeric_level)
    
    if HAS_CUPY:
        cupy_logger = logging.getLogger('cupy')
        cupy_logger.setLevel(numeric_level)

def benchmark_gpu(size: int = 10000) -> Optional[Dict[str, float]]:
    """ç°¡æ˜“GPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    if not GPU_AVAILABLE:
        logger.warning("GPU not available for benchmarking")
        return None
    
    import time
    import numpy as np
    import cupy as cp
    
    # CPUè¨ˆæ¸¬
    cpu_array = np.random.randn(size, size).astype(np.float32)
    
    cpu_start = time.perf_counter()
    cpu_result = np.matmul(cpu_array, cpu_array)
    cpu_time = time.perf_counter() - cpu_start
    
    # GPUè¨ˆæ¸¬
    gpu_array = cp.asarray(cpu_array)
    cp.cuda.Stream.null.synchronize()
    
    gpu_start = time.perf_counter()
    gpu_result = cp.matmul(gpu_array, gpu_array)
    cp.cuda.Stream.null.synchronize()
    gpu_time = time.perf_counter() - gpu_start
    
    speedup = cpu_time / gpu_time
    
    result = {
        'matrix_size': size,
        'cpu_time': cpu_time,
        'gpu_time': gpu_time,
        'speedup': speedup
    }
    
    logger.info(f"Benchmark result: {speedup:.1f}x speedup on {size}x{size} matrix multiplication")
    
    return result

# ===============================
# Banner Display
# ===============================

def _print_banner():
    """ã‹ã‚ã„ã„ãƒãƒŠãƒ¼è¡¨ç¤º"""
    if sys.stdout.isatty():
        print("\n" + "="*60)
        print("ğŸŒŸ LambdaÂ³ GPU - Structural Analysis at Light Speed! ğŸš€")
        print("="*60)
        if GPU_AVAILABLE:
            print(f"âœ¨ GPU Mode: {GPU_NAME} ({GPU_MEMORY:.1f}GB)")
        else:
            print("ğŸ’» CPU Mode (Install CuPy for GPU acceleration)")
        print("="*60 + "\n")

# Show banner in interactive mode
if hasattr(sys, 'ps1'):
    _print_banner()

# ===============================
# Error Classes
# ===============================

class Lambda3GPUError(Exception):
    """Lambda3GPUåŸºæœ¬ä¾‹å¤–ã‚¯ãƒ©ã‚¹"""
    pass

class GPUMemoryError(Lambda3GPUError):
    """GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼"""
    pass

class GPUNotAvailableError(Lambda3GPUError):
    """GPUåˆ©ç”¨ä¸å¯ã‚¨ãƒ©ãƒ¼"""
    pass

# ===============================
# Environment Variables
# ===============================

# GPU memory limit
if 'LAMBDA3_GPU_MEMORY_LIMIT' in os.environ:
    try:
        memory_limit_gb = float(os.environ['LAMBDA3_GPU_MEMORY_LIMIT'])
        logger.info(f"GPU memory limit set to: {memory_limit_gb} GB")
    except ValueError:
        logger.warning("Invalid LAMBDA3_GPU_MEMORY_LIMIT value")

# Debug mode
if os.environ.get('LAMBDA3_DEBUG', '').lower() in ('1', 'true', 'yes'):
    enable_gpu_logging('DEBUG')
    logger.debug("Debug mode enabled")

# ===============================
# Public API
# ===============================

__all__ = [
    # Version info
    '__version__',
    '__author__',
    
    # GPU info
    'GPU_AVAILABLE',
    'GPU_NAME', 
    'GPU_MEMORY',
    'CUDA_VERSION_STR',
    'GPU_COMPUTE_CAPABILITY',
    
    # Classes (lazy import)
    'MDConfig',
    'ResidueAnalysisConfig',
    'MDLambda3DetectorGPU',
    'TwoStageAnalyzerGPU',
    
    # Utilities
    'get_gpu_info',
    'set_gpu_device',
    'enable_gpu_logging',
    'benchmark_gpu',
    
    # Errors
    'Lambda3GPUError',
    'GPUMemoryError',
    'GPUNotAvailableError'
]

# ===============================
# Lazy Imports
# ===============================

def __getattr__(name):
    """é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§èµ·å‹•æ™‚é–“çŸ­ç¸®"""
    
    # Config classes
    if name == 'MDConfig':
        from lambda3_gpu.analysis.md_lambda3_detector_gpu import MDConfig
        return MDConfig
    
    elif name == 'ResidueAnalysisConfig':
        from lambda3_gpu.analysis.two_stage_analyzer_gpu import ResidueAnalysisConfig
        return ResidueAnalysisConfig
    
    # Main classes
    elif name == 'MDLambda3DetectorGPU':
        from lambda3_gpu.analysis.md_lambda3_detector_gpu import MDLambda3DetectorGPU
        return MDLambda3DetectorGPU
    
    elif name == 'TwoStageAnalyzerGPU':
        from lambda3_gpu.analysis.two_stage_analyzer_gpu import TwoStageAnalyzerGPU
        return TwoStageAnalyzerGPU
    
    # Not found
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")

# ===============================
# Import Hook for Monkey Patching
# ===============================
def _apply_patches():
    """å¿…è¦ãªãƒ‘ãƒƒãƒã‚’é©ç”¨"""
    try:
        # find_peaks ãƒ‘ãƒƒãƒï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        from cupyx.scipy import signal
        
        # find_peaksãŒå­˜åœ¨ã—ã¦ã‚‚ãƒ‘ãƒƒãƒã‚’é©ç”¨ï¼ˆå¼•æ•°äº’æ›æ€§ã®ãŸã‚ï¼‰
        if hasattr(signal, 'find_peaks'):
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚’ä¿å­˜
            signal._find_peaks_original = signal.find_peaks
            
            # ãƒ‘ãƒƒãƒé–¢æ•°
            def find_peaks_patched(array, height=None, distance=None, prominence=None, **kwargs):
                """å¼•æ•°ã‚’è‡ªå‹•èª¿æ•´ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼"""
                try:
                    return signal._find_peaks_original(array, height=height, distance=distance, 
                                                     prominence=prominence, **kwargs)
                except ValueError as e:
                    if "array size" in str(e) and height is not None:
                        # heightãŒã‚¹ã‚«ãƒ©ãƒ¼ã®å ´åˆã€é…åˆ—ã«å¤‰æ›
                        import cupy as cp
                        if not hasattr(height, '__len__'):
                            height = cp.full(len(array), height)
                        return signal._find_peaks_original(array, height=height, distance=distance,
                                                         prominence=prominence, **kwargs)
                    else:
                        raise
            
            # ç½®ãæ›ãˆ
            signal.find_peaks = find_peaks_patched
            logger.debug("Applied find_peaks compatibility patch")
        else:
            # find_peaksãŒå­˜åœ¨ã—ãªã„å ´åˆï¼ˆå¤ã„CuPyï¼‰
            from .core.gpu_patches import find_peaks_gpu_fallback
            signal.find_peaks = find_peaks_gpu_fallback
            logger.debug("Applied find_peaks fallback patch")
            
    except Exception as e:
        logger.debug(f"Failed to apply patches: {e}")

# Apply patches on import
_apply_patches()
