"""
Lambda¬≥ Structure Computation (GPU Version)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Lambda¬≥ÊßãÈÄ†„ÅÆË®àÁÆó„ÇíGPU„ÅßË∂ÖÈ´òÈÄüÂåñÔºÅ
NO TIME, NO PHYSICS, ONLY STRUCTURE... but FASTER! üöÄ

‚ö° 2025/01/16 Áí∞„Å°„ÇÉ„ÇìÂÆåÂÖ®‰øÆÊ≠£Áâà v2
- GPUBackend (core/utils.py) „ÇíÊ≠£„Åó„ÅèÁ∂ôÊâø
- self.xp„ÅØË¶™„ÇØ„É©„Çπ„ÅßÂàùÊúüÂåñ„Åï„Çå„Çã„Åã„ÇâÂÆâÂøÉÔºÅ
"""
import numpy as np
import logging
from typing import Dict, Optional, Tuple, Union, Any
from dataclasses import dataclass

# ===============================
# GPU Setup
# ===============================
try:
    import cupy as cp
    HAS_GPU = True
    print("‚úÖ CuPy successfully imported")
except ImportError as e:
    cp = None
    cp_savgol_filter = None
    HAS_GPU = False
    print(f"‚ö†Ô∏è CuPy not available: {e}")

# Local imports - Ê≠£„Åó„ÅÑ„Éë„Çπ„Åã„Çâ„Ç§„É≥„Éù„Éº„Éà
from ..types import ArrayType, NDArray
from ..core.gpu_utils import GPUBackend  
from ..core.gpu_memory import GPUMemoryManager

# „Ç´„Éº„Éç„É´„Ç§„É≥„Éù„Éº„ÉàÔºà„Ç™„Éó„Ç∑„Éß„Éä„É´Ôºâ
tension_field_kernel = None
topological_charge_kernel = None

if HAS_GPU:
    try:
        from ..core import tension_field_kernel, topological_charge_kernel
        print("‚úÖ Custom kernels imported")
    except ImportError as e:
        print(f"‚ö†Ô∏è Custom kernels not available: {e}")

logger = logging.getLogger(__name__)

# ===============================
# Configuration
# ===============================

@dataclass
class LambdaStructureConfig:
    """LambdaÊßãÈÄ†Ë®àÁÆó„ÅÆË®≠ÂÆö"""
    use_mixed_precision: bool = True
    batch_size: Optional[int] = None
    cache_intermediates: bool = True
    profile: bool = False

# ===============================
# Lambda Structures GPU Class
# ===============================

class LambdaStructuresGPU(GPUBackend):
    """
    Lambda¬≥ÊßãÈÄ†Ë®àÁÆó„ÅÆGPUÂÆüË£Ö„ÇØ„É©„Çπ
    GPUBackend„ÇíÊ≠£„Åó„ÅèÁ∂ôÊâø„Åó„Å¶„Çã„Åã„ÇâÂ§ß‰∏àÂ§´ÔºÅ‚ú®
    """
    
    def __init__(self, 
                 config: Optional[LambdaStructureConfig] = None,
                 memory_manager: Optional[GPUMemoryManager] = None,
                 force_cpu: bool = False,
                 device: Union[str, int] = 'auto',
                 **kwargs):
        """
        Parameters
        ----------
        config : LambdaStructureConfig
            Ë®àÁÆóË®≠ÂÆö
        memory_manager : GPUMemoryManager
            „É°„É¢„É™ÁÆ°ÁêÜ„Ç§„É≥„Çπ„Çø„É≥„ÇπÔºàË¶™„ÇØ„É©„Çπ„Åß„ÇÇÂàùÊúüÂåñ„Åï„Çå„ÇãÔºâ
        force_cpu : bool
            Âº∑Âà∂ÁöÑ„Å´CPU„É¢„Éº„Éâ„Å´„Åô„Çã
        device : str or int
            ‰ΩøÁî®„Åô„Çã„Éá„Éê„Ç§„Çπ
        """
        # Ë®≠ÂÆö„Çí‰øùÂ≠ò
        self.config = config or LambdaStructureConfig()
        
        # Ë¶™„ÇØ„É©„Çπ„ÅÆÂàùÊúüÂåñ„ÇíÂëº„Å∂ - „Åì„Çå„Åßself.xp„ÅåË®≠ÂÆö„Åï„Çå„ÇãÔºÅ
        super().__init__(
            device=device,
            force_cpu=force_cpu,
            mixed_precision=self.config.use_mixed_precision,
            profile=self.config.profile,
            memory_manager_config=None,  # Ë¶™„ÇØ„É©„Çπ„Åß„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Åï„Çå„Çã
            **kwargs
        )
        
        # ËøΩÂä†„ÅÆÂàùÊúüÂåñ
        self._cache = {} if self.config.cache_intermediates else None
        
        # „Ç´„Çπ„Çø„É†„É°„É¢„É™„Éû„Éç„Éº„Ç∏„É£„Éº„Åå„ÅÇ„Çå„Å∞‰∏äÊõ∏„Åç
        if memory_manager is not None:
            self.memory_manager = memory_manager
        
        # „É≠„Ç∞Âá∫ÂäõÔºàxp„ÅåÊ≠£„Åó„ÅèË®≠ÂÆö„Åï„Çå„Å¶„Çã„ÅãÁ¢∫Ë™çÔºâ
        logger.info(f"‚úÖ LambdaStructuresGPU initialized:")
        logger.info(f"   Backend: {self.xp.__name__}")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   GPU Mode: {self.is_gpu}")
        
        # ÂÆâÂÖ®Á¢∫Ë™ç
        self._verify_backend()
    
    def _verify_backend(self):
        """„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅåÊ≠£„Åó„ÅèË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç"""
        try:
            # xp„ÅÆÂ≠òÂú®Á¢∫Ë™ç
            if not hasattr(self, 'xp') or self.xp is None:
                raise RuntimeError("xp not initialized by parent class!")
            
            # Âü∫Êú¨ÁöÑ„Å™ÊºîÁÆó„ÉÜ„Çπ„Éà
            test = self.xp.array([1, 2, 3])
            diff = self.xp.diff(test)
            norm = self.xp.linalg.norm(diff)
            
            logger.info(f"‚úÖ Backend test passed: diff={diff}, norm={norm}")
            
        except Exception as e:
            logger.error(f"‚ùå Backend verification failed: {e}")
            raise
    
    def compute_lambda_structures(self,
                                trajectory: np.ndarray,
                                md_features: Dict[str, np.ndarray],
                                window_steps: int) -> Dict[str, np.ndarray]:
        """
        Lambda¬≥ÊßãÈÄ†„ÇíGPU„ÅßË®àÁÆó
        
        Parameters
        ----------
        trajectory : np.ndarray
            MD„Éà„É©„Ç∏„Çß„ÇØ„Éà„É™
        md_features : Dict[str, np.ndarray]
            MDÁâπÂæ¥ÈáèÔºàcom_positions, rmsd, radius_of_gyrationÁ≠âÔºâ
        window_steps : int
            „Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫
            
        Returns
        -------
        Dict[str, np.ndarray]
            LambdaÊßãÈÄ†ËæûÊõ∏
        """
        try:
            with self.timer('compute_lambda_structures'):
                logger.info(f"üöÄ Computing Lambda¬≥ structures (window={window_steps}, mode={'GPU' if self.is_gpu else 'CPU'})")
                
                # ÂÖ•ÂäõÊ§úË®º
                if 'com_positions' not in md_features:
                    raise ValueError("com_positions not found in md_features")
                
                # GPUËª¢ÈÄÅÔºàË¶™„ÇØ„É©„Çπ„ÅÆ„É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®Ôºâ
                positions_gpu = self.to_gpu(md_features['com_positions'])
                n_frames = positions_gpu.shape[0]
                
                logger.debug(f"Processing {n_frames} frames")
                
                # 1. ŒõF - ÊßãÈÄ†„Éï„É≠„Éº
                with self.timer('lambda_F'):
                    lambda_F, lambda_F_mag = self._compute_lambda_F(positions_gpu)
                
                # 2. ŒõFF - ‰∫åÊ¨°ÊßãÈÄ†
                with self.timer('lambda_FF'):
                    lambda_FF, lambda_FF_mag = self._compute_lambda_FF(lambda_F)
                
                # 3. œÅT - „ÉÜ„É≥„Ç∑„Éß„É≥Â†¥
                with self.timer('rho_T'):
                    rho_T = self._compute_rho_T(positions_gpu, window_steps)
                
                # 4. Q_Œõ - „Éà„Éù„É≠„Ç∏„Ç´„É´„ÉÅ„É£„Éº„Ç∏
                with self.timer('Q_lambda'):
                    Q_lambda, Q_cumulative = self._compute_Q_lambda(lambda_F, lambda_F_mag)
                
                # 5. œÉ‚Çõ - ÊßãÈÄ†ÂêåÊúüÁéá
                with self.timer('sigma_s'):
                    sigma_s = self._compute_sigma_s(md_features, window_steps)
                
                # 6. ÊßãÈÄ†ÁöÑ„Ç≥„Éí„Éº„É¨„É≥„Çπ
                with self.timer('coherence'):
                    coherence = self._compute_coherence(lambda_F, window_steps)
                
                # ÁµêÊûú„ÇíCPU„Å´Ëª¢ÈÄÅÔºàË¶™„ÇØ„É©„Çπ„ÅÆ„É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®Ôºâ
                results = {
                    'lambda_F': self.to_cpu(lambda_F),
                    'lambda_F_mag': self.to_cpu(lambda_F_mag),
                    'lambda_FF': self.to_cpu(lambda_FF),
                    'lambda_FF_mag': self.to_cpu(lambda_FF_mag),
                    'rho_T': self.to_cpu(rho_T),
                    'Q_lambda': self.to_cpu(Q_lambda),
                    'Q_cumulative': self.to_cpu(Q_cumulative),
                    'sigma_s': self.to_cpu(sigma_s),
                    'structural_coherence': self.to_cpu(coherence)
                }
                
                # Áµ±Ë®àÊÉÖÂ†±Âá∫Âäõ
                self._print_statistics(results)
                
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Error in compute_lambda_structures: {e}")
            logger.error(f"   xp={self.xp if hasattr(self, 'xp') else 'NOT SET'}")
            logger.error(f"   is_gpu={self.is_gpu if hasattr(self, 'is_gpu') else 'NOT SET'}")
            if self.is_gpu and "out of memory" in str(e).lower():
                logger.info("üí° Try reducing batch_size or use force_cpu=True")
            raise
    
    def _compute_lambda_F(self, positions: NDArray) -> Tuple[NDArray, NDArray]:
        """ŒõF - ÊßãÈÄ†„Éï„É≠„ÉºË®àÁÆó"""
        # „Éï„É¨„Éº„É†Èñì„ÅÆÂ∑ÆÂàÜ„Éô„ÇØ„Éà„É´
        lambda_F = self.xp.diff(positions, axis=0)
        
        # Â§ß„Åç„ÅïÔºà„Éé„É´„É†Ôºâ
        lambda_F_mag = self.xp.linalg.norm(lambda_F, axis=1)
        
        return lambda_F, lambda_F_mag
    
    def _compute_lambda_FF(self, lambda_F: NDArray) -> Tuple[NDArray, NDArray]:
        """ŒõFF - ‰∫åÊ¨°ÊßãÈÄ†„Éï„É≠„ÉºË®àÁÆó"""
        # ‰∫åÊ¨°Â∑ÆÂàÜÔºàÂä†ÈÄüÂ∫¶ÁöÑ„Å™ÈáèÔºâ
        lambda_FF = self.xp.diff(lambda_F, axis=0)
        
        # Â§ß„Åç„Åï
        lambda_FF_mag = self.xp.linalg.norm(lambda_FF, axis=1)
        
        return lambda_FF, lambda_FF_mag
    
    def _compute_rho_T(self, positions: NDArray, window_steps: int) -> NDArray:
        """œÅT - „ÉÜ„É≥„Ç∑„Éß„É≥Â†¥Ë®àÁÆó"""
        n_frames = len(positions)
        
        # „Ç´„Çπ„Çø„É†„Ç´„Éº„Éç„É´„Åå‰Ωø„Åà„ÇãÂ†¥Âêà
        if self.is_gpu and tension_field_kernel is not None:
            try:
                return tension_field_kernel(positions, window_steps)
            except Exception as e:
                logger.warning(f"Custom kernel failed: {e}, using fallback")
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂÆüË£Ö
        rho_T = self.xp.zeros(n_frames)
        
        for step in range(n_frames):
            start = max(0, step - window_steps)
            end = min(n_frames, step + window_steps + 1)
            local_positions = positions[start:end]
            
            if len(local_positions) > 1:
                # ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆ„Éà„É¨„Éº„ÇπÔºàaxisÊòéÁ§∫Ôºâ
                centered = local_positions - self.xp.mean(local_positions, axis=0, keepdims=True)
                cov = self.xp.cov(centered.T)
                rho_T[step] = self.xp.trace(cov)
        
        return rho_T
    
    def _compute_Q_lambda(self, 
                         lambda_F: NDArray, 
                         lambda_F_mag: NDArray) -> Tuple[NDArray, NDArray]:
        """Q_Œõ - „Éà„Éù„É≠„Ç∏„Ç´„É´„ÉÅ„É£„Éº„Ç∏Ë®àÁÆó"""
        n_steps = len(lambda_F_mag)
        
        # „Ç´„Çπ„Çø„É†„Ç´„Éº„Éç„É´„Åå‰Ωø„Åà„ÇãÂ†¥Âêà
        if self.is_gpu and topological_charge_kernel is not None:
            try:
                Q_lambda = topological_charge_kernel(lambda_F, lambda_F_mag)
                Q_cumulative = self.xp.cumsum(Q_lambda)
                return Q_lambda, Q_cumulative
            except Exception as e:
                logger.warning(f"Custom kernel failed: {e}, using fallback")
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂÆüË£Ö
        Q_lambda = self.xp.zeros(n_steps)
        
        for step in range(1, n_steps):
            if lambda_F_mag[step] > 1e-10 and lambda_F_mag[step-1] > 1e-10:
                # Ê≠£Ë¶èÂåñ„Éô„ÇØ„Éà„É´
                v1 = lambda_F[step-1] / lambda_F_mag[step-1]
                v2 = lambda_F[step] / lambda_F_mag[step]
                
                # ËßíÂ∫¶Ë®àÁÆó
                cos_angle = self.xp.clip(self.xp.dot(v1, v2), -1, 1)
                angle = self.xp.arccos(cos_angle)
                
                # 2DÂõûËª¢ÊñπÂêëÔºàÁ¨¶Âè∑‰ªò„ÅçËßíÂ∫¶Ôºâ
                if len(v1) >= 2:  # 2D‰ª•‰∏ä
                    cross_z = v1[0]*v2[1] - v1[1]*v2[0]
                    signed_angle = angle if cross_z >= 0 else -angle
                else:
                    signed_angle = angle
                
                Q_lambda[step] = signed_angle / (2 * self.xp.pi)
        
        Q_cumulative = self.xp.cumsum(Q_lambda)
        
        return Q_lambda, Q_cumulative
    
    def _compute_sigma_s(self, 
                        md_features: Dict[str, np.ndarray],
                        window_steps: int) -> NDArray:
        """œÉ‚Çõ - ÊßãÈÄ†ÂêåÊúüÁéáË®àÁÆó"""
        # ÂøÖË¶Å„Å™ÁâπÂæ¥Èáè„Åå„Å™„ÅÑÂ†¥Âêà
        if 'rmsd' not in md_features or 'radius_of_gyration' not in md_features:
            n_frames = len(md_features.get('com_positions', []))
            return self.xp.zeros(n_frames)
        
        # GPUËª¢ÈÄÅ
        rmsd = self.to_gpu(md_features['rmsd'])
        rg = self.to_gpu(md_features['radius_of_gyration'])
        n_frames = len(rmsd)
        
        sigma_s = self.xp.zeros(n_frames)
        
        # „Çπ„É©„Ç§„Éá„Ç£„É≥„Ç∞„Ç¶„Ç£„É≥„Éâ„Ç¶„ÅßÁõ∏Èñ¢Ë®àÁÆó
        for step in range(n_frames):
            start = max(0, step - window_steps)
            end = min(n_frames, step + window_steps + 1)
            
            if end - start > 2:  # Áõ∏Èñ¢Ë®àÁÆó„Å´ÊúÄ‰Ωé3ÁÇπÂøÖË¶Å
                local_rmsd = rmsd[start:end]
                local_rg = rg[start:end]
                
                # Ê®ôÊ∫ñÂÅèÂ∑Æ„ÉÅ„Çß„ÉÉ„ÇØ
                std_rmsd = self.xp.std(local_rmsd)
                std_rg = self.xp.std(local_rg)
                
                if std_rmsd > 1e-10 and std_rg > 1e-10:
                    # Áõ∏Èñ¢‰øÇÊï∞
                    corr_matrix = self.xp.corrcoef(
                        self.xp.stack([local_rmsd, local_rg])
                    )
                    sigma_s[step] = self.xp.abs(corr_matrix[0, 1])
        
        return sigma_s
    
    def _compute_coherence(self, 
                          lambda_F: NDArray,
                          window: int) -> NDArray:
        """ÊßãÈÄ†ÁöÑ„Ç≥„Éí„Éº„É¨„É≥„ÇπË®àÁÆó"""
        n_frames = len(lambda_F)
        coherence = self.xp.zeros(n_frames)
        
        for i in range(window, n_frames - window):
            local_F = lambda_F[i-window:i+window]
            
            # Âπ≥ÂùáÊñπÂêë„Éô„ÇØ„Éà„É´
            mean_dir = self.xp.mean(local_F, axis=0, keepdims=True).ravel()
            mean_norm = self.xp.linalg.norm(mean_dir)
            
            if mean_norm > 1e-10:
                mean_dir /= mean_norm
                
                # ÂêÑ„Éô„ÇØ„Éà„É´„Å®„ÅÆÂÜÖÁ©çÔºà„Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ôºâ
                norms = self.xp.linalg.norm(local_F, axis=1)
                valid_mask = norms > 1e-10
                
                if self.xp.any(valid_mask):
                    normalized_F = local_F[valid_mask] / norms[valid_mask, self.xp.newaxis]
                    coherences = self.xp.dot(normalized_F, mean_dir)
                    coherence[i] = self.xp.mean(coherences)
        
        return coherence
    
    def _print_statistics(self, results: Dict[str, np.ndarray]):
        """Áµ±Ë®àÊÉÖÂ†±„ÇíÂá∫Âäõ"""
        try:
            logger.info("üìä Lambda Structure Statistics:")
            
            for key in ['lambda_F_mag', 'lambda_FF_mag', 'rho_T', 'Q_cumulative', 'sigma_s']:
                if key in results and len(results[key]) > 0:
                    data = results[key]
                    logger.info(f"   {key}: min={np.min(data):.3e}, max={np.max(data):.3e}, "
                               f"mean={np.mean(data):.3e}, std={np.std(data):.3e}")
                
        except Exception as e:
            logger.warning(f"Failed to print statistics: {e}")
    
    def compute_adaptive_window_size(self,
                                   md_features: Dict[str, np.ndarray],
                                   lambda_structures: Dict[str, np.ndarray],
                                   n_frames: int,
                                   config: any) -> Dict[str, Union[int, float, dict]]:
        """ÈÅ©ÂøúÁöÑ„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫Ë®àÁÆó"""
        try:
            base_window = int(n_frames * config.window_scale)
            
            # Â§âÂãïÊÄß„É°„Éà„É™„ÇØ„ÇπË®àÁÆó
            volatility_metrics = {}
            
            # RMSDÂ§âÂãïÊÄß
            if 'rmsd' in md_features:
                rmsd = self.to_gpu(md_features['rmsd'])
                mean_val = float(self.xp.mean(rmsd))
                if abs(mean_val) > 1e-10:
                    volatility_metrics['rmsd'] = float(self.xp.std(rmsd) / mean_val)
                else:
                    volatility_metrics['rmsd'] = 0.0
            
            # Lambda FÂ§âÂãïÊÄß
            if 'lambda_F_mag' in lambda_structures:
                lf_mag = self.to_gpu(lambda_structures['lambda_F_mag'])
                mean_val = float(self.xp.mean(lf_mag))
                if abs(mean_val) > 1e-10:
                    volatility_metrics['lambda_f'] = float(self.xp.std(lf_mag) / mean_val)
                else:
                    volatility_metrics['lambda_f'] = 0.0
            
            # œÅTÂÆâÂÆöÊÄß
            if 'rho_T' in lambda_structures:
                rho_t = self.to_gpu(lambda_structures['rho_T'])
                mean_val = float(self.xp.mean(rho_t))
                if abs(mean_val) > 1e-10:
                    volatility_metrics['rho_t'] = float(self.xp.std(rho_t) / mean_val)
                else:
                    volatility_metrics['rho_t'] = 0.0
            
            # „Çπ„Ç±„Éº„É´„Éï„Ç°„ÇØ„Çø„ÉºË®àÁÆó
            scale_factor = 1.0
            
            if volatility_metrics.get('rmsd', 0) > 0.5:
                scale_factor *= 0.7
            elif volatility_metrics.get('rmsd', 0) < 0.1:
                scale_factor *= 1.5
            
            if volatility_metrics.get('lambda_f', 0) > 1.0:
                scale_factor *= 0.8
            elif volatility_metrics.get('lambda_f', 0) < 0.2:
                scale_factor *= 1.3
            
            # „Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫Ê±∫ÂÆö
            adaptive_window = int(base_window * scale_factor)
            adaptive_window = np.clip(adaptive_window, config.min_window, config.max_window)
            
            windows = {
                'primary': adaptive_window,
                'fast': max(config.min_window, adaptive_window // 2),
                'slow': min(config.max_window, adaptive_window * 2),
                'boundary': max(10, adaptive_window // 3),
                'scale_factor': scale_factor,
                'volatility_metrics': volatility_metrics
            }
            
            logger.info(f"üéØ Adaptive window sizes: primary={windows['primary']}, "
                       f"scale_factor={scale_factor:.2f}")
            
            return windows
            
        except Exception as e:
            logger.error(f"Failed to compute adaptive window size: {e}")
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
            return {
                'primary': 50,
                'fast': 25,
                'slow': 100,
                'boundary': 10,
                'scale_factor': 1.0,
                'volatility_metrics': {}
            }

# ===============================
# „Çπ„Çø„É≥„Éâ„Ç¢„É≠„É≥Èñ¢Êï∞
# ===============================

def compute_lambda_structures_gpu(trajectory: np.ndarray,
                                md_features: Dict[str, np.ndarray],
                                window_steps: int,
                                config: Optional[LambdaStructureConfig] = None,
                                memory_manager: Optional[GPUMemoryManager] = None,
                                force_cpu: bool = False) -> Dict[str, np.ndarray]:
    """Lambda¬≥ÊßãÈÄ†Ë®àÁÆó„ÅÆ„É°„Ç§„É≥Èñ¢Êï∞"""
    calculator = LambdaStructuresGPU(config, memory_manager, force_cpu=force_cpu)
    return calculator.compute_lambda_structures(trajectory, md_features, window_steps)

def compute_adaptive_window_size_gpu(md_features: Dict[str, np.ndarray],
                                   lambda_structures: Dict[str, np.ndarray],
                                   n_frames: int,
                                   config: any,
                                   force_cpu: bool = False) -> Dict[str, Union[int, float, dict]]:
    """ÈÅ©ÂøúÁöÑ„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫Ë®àÁÆó"""
    calculator = LambdaStructuresGPU(force_cpu=force_cpu)
    return calculator.compute_adaptive_window_size(
        md_features, lambda_structures, n_frames, config
    )

# ===============================
# „Éò„É´„Éë„ÉºÈñ¢Êï∞
# ===============================

def compute_structural_coherence_gpu(lambda_F: np.ndarray,
                                   window: int = 50,
                                   force_cpu: bool = False) -> np.ndarray:
    """ÊßãÈÄ†ÁöÑ„Ç≥„Éí„Éº„É¨„É≥„ÇπË®àÁÆó"""
    calculator = LambdaStructuresGPU(force_cpu=force_cpu)
    lambda_F_gpu = calculator.to_gpu(lambda_F)
    coherence = calculator._compute_coherence(lambda_F_gpu, window)
    return calculator.to_cpu(coherence)

def compute_local_fractal_dimension_gpu(q_cumulative: np.ndarray,
                                       window: int = 50,
                                       force_cpu: bool = False) -> np.ndarray:
    """Â±ÄÊâÄ„Éï„É©„ÇØ„Çø„É´Ê¨°ÂÖÉË®àÁÆó"""
    from ..detection.boundary_detection_gpu import BoundaryDetectorGPU
    detector = BoundaryDetectorGPU(force_cpu=force_cpu)
    return detector._compute_fractal_dimensions_gpu(q_cumulative, window)

def compute_coupling_strength_gpu(q_cumulative: np.ndarray,
                                 window: int = 50,
                                 force_cpu: bool = False) -> np.ndarray:
    """ÁµêÂêàÂº∑Â∫¶Ë®àÁÆó"""
    from ..detection.boundary_detection_gpu import BoundaryDetectorGPU
    detector = BoundaryDetectorGPU(force_cpu=force_cpu)
    return detector._compute_coupling_strength_gpu(q_cumulative, window)

def compute_structural_entropy_gpu(rho_t: np.ndarray,
                                  window: int = 50,
                                  force_cpu: bool = False) -> np.ndarray:
    """ÊßãÈÄ†„Ç®„É≥„Éà„É≠„Éî„ÉºË®àÁÆó"""
    from ..detection.boundary_detection_gpu import BoundaryDetectorGPU
    detector = BoundaryDetectorGPU(force_cpu=force_cpu)
    return detector._compute_structural_entropy_gpu(rho_t, window)

# ===============================
# „Ç®„ÇØ„Çπ„Éù„Éº„Éà
# ===============================
__all__ = [
    'LambdaStructuresGPU',
    'LambdaStructureConfig',
    'compute_lambda_structures_gpu',
    'compute_adaptive_window_size_gpu',
    'compute_structural_coherence_gpu',
    'compute_local_fractal_dimension_gpu',
    'compute_coupling_strength_gpu',
    'compute_structural_entropy_gpu',
]
