"""
Tensor Operations for LambdaÂ³ (GPU Version)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

æ±ç”¨çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã®GPUå®Ÿè£…ã ã‚ˆã€œï¼ğŸ’•
ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ã‹ã€ç›¸é–¢è¨ˆç®—ã¨ã‹ã€ãªã‚“ã§ã‚‚é€Ÿã„ï¼

by ç’°ã¡ã‚ƒã‚“
"""

import numpy as np
import logging
from typing import Callable, Optional, Union, Tuple, List, Any
from functools import wraps
import warnings

# GPU imports
try:
    import cupy as cp
    from cupyx.scipy import ndimage as cp_ndimage
    from cupyx.scipy import signal as cp_signal
    HAS_GPU = True
except ImportError:
    HAS_GPU = False
    cp = None
    cp_ndimage = None
    cp_signal = None

# Local imports
from ..types import ArrayType, NDArray
from ..core import GPUBackend, GPUMemoryManager, profile_gpu

logger = logging.getLogger('lambda3_gpu.structures.tensor_operations')

# ===============================
# Tensor Operations GPU Class
# ===============================

class TensorOperationsGPU(GPUBackend):
    """
    ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã®GPUå®Ÿè£…ã‚¯ãƒ©ã‚¹
    LambdaÂ³ã§ä½¿ã†è‰²ã‚“ãªæ¼”ç®—ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã‚ˆã€œï¼
    """
    
    def __init__(self, 
                 memory_manager: Optional[GPUMemoryManager] = None,
                 **kwargs):
        """
        Parameters
        ----------
        memory_manager : GPUMemoryManager
            ãƒ¡ãƒ¢ãƒªç®¡ç†
        """
        super().__init__(**kwargs)
        self.memory_manager = memory_manager or GPUMemoryManager()
        
    @profile_gpu
    def compute_gradient(self,
                        array: NDArray,
                        axis: Optional[Union[int, Tuple[int, ...]]] = None,
                        edge_order: int = 1) -> NDArray:
        """
        å‹¾é…è¨ˆç®—ï¼ˆGPUç‰ˆï¼‰
        
        Parameters
        ----------
        array : array-like
            å…¥åŠ›é…åˆ—
        axis : int or tuple, optional
            å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹è»¸
        edge_order : int
            ç«¯ã§ã®ç²¾åº¦ï¼ˆ1 or 2ï¼‰
            
        Returns
        -------
        gradient : array
            å‹¾é…
        """
        array_gpu = self.to_gpu(array)
        
        if axis is None:
            # å…¨è»¸ã§å‹¾é…
            gradients = []
            for i in range(array_gpu.ndim):
                grad = self.xp.gradient(array_gpu, axis=i, edge_order=edge_order)
                gradients.append(grad)
            return gradients if self.is_gpu else [self.to_cpu(g) for g in gradients]
        else:
            gradient = self.xp.gradient(array_gpu, axis=axis, edge_order=edge_order)
            return gradient if self.is_gpu else self.to_cpu(gradient)
    
    @profile_gpu
    def compute_covariance(self,
                         x: NDArray,
                         y: Optional[NDArray] = None,
                         rowvar: bool = True,
                         bias: bool = False,
                         ddof: Optional[int] = None) -> NDArray:
        """
        å…±åˆ†æ•£è¡Œåˆ—è¨ˆç®—ï¼ˆGPUç‰ˆï¼‰
        
        Parameters
        ----------
        x : array-like
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        y : array-like, optional
            2ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆNoneãªã‚‰xã®è‡ªå·±å…±åˆ†æ•£ï¼‰
        rowvar : bool
            è¡ŒãŒå¤‰æ•°ãªã‚‰True
        bias : bool
            æ­£è¦åŒ–ã§N-1ã§ãªãNã‚’ä½¿ã†ã‹
        ddof : int, optional
            è‡ªç”±åº¦ã®å·®åˆ†
            
        Returns
        -------
        cov : array
            å…±åˆ†æ•£è¡Œåˆ—
        """
        x_gpu = self.to_gpu(x)
        
        if y is not None:
            y_gpu = self.to_gpu(y)
            # ã‚¹ã‚¿ãƒƒã‚¯
            if rowvar:
                data = self.xp.vstack([x_gpu, y_gpu])
            else:
                data = self.xp.hstack([x_gpu, y_gpu])
            cov = self.xp.cov(data, rowvar=rowvar, bias=bias, ddof=ddof)
        else:
            cov = self.xp.cov(x_gpu, rowvar=rowvar, bias=bias, ddof=ddof)
        
        return cov if self.is_gpu else self.to_cpu(cov)
    
    @profile_gpu
    def compute_correlation(self,
                          x: NDArray,
                          y: Optional[NDArray] = None,
                          rowvar: bool = True) -> NDArray:
        """
        ç›¸é–¢ä¿‚æ•°è¡Œåˆ—è¨ˆç®—ï¼ˆGPUç‰ˆï¼‰
        
        Parameters
        ----------
        x : array-like
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        y : array-like, optional
            2ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿
        rowvar : bool
            è¡ŒãŒå¤‰æ•°ãªã‚‰True
            
        Returns
        -------
        corr : array
            ç›¸é–¢ä¿‚æ•°è¡Œåˆ—
        """
        x_gpu = self.to_gpu(x)
        
        if y is not None:
            y_gpu = self.to_gpu(y)
            if rowvar:
                data = self.xp.vstack([x_gpu, y_gpu])
            else:
                data = self.xp.hstack([x_gpu, y_gpu])
            corr = self.xp.corrcoef(data, rowvar=rowvar)
        else:
            corr = self.xp.corrcoef(x_gpu, rowvar=rowvar)
        
        return corr if self.is_gpu else self.to_cpu(corr)
    
    def sliding_window_operation(self,
                               array: NDArray,
                               window_size: int,
                               operation: Union[str, Callable],
                               axis: int = 0,
                               step: int = 1,
                               mode: str = 'valid') -> NDArray:
        """
        ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ¼”ç®—ï¼ˆGPUç‰ˆï¼‰
        
        Parameters
        ----------
        array : array-like
            å…¥åŠ›é…åˆ—
        window_size : int
            ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        operation : str or callable
            æ¼”ç®—ï¼ˆ'mean', 'std', 'max', 'min' ã¾ãŸã¯é–¢æ•°ï¼‰
        axis : int
            ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é©ç”¨ã™ã‚‹è»¸
        step : int
            ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º
        mode : str
            ç«¯ã®å‡¦ç†ï¼ˆ'valid', 'same'ï¼‰
            
        Returns
        -------
        result : array
            æ¼”ç®—çµæœ
        """
        array_gpu = self.to_gpu(array)
        
        # çµ„ã¿è¾¼ã¿æ¼”ç®—
        if isinstance(operation, str):
            op_func = self._get_operation_func(operation)
        else:
            op_func = operation
        
        # çµæœã®å½¢çŠ¶ã‚’è¨ˆç®—
        if mode == 'valid':
            out_size = (array_gpu.shape[axis] - window_size + 1) // step
        else:  # 'same'
            out_size = array_gpu.shape[axis] // step
        
        # å‡ºåŠ›é…åˆ—
        out_shape = list(array_gpu.shape)
        out_shape[axis] = out_size
        result = self.xp.empty(tuple(out_shape))
        
        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‡¦ç†
        with self.timer('sliding_window'):
            for i in range(out_size):
                start_idx = i * step
                end_idx = min(start_idx + window_size, array_gpu.shape[axis])
                
                # ã‚¹ãƒ©ã‚¤ã‚¹ä½œæˆ
                slices = [slice(None)] * array_gpu.ndim
                slices[axis] = slice(start_idx, end_idx)
                window_data = array_gpu[tuple(slices)]
                
                # æ¼”ç®—å®Ÿè¡Œ
                window_result = op_func(window_data, axis=axis)
                
                # çµæœã‚’æ ¼ç´
                out_slices = [slice(None)] * result.ndim
                out_slices[axis] = i
                result[tuple(out_slices)] = window_result
        
        return result if self.is_gpu else self.to_cpu(result)
    
    def _get_operation_func(self, operation: str) -> Callable:
        """æ¼”ç®—åã‹ã‚‰é–¢æ•°ã‚’å–å¾—"""
        ops = {
            'mean': self.xp.mean,
            'std': self.xp.std,
            'var': self.xp.var,
            'max': self.xp.max,
            'min': self.xp.min,
            'sum': self.xp.sum,
            'median': self.xp.median
        }
        
        if operation not in ops:
            raise ValueError(f"Unknown operation: {operation}")
        
        return ops[operation]
    
    def batch_tensor_operation(self,
                             tensors: List[NDArray],
                             operation: Callable,
                             batch_size: Optional[int] = None,
                             concat_axis: Optional[int] = 0,
                             **kwargs) -> Union[NDArray, List]:
        """
        è¤‡æ•°ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒƒãƒæ¼”ç®—
        
        Parameters
        ----------
        tensors : list of arrays
            å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒªã‚¹ãƒˆ
        operation : callable
            å„ãƒãƒƒãƒã«é©ç”¨ã™ã‚‹æ¼”ç®—
        batch_size : int, optional
            ãƒãƒƒãƒã‚µã‚¤ã‚º
        concat_axis : int, optional
            çµæœã‚’çµåˆã™ã‚‹è»¸ï¼ˆNoneãªã‚‰ãƒªã‚¹ãƒˆã§è¿”ã™ï¼‰
        **kwargs
            operationã«æ¸¡ã™è¿½åŠ å¼•æ•°
            
        Returns
        -------
        result : array or list
            æ¼”ç®—çµæœ
        """
        n_tensors = len(tensors)
        
        if batch_size is None:
            # ãƒ¡ãƒ¢ãƒªã‹ã‚‰è‡ªå‹•æ±ºå®š
            sample_size = tensors[0].nbytes
            batch_size = self.memory_manager.estimate_batch_size(
                tensors[0].shape, 
                operations_multiplier=2.0
            )
            batch_size = min(batch_size, n_tensors)
        
        results = []
        
        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, n_tensors, batch_size):
            end = min(i + batch_size, n_tensors)
            batch_tensors = tensors[i:end]
            
            # GPUè»¢é€
            if self.is_gpu:
                batch_gpu = [self.to_gpu(t) for t in batch_tensors]
            else:
                batch_gpu = batch_tensors
            
            # æ¼”ç®—å®Ÿè¡Œ
            with self.timer(f'batch_operation_{i//batch_size}'):
                batch_results = operation(batch_gpu, **kwargs)
            
            # CPUè»¢é€ï¼ˆå¿…è¦ãªã‚‰ï¼‰
            if self.is_gpu and isinstance(batch_results, list):
                batch_results = [self.to_cpu(r) for r in batch_results]
            elif self.is_gpu:
                batch_results = self.to_cpu(batch_results)
            
            results.extend(batch_results if isinstance(batch_results, list) else [batch_results])
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if i % (batch_size * 5) == 0:
                self.clear_memory()
        
        # çµæœã®çµåˆ
        if concat_axis is not None and len(results) > 0:
            if isinstance(results[0], (np.ndarray, cp.ndarray)):
                return np.concatenate(results, axis=concat_axis)
        
        return results
    
    @profile_gpu
    def convolution(self,
                   array: NDArray,
                   kernel: NDArray,
                   mode: str = 'same',
                   method: str = 'auto') -> NDArray:
        """
        ç•³ã¿è¾¼ã¿æ¼”ç®—ï¼ˆGPUç‰ˆï¼‰
        
        Parameters
        ----------
        array : array-like
            å…¥åŠ›é…åˆ—
        kernel : array-like
            ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«
        mode : str
            å‡ºåŠ›ã‚µã‚¤ã‚ºï¼ˆ'full', 'valid', 'same'ï¼‰
        method : str
            è¨ˆç®—æ–¹æ³•ï¼ˆ'auto', 'direct', 'fft'ï¼‰
            
        Returns
        -------
        result : array
            ç•³ã¿è¾¼ã¿çµæœ
        """
        array_gpu = self.to_gpu(array)
        kernel_gpu = self.to_gpu(kernel)
        
        if self.is_gpu and HAS_GPU:
            # CuPyã®é«˜é€Ÿç•³ã¿è¾¼ã¿
            if array_gpu.ndim == 1:
                result = cp_signal.convolve(array_gpu, kernel_gpu, mode=mode, method=method)
            else:
                result = cp_signal.convolve2d(array_gpu, kernel_gpu, mode=mode)
        else:
            # NumPyç‰ˆ
            from scipy import signal
            if array_gpu.ndim == 1:
                result = signal.convolve(array_gpu, kernel_gpu, mode=mode, method=method)
            else:
                result = signal.convolve2d(array_gpu, kernel_gpu, mode=mode)
        
        return result if self.is_gpu else self.to_cpu(result)
    
    @profile_gpu
    def gaussian_filter(self,
                       array: NDArray,
                       sigma: Union[float, Tuple[float, ...]],
                       order: Union[int, Tuple[int, ...]] = 0,
                       mode: str = 'reflect',
                       truncate: float = 4.0) -> NDArray:
        """
        ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆGPUç‰ˆï¼‰
        
        Parameters
        ----------
        array : array-like
            å…¥åŠ›é…åˆ—
        sigma : float or tuple
            ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ã®æ¨™æº–åå·®
        order : int or tuple
            å¾®åˆ†ã®æ¬¡æ•°
        mode : str
            å¢ƒç•Œå‡¦ç†
        truncate : float
            ã‚«ãƒ¼ãƒãƒ«ã®æ‰“ã¡åˆ‡ã‚Š
            
        Returns
        -------
        result : array
            ãƒ•ã‚£ãƒ«ã‚¿çµæœ
        """
        array_gpu = self.to_gpu(array)
        
        if self.is_gpu and HAS_GPU:
            # CuPyã®ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿
            if array_gpu.ndim == 1:
                result = cp_ndimage.gaussian_filter1d(
                    array_gpu, sigma, order=order, mode=mode, truncate=truncate
                )
            else:
                result = cp_ndimage.gaussian_filter(
                    array_gpu, sigma, order=order, mode=mode, truncate=truncate
                )
        else:
            # SciPyç‰ˆ
            from scipy import ndimage
            if array_gpu.ndim == 1:
                result = ndimage.gaussian_filter1d(
                    array_gpu, sigma, order=order, mode=mode, truncate=truncate
                )
            else:
                result = ndimage.gaussian_filter(
                    array_gpu, sigma, order=order, mode=mode, truncate=truncate
                )
        
        return result if self.is_gpu else self.to_cpu(result)

# ===============================
# Standalone Functions
# ===============================

def compute_gradient_gpu(array: NDArray,
                       axis: Optional[Union[int, Tuple[int, ...]]] = None,
                       backend: Optional[GPUBackend] = None) -> Union[np.ndarray, List[np.ndarray]]:
    """å‹¾é…è¨ˆç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    ops = TensorOperationsGPU() if backend is None else TensorOperationsGPU(device=backend.device)
    return ops.compute_gradient(array, axis)

def compute_covariance_gpu(x: NDArray,
                         y: Optional[NDArray] = None,
                         backend: Optional[GPUBackend] = None) -> np.ndarray:
    """å…±åˆ†æ•£è¨ˆç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    ops = TensorOperationsGPU() if backend is None else TensorOperationsGPU(device=backend.device)
    result = ops.compute_covariance(x, y)
    return ops.to_cpu(result) if ops.is_gpu else result

def compute_correlation_gpu(x: NDArray,
                          y: Optional[NDArray] = None,
                          backend: Optional[GPUBackend] = None) -> np.ndarray:
    """ç›¸é–¢è¨ˆç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    ops = TensorOperationsGPU() if backend is None else TensorOperationsGPU(device=backend.device)
    result = ops.compute_correlation(x, y)
    return ops.to_cpu(result) if ops.is_gpu else result

def sliding_window_operation_gpu(array: NDArray,
                               window_size: int,
                               operation: Union[str, Callable],
                               axis: int = 0,
                               backend: Optional[GPUBackend] = None) -> np.ndarray:
    """ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ¼”ç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    ops = TensorOperationsGPU() if backend is None else TensorOperationsGPU(device=backend.device)
    result = ops.sliding_window_operation(array, window_size, operation, axis)
    return ops.to_cpu(result) if ops.is_gpu else result

def batch_tensor_operation(tensors: List[NDArray],
                         operation: Callable,
                         batch_size: Optional[int] = None,
                         concat_axis: Optional[int] = 0,
                         backend: Optional[GPUBackend] = None,
                         **kwargs) -> Union[np.ndarray, List]:
    """ãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³é–¢æ•°"""
    ops = TensorOperationsGPU() if backend is None else TensorOperationsGPU(device=backend.device)
    return ops.batch_tensor_operation(tensors, operation, batch_size, concat_axis, **kwargs)

# ===============================
# Decorators for Tensor Operations
# ===============================

def tensorize(func: Callable) -> Callable:
    """
    é–¢æ•°ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–å¯¾å¿œã«ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    è‡ªå‹•çš„ã«GPU/CPUåˆ‡ã‚Šæ›¿ãˆã¨ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã†
    """
    @wraps(func)
    def wrapper(*args, backend: Optional[GPUBackend] = None, **kwargs):
        # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æ±ºå®š
        if backend is None:
            backend = GPUBackend()
        
        # å¼•æ•°ã‚’GPUã«è»¢é€
        gpu_args = []
        for arg in args:
            if isinstance(arg, (np.ndarray, cp.ndarray)):
                gpu_args.append(backend.to_gpu(arg))
            else:
                gpu_args.append(arg)
        
        # é–¢æ•°å®Ÿè¡Œ
        result = func(*gpu_args, **kwargs)
        
        # çµæœã‚’CPUã«æˆ»ã™
        if isinstance(result, (list, tuple)):
            return type(result)(
                backend.to_cpu(r) if HAS_GPU and isinstance(r, cp.ndarray) else r 
                for r in result
            )
        elif HAS_GPU and isinstance(result, cp.ndarray):
            return backend.to_cpu(result)
        else:
            return result
    
    return wrapper

# ===============================
# Performance Testing
# ===============================

def benchmark_tensor_operations(size: int = 10000):
    """ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    import time
    
    if not HAS_GPU:
        logger.warning("GPU not available for benchmarking")
        return
    
    ops = TensorOperationsGPU()
    
    logger.info(f"\n{'='*60}")
    logger.info("Tensor Operations Benchmarks")
    logger.info(f"{'='*60}")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    data = np.random.randn(size, 100).astype(np.float32)
    
    # 1. å‹¾é…è¨ˆç®—
    start = time.time()
    _ = ops.compute_gradient(data, axis=0)
    gpu_time = time.time() - start
    
    start = time.time()
    _ = np.gradient(data, axis=0)
    cpu_time = time.time() - start
    
    logger.info(f"\nGradient computation:")
    logger.info(f"  CPU: {cpu_time:.4f}s")
    logger.info(f"  GPU: {gpu_time:.4f}s")
    logger.info(f"  Speedup: {cpu_time/gpu_time:.1f}x")
    
    # 2. ç›¸é–¢è¨ˆç®—
    start = time.time()
    _ = ops.compute_correlation(data[:1000])
    gpu_time = time.time() - start
    
    start = time.time()
    _ = np.corrcoef(data[:1000])
    cpu_time = time.time() - start
    
    logger.info(f"\nCorrelation computation (1000x100):")
    logger.info(f"  CPU: {cpu_time:.4f}s")
    logger.info(f"  GPU: {gpu_time:.4f}s")
    logger.info(f"  Speedup: {cpu_time/gpu_time:.1f}x")
    
    # 3. ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    start = time.time()
    _ = ops.sliding_window_operation(data[:1000, 0], 50, 'mean')
    gpu_time = time.time() - start
    
    logger.info(f"\nSliding window (mean, window=50):")
    logger.info(f"  GPU: {gpu_time:.4f}s")
    
    logger.info(f"\n{'='*60}\n")
