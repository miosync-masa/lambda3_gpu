#!/usr/bin/env python3
"""
Maximum Report Generator from LambdaÂ³ GPU Results - Version 4.0.3 (RESTORED)
=============================================================================

æ—¢å­˜ã®è§£æçµæœã‹ã‚‰æœ€å¤§é™ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼
Version 4.0ã®æ–°æ©Ÿèƒ½ï¼ˆLambdaç•°å¸¸æ€§ã€åŸå­ãƒ¬ãƒ™ãƒ«è¨¼æ‹ ã€3ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ï¼‰å®Œå…¨å¯¾å¿œç‰ˆ

ã€ä¿®æ­£å†…å®¹ v4.0.3ã€‘
- v4.0.2ã®å…¨æ©Ÿèƒ½ã‚’å¾©å…ƒ
- quantum_events â†’ quantum_assessments ã«å¯¾å¿œ
- ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ä¿®æ­£ï¼ˆtop_XX_score_Y.YYå½¢å¼ï¼‰
- Bootstrapä¿¡é ¼åŒºé–“ã®å®Œå…¨çµ±åˆ
- ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã®Propagation Pathwayè§£æ
"""

import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
from scipy.signal import find_peaks
import logging

logger = logging.getLogger('maximum_report_generator')

def generate_maximum_report_from_results(
    lambda_result,
    two_stage_result=None,
    quantum_events=None,
    metadata=None,
    output_dir='./maximum_report',
    verbose=True
) -> str:
    """
    Version 3.0äº’æ›æ€§ã®ãŸã‚ã®æ—¢å­˜é–¢æ•°ï¼ˆç¶­æŒï¼‰
    """
    # æ—¢å­˜ã®å®Ÿè£…ã‚’ãã®ã¾ã¾ç¶­æŒ
    return _generate_v3_report(
        lambda_result, two_stage_result, quantum_events, 
        metadata, output_dir, verbose
    )

def generate_maximum_report_from_results_v4(
    lambda_result,
    two_stage_result=None,
    quantum_assessments=None,  # Version 4.0: QuantumAssessmentå‹ï¼ˆquantum_eventsã‹ã‚‰å¤‰æ›´ï¼‰
    metadata=None,
    output_dir='./maximum_report_v4',
    verbose=True
) -> str:
    """
    Version 4.0.3å¯¾å¿œã®æœ€å¼·ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ï¼ˆå¾©å…ƒç‰ˆï¼‰
    
    Parameters
    ----------
    lambda_result : MDLambda3Result
        LambdaÂ³ãƒã‚¯ãƒ­è§£æçµæœ
    two_stage_result : TwoStageLambda3Result, optional
        æ®‹åŸºãƒ¬ãƒ™ãƒ«è§£æçµæœ
    quantum_assessments : List[QuantumAssessment], optional
        Version 4.0ã®é‡å­è©•ä¾¡çµæœï¼ˆquantum_eventsã‹ã‚‰å¤‰æ›´ï¼‰
    metadata : dict, optional
        ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    output_dir : str
        å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    verbose : bool
        è©³ç´°å‡ºåŠ›
        
    Returns
    -------
    str
        ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdownå½¢å¼ï¼‰
    
    Version History
    ---------------
    4.0.0 : åˆæœŸãƒªãƒªãƒ¼ã‚¹
    4.0.1 : ResidueEventä¿®æ­£ã€Bootstrapçµ±åˆ
    4.0.2 : Propagation Pathwayè§£æè¿½åŠ 
    4.0.3 : quantum_assessmentså¯¾å¿œã€ã‚­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ä¿®æ­£
    """
    
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    if verbose:
        print("\n" + "="*80)
        print("ğŸŒŸ GENERATING MAXIMUM REPORT v4.0.3 - LambdaÂ³ Integrated Edition")
        print("="*80)
    
    report = """# ğŸŒŸ LambdaÂ³ GPU Complete Analysis Report - VERSION 4.0.3

## Executive Summary
"""
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    if metadata:
        report += f"""
- **System**: {metadata.get('system_name', 'Unknown')}
- **Temperature**: {metadata.get('temperature', 300)} K
- **Time step**: {metadata.get('time_step_ps', 100.0)} ps
"""
    
    report += f"""
- **Frames analyzed**: {lambda_result.n_frames}
- **Atoms**: {lambda_result.n_atoms}
- **Computation time**: {lambda_result.computation_time:.2f}s
- **Analysis version**: 4.0.3 (LambdaÂ³ Integrated - RESTORED + Bootstrap + Pathways)
"""
    
    # GPUæƒ…å ±
    if hasattr(lambda_result, 'gpu_info') and lambda_result.gpu_info:
        report += f"- **GPU**: {lambda_result.gpu_info.get('device_name', 'Unknown')}\n"
        if 'memory_used' in lambda_result.gpu_info:
            report += f"- **Memory used**: {lambda_result.gpu_info['memory_used']:.2f} GB\n"
        if 'speedup' in lambda_result.gpu_info:
            report += f"- **GPU speedup**: {lambda_result.gpu_info['speedup']:.1f}x\n"
    
    # ========================================
    # 1. LambdaÂ³çµæœã®å®Œå…¨è§£æï¼ˆæ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼‰
    # ========================================
    if verbose:
        print("\nğŸ“Š Extracting all LambdaÂ³ details...")
    
    report += "\n## ğŸ“Š LambdaÂ³ Macro Analysis (Complete)\n"
    
    all_events = []
    
    # æ§‹é€ å¢ƒç•Œã®è©³ç´°
    if hasattr(lambda_result, 'structural_boundaries'):
        boundaries = lambda_result.structural_boundaries.get('boundary_locations', [])
        if len(boundaries) > 0:
            report += f"\n### Structural Boundaries ({len(boundaries)} detected)\n"
            for i, loc in enumerate(boundaries):
                all_events.append({
                    'frame': loc,
                    'type': 'structural_boundary',
                    'score': 5.0
                })
                if i < 20:
                    report += f"- Boundary {i+1}: Frame {loc}\n"
            if len(boundaries) > 20:
                report += f"- ... and {len(boundaries)-20} more\n"
    
    # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ç ´ã‚Œ
    if hasattr(lambda_result, 'topological_breaks'):
        breaks = lambda_result.topological_breaks.get('break_points', [])
        if len(breaks) > 0:
            report += f"\n### Topological Breaks ({len(breaks)} detected)\n"
            for i, point in enumerate(breaks[:10]):
                report += f"- Break {i+1}: Frame {point}\n"
                all_events.append({
                    'frame': point,
                    'type': 'topological_break',
                    'score': 4.0
                })
    
    # ç•°å¸¸ã‚¹ã‚³ã‚¢ã®å®Œå…¨è§£æ
    if hasattr(lambda_result, 'anomaly_scores'):
        report += "\n### Anomaly Score Analysis (All Types)\n"
        
        score_stats = {}
        for score_type in ['global', 'structural', 'topological', 'combined', 'final_combined']:
            if score_type in lambda_result.anomaly_scores:
                scores = lambda_result.anomaly_scores[score_type]
                
                score_stats[score_type] = {
                    'mean': np.mean(scores),
                    'max': np.max(scores),
                    'min': np.min(scores),
                    'std': np.std(scores),
                    'median': np.median(scores)
                }
                
                for threshold in [2.0, 2.5, 3.0]:
                    peaks, properties = find_peaks(scores, height=threshold, distance=50)
                    if len(peaks) > 0:
                        score_stats[score_type][f'peaks_{threshold}'] = len(peaks)
                        
                        for peak in peaks[:10]:
                            all_events.append({
                                'frame': peak,
                                'type': f'{score_type}_peak_{threshold}',
                                'score': float(scores[peak])
                            })
        
        # çµ±è¨ˆè¡¨ç¤º
        report += "\n| Score Type | Mean | Max | Std | Median | Peaks(2.0) | Peaks(3.0) |\n"
        report += "|------------|------|-----|-----|--------|------------|------------|\n"
        for stype, stats in score_stats.items():
            report += f"| {stype} | {stats['mean']:.3f} | {stats['max']:.3f} | "
            report += f"{stats['std']:.3f} | {stats['median']:.3f} | "
            report += f"{stats.get('peaks_2.0', 0)} | {stats.get('peaks_3.0', 0)} |\n"
    
    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°
    if lambda_result.critical_events:
        report += f"\n### Critical Events ({len(lambda_result.critical_events)} detected)\n"
        for i, event in enumerate(lambda_result.critical_events):
            if isinstance(event, tuple) and len(event) >= 2:
                report += f"- Event {i+1}: Frames {event[0]}-{event[1]} "
                report += f"(duration: {event[1]-event[0]} frames)\n"
                all_events.append({
                    'frame': (event[0] + event[1]) // 2,
                    'type': 'critical',
                    'score': 10.0,
                    'duration': event[1] - event[0]
                })
    
    # Lambdaæ§‹é€ ã®è©³ç´°ï¼ˆæ­£ã—ã„ã‚­ãƒ¼åã§ï¼‰
    if hasattr(lambda_result, 'lambda_structures'):
        structures = lambda_result.lambda_structures
        report += "\n### Lambda Structure Components\n"
        
        # lambda_F_mag
        if 'lambda_F_mag' in structures:
            lambda_vals = structures['lambda_F_mag']
            report += f"- Lambda_F_mag: mean={np.mean(lambda_vals):.3f}, "
            report += f"std={np.std(lambda_vals):.3f}, "
            report += f"max={np.max(lambda_vals):.3f}\n"
        
        # rho_T
        if 'rho_T' in structures:
            rho_vals = structures['rho_T']
            report += f"- Rho_T (tension): mean={np.mean(rho_vals):.3f}, "
            report += f"max={np.max(rho_vals):.3f}\n"
        
        # sigma_s
        if 'sigma_s' in structures:
            sigma_vals = structures['sigma_s']
            report += f"- Sigma_S (sync): mean={np.mean(sigma_vals):.3f}, "
            report += f"max={np.max(sigma_vals):.3f}\n"
    
    # ã‚¤ãƒ™ãƒ³ãƒˆç·è¨ˆ
    report += f"\n### Total LambdaÂ³ Events Extracted: {len(all_events)}\n"
    event_types = Counter(e['type'] for e in all_events)
    for etype, count in event_types.most_common():
        report += f"- {etype}: {count}\n"
    
    # ========================================
    # 2.5. ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã®Pathwayè§£æï¼ˆæ–°è¦è¿½åŠ ï¼‰
    # ========================================
    if lambda_result.critical_events and two_stage_result and hasattr(two_stage_result, 'residue_analyses'):
        if verbose:
            print("\nğŸ”¬ Extracting event pathways...")
        
        report += "\n## ğŸ”¬ Structural Events with Propagation Pathways\n"
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
        report += "\n### ğŸ“… Events Timeline:\n"
        for i, event in enumerate(lambda_result.critical_events):
            if isinstance(event, tuple) and len(event) >= 2:
                start, end = event[0], event[1]
                duration = end - start
                report += f"- **Event {i+1}**: frames {start:6d}-{end:6d} ({duration:5d} frames)\n"
        
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°è§£æ
        report += "\n### ğŸ§¬ Detailed Event Analysis:\n"
        
        for i, event in enumerate(lambda_result.critical_events):
            if isinstance(event, tuple) and len(event) >= 2:
                start, end = event[0], event[1]
                
                # æ­£ã—ã„ã‚­ãƒ¼å½¢å¼ã§æ¢ã™ï¼
                found_key = None
                for key in two_stage_result.residue_analyses.keys():
                    if key.startswith(f"top_{i:02d}_"):  # "top_00_", "top_01_"...
                        found_key = key
                        break
                
                report += f"\n#### Event {i+1} (frames {start}-{end}):\n"
                
                if found_key:  # è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ã§å–å¾—ï¼
                    analysis = two_stage_result.residue_analyses[found_key]
                    
                    # Initiator residues
                    initiators = []
                    if hasattr(analysis, 'initiator_residues'):
                        initiators = analysis.initiator_residues[:5]  # Top 5
                        initiators_str = ', '.join([f"R{r+1}" for r in initiators])
                        report += f"- **ğŸ¯ Initiator residues**: {initiators_str}\n"
                    
                    # Propagation Pathways
                    if hasattr(analysis, 'network_result') and analysis.network_result:
                        network = analysis.network_result
                        if hasattr(network, 'causal_network') and network.causal_network:
                            # ãƒ‘ã‚¹ã‚¦ã‚§ã‚¤ã®æ§‹ç¯‰
                            pathways = _build_propagation_paths(
                                network.causal_network,
                                initiators
                            )
                            
                            if pathways:
                                report += f"- **ğŸ”„ Propagation Pathways**:\n"
                                for j, path in enumerate(pathways[:3], 1):  # Top 3 paths
                                    path_str = ' â†’ '.join([f"R{r+1}" for r in path])
                                    report += f"  - Path {j}: {path_str}\n"
                    
                    # çµ±è¨ˆæƒ…å ±
                    n_residues = len(analysis.residue_events) if hasattr(analysis, 'residue_events') else 0
                    n_causal = len(network.causal_network) if hasattr(network, 'causal_network') else 0
                    n_sync = len(network.sync_network) if hasattr(network, 'sync_network') else 0
                    n_async = len(network.async_strong_bonds) if hasattr(network, 'async_strong_bonds') else 0
                    
                    report += f"- **ğŸ“Š Statistics**:\n"
                    report += f"  - Residues involved: {n_residues}\n"
                    report += f"  - Causal links: {n_causal}\n"
                    report += f"  - Sync links: {n_sync}\n"
                    report += f"  - Async bonds: {n_async}\n"
                    
                    # Lambdaå¤‰åŒ–ã®çµ±è¨ˆï¼ˆã‚ã‚Œã°ï¼‰
                    if 'lambda_F_mag' in lambda_result.lambda_structures:
                        lambda_vals = lambda_result.lambda_structures['lambda_F_mag'][start:min(end, len(lambda_result.lambda_structures['lambda_F_mag']))]
                        if len(lambda_vals) > 0:
                            mean_lambda = np.mean(lambda_vals)
                            max_lambda = np.max(lambda_vals)
                            report += f"  - Mean Î›: {mean_lambda:.3f}\n"
                            report += f"  - Max Î›: {max_lambda:.3f}\n"
                else:
                    report += f"- *Residue analysis not available for this event*\n"
    
    # ========================================
    # 3. Two-Stageçµæœã®å®Œå…¨è§£æï¼ˆæ—¢å­˜ã€ä½ç½®èª¿æ•´ï¼‰
    # ========================================
    if two_stage_result:
        if verbose:
            print("\nğŸ§¬ Extracting all residue-level details...")
        
        report += "\n## ğŸ§¬ Residue-Level Analysis (Complete)\n"
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ
        if hasattr(two_stage_result, 'global_network_stats'):
            stats = two_stage_result.global_network_stats
            report += f"""
### Global Network Statistics
- **Total causal links**: {stats.get('total_causal_links', 0)}
- **Total sync links**: {stats.get('total_sync_links', 0)}
- **Total async bonds**: {stats.get('total_async_bonds', 0)}
- **Async/Causal ratio**: {stats.get('async_to_causal_ratio', 0):.2%}
- **Mean adaptive window**: {stats.get('mean_adaptive_window', 0):.1f} frames
"""
        
        # å…¨æ®‹åŸºã®é‡è¦åº¦ã‚¹ã‚³ã‚¢
        if hasattr(two_stage_result, 'global_residue_importance'):
            all_residues = sorted(
                two_stage_result.global_residue_importance.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            if all_residues:
                report += "\n### Complete Residue Importance Ranking\n"
                report += "| Rank | Residue | Score | Category |\n"
                report += "|------|---------|-------|----------|\n"
                
                for rank, (res_id, score) in enumerate(all_residues, 1):
                    if rank <= 5:
                        category = "ğŸ”¥ Critical"
                    elif rank <= 20:
                        category = "â­ Important"
                    elif rank <= len(all_residues) * 0.3:
                        category = "ğŸ“ Notable"
                    else:
                        category = "Normal"
                    
                    report += f"| {rank} | R{res_id+1} | {score:.4f} | {category} |\n"
        
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã®è¶…è©³ç´°è§£æï¼ˆä¿®æ­£ç‰ˆï¼‰
        if hasattr(two_stage_result, 'residue_analyses'):
            report += "\n### Event-by-Event Detailed Analysis\n"
            
            for event_idx, (event_name, analysis) in enumerate(two_stage_result.residue_analyses.items()):
                report += f"\n#### Event: {event_name}\n"
                
                if hasattr(analysis, 'frame_range'):
                    report += f"- Frame range: {analysis.frame_range[0]}-{analysis.frame_range[1]}\n"
                
                if hasattr(analysis, 'gpu_time'):
                    report += f"- GPU computation time: {analysis.gpu_time:.3f}s\n"
                
                if hasattr(analysis, 'residue_events'):
                    report += f"- **Residues involved**: {len(analysis.residue_events)}\n"
                    
                    # event_scoreã‚’ä½¿ç”¨ï¼ˆanomaly_scoreã§ã¯ãªãï¼‰
                    all_scores = []
                    for re in analysis.residue_events:
                        # ResidueEventã®å±æ€§ã‚’å®‰å…¨ã«ãƒã‚§ãƒƒã‚¯
                        if hasattr(re, 'residue_id'):
                            res_id = re.residue_id
                        elif hasattr(re, 'residues_involved') and re.residues_involved:
                            res_id = re.residues_involved[0]
                        else:
                            continue
                        
                        # ã‚¹ã‚³ã‚¢ã®å–å¾—ï¼ˆevent_scoreã‚’å„ªå…ˆï¼‰
                        if hasattr(re, 'event_score'):
                            score = re.event_score
                        elif hasattr(re, 'anomaly_score'):  # å¿µã®ãŸã‚äº’æ›æ€§
                            score = re.anomaly_score
                        else:
                            score = 0.0
                        
                        all_scores.append((res_id, score))
                    
                    all_scores.sort(key=lambda x: x[1], reverse=True)
                    
                    if all_scores:
                        report += "  - Top 10 anomalous residues:\n"
                        for res_id, score in all_scores[:10]:
                            report += f"    - R{res_id+1}: {score:.3f}\n"
                
                if hasattr(analysis, 'network_result'):
                    network = analysis.network_result
                    if hasattr(network, 'async_strong_bonds'):
                        report += f"- **Async bonds**: {len(network.async_strong_bonds)}\n"
                
                # Bootstrapä¿¡é ¼åŒºé–“ã®çµæœ
                if hasattr(analysis, 'confidence_results') and analysis.confidence_results:
                    report += "\n##### Bootstrap Confidence Intervals\n"
                    report += f"- **Total pairs analyzed**: {len(analysis.confidence_results)}\n"
                    
                    # æœ‰æ„ãªãƒšã‚¢ã®ã¿æŠ½å‡º
                    significant_results = [r for r in analysis.confidence_results 
                                         if r.get('is_significant', False)]
                    
                    if significant_results:
                        report += f"- **Significant pairs**: {len(significant_results)} "
                        report += f"({len(significant_results)/len(analysis.confidence_results)*100:.1f}%)\n"
                        
                        # Top 10æœ‰æ„ãªãƒšã‚¢
                        report += "\n###### Top Significant Correlations (95% CI):\n"
                        for i, conf in enumerate(significant_results[:10], 1):
                            from_res = conf.get('from_res', 0)
                            to_res = conf.get('to_res', 0)
                            corr = conf.get('correlation', 0)
                            ci_lower = conf.get('ci_lower', 0)
                            ci_upper = conf.get('ci_upper', 0)
                            
                            report += f"{i}. **R{from_res+1} â†” R{to_res+1}**: "
                            report += f"r={corr:.3f} [{ci_lower:.3f}, {ci_upper:.3f}]"
                            
                            # æ¨™æº–èª¤å·®ã¨ãƒã‚¤ã‚¢ã‚¹
                            if conf.get('standard_error'):
                                report += f" (SE={conf['standard_error']:.3f}"
                                if conf.get('bias'):
                                    report += f", bias={conf['bias']:.3f}"
                                report += ")"
                            report += "\n"
                    else:
                        report += "- No statistically significant pairs found\n"
    
    # ========================================
    # 4. é‡å­è©•ä¾¡ã®å®Œå…¨è§£æï¼ˆVersion 4.0æ–°æ©Ÿèƒ½ï¼‰
    # ========================================
    if quantum_assessments:
        if verbose:
            print("\nâš›ï¸ Extracting all quantum assessment details (v4.0)...")
        
        report += "\n## âš›ï¸ Quantum Assessment Analysis v4.0 (Complete)\n"
        
        total = len(quantum_assessments)
        quantum_count = sum(1 for a in quantum_assessments if getattr(a, 'is_quantum', False))
        
        percent = quantum_count/total*100 if total > 0 else 0
        report += f"""
### Overview
- **Total events analyzed**: {total}
- **Quantum events confirmed**: {quantum_count} ({percent:.1f}%)
"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒï¼ˆVersion 4.0ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ï¼‰
        if total > 0 and hasattr(quantum_assessments[0], 'pattern'):
            pattern_counts = Counter(getattr(a, 'pattern').value for a in quantum_assessments)
            report += "\n### Pattern Distribution (3-Pattern Classification)\n"
            for pattern, count in pattern_counts.items():
                quantum_in_pattern = sum(1 for a in quantum_assessments 
                                        if getattr(a, 'pattern').value == pattern and getattr(a, 'is_quantum', False))
                report += f"- **{pattern}**: {count} events, "
                percent = quantum_in_pattern/count*100 if count > 0 else 0
                report += f"{quantum_in_pattern} quantum ({percent:.1f}%)\n"
        
        # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ†å¸ƒï¼ˆVersion 4.0ï¼‰
        if total > 0 and hasattr(quantum_assessments[0], 'signature'):
            sig_counts = Counter(getattr(a, 'signature').value for a in quantum_assessments 
                               if getattr(a, 'signature').value != 'classical')
            if sig_counts:
                report += "\n### Quantum Signature Distribution\n"
                for sig, count in sig_counts.most_common():
                    report += f"- **{sig}**: {count}\n"
        
        # Lambdaç•°å¸¸æ€§çµ±è¨ˆï¼ˆVersion 4.0æ–°æ©Ÿèƒ½ï¼‰
        lambda_anomalies = [getattr(a, 'lambda_anomaly') for a in quantum_assessments 
                          if hasattr(a, 'lambda_anomaly') and getattr(a, 'lambda_anomaly') is not None]
        if lambda_anomalies:
            report += "\n### Lambda Anomaly Statistics (v4.0)\n"
            
            lambda_jumps = [la.lambda_jump for la in lambda_anomalies if hasattr(la, 'lambda_jump') and la.lambda_jump > 0]
            lambda_zscores = [la.lambda_zscore for la in lambda_anomalies if hasattr(la, 'lambda_zscore') and la.lambda_zscore > 0]
            rho_t_spikes = [la.rho_t_spike for la in lambda_anomalies if hasattr(la, 'rho_t_spike') and la.rho_t_spike > 0]
            
            if lambda_jumps:
                report += f"- **Lambda jumps**: mean={np.mean(lambda_jumps):.3f}, "
                report += f"max={np.max(lambda_jumps):.3f}\n"
            
            if lambda_zscores:
                report += f"- **Lambda Z-scores**: mean={np.mean(lambda_zscores):.2f}, "
                report += f"max={np.max(lambda_zscores):.2f}\n"
                report += f"  - Significant (>3Ïƒ): {sum(1 for z in lambda_zscores if z > 3)}\n"
                report += f"  - Highly significant (>5Ïƒ): {sum(1 for z in lambda_zscores if z > 5)}\n"
            
            if rho_t_spikes:
                report += f"- **ÏT spikes**: mean={np.mean(rho_t_spikes):.3f}, "
                report += f"max={np.max(rho_t_spikes):.3f}\n"
        
        # åŸå­ãƒ¬ãƒ™ãƒ«è¨¼æ‹ çµ±è¨ˆï¼ˆVersion 4.0æ–°æ©Ÿèƒ½ï¼‰
        atomic_evidences = [getattr(a, 'atomic_evidence') for a in quantum_assessments 
                          if hasattr(a, 'atomic_evidence') and getattr(a, 'atomic_evidence') is not None]
        if atomic_evidences:
            report += "\n### Atomic-Level Evidence Statistics (v4.0)\n"
            
            max_velocities = [ae.max_velocity for ae in atomic_evidences if hasattr(ae, 'max_velocity') and ae.max_velocity > 0]
            correlations = [ae.correlation_coefficient for ae in atomic_evidences 
                          if hasattr(ae, 'correlation_coefficient') and ae.correlation_coefficient > 0]
            
            if max_velocities:
                report += f"- **Max atomic velocities**: mean={np.mean(max_velocities):.2f} Ã…/ps, "
                report += f"max={np.max(max_velocities):.2f} Ã…/ps\n"
            
            if correlations:
                report += f"- **Atomic correlations**: mean={np.mean(correlations):.3f}, "
                report += f"max={np.max(correlations):.3f}\n"
                report += f"  - High correlation (>0.8): {sum(1 for c in correlations if c > 0.8)}\n"
            
            bond_anomaly_counts = [len(getattr(ae, 'bond_anomalies', [])) for ae in atomic_evidences]
            if any(bond_anomaly_counts):
                report += f"- **Bond anomalies detected**: {sum(bond_anomaly_counts)} total\n"
        
        # ä¿¡é ¼åº¦çµ±è¨ˆ
        confidences = [getattr(a, 'confidence', 0) for a in quantum_assessments if getattr(a, 'is_quantum', False)]
        if confidences:
            report += f"\n### Confidence Statistics\n"
            report += f"- Mean: {np.mean(confidences):.3f}\n"
            report += f"- Max: {np.max(confidences):.3f}\n"
            report += f"- Min: {np.min(confidences):.3f}\n"
            report += f"- Std: {np.std(confidences):.3f}\n"
        
        # Bellä¸ç­‰å¼ï¼ˆã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã‚¤ãƒ™ãƒ³ãƒˆï¼‰
        bell_values = [getattr(a, 'bell_inequality') for a in quantum_assessments 
                      if hasattr(a, 'bell_inequality') and getattr(a, 'bell_inequality') is not None]
        if bell_values:
            report += f"\n### Bell Inequality Analysis (Cascade Events)\n"
            report += f"- Events with Bell test: {len(bell_values)}\n"
            violations = sum(1 for b in bell_values if b > 2.0)
            report += f"- Violations (S > 2): {violations} ({violations/len(bell_values)*100:.1f}%)\n"
            report += f"- Max CHSH value: {np.max(bell_values):.3f}\n"
            report += f"- Classical bound: 2.000\n"
            report += f"- Tsirelson bound: {2*np.sqrt(2):.3f}\n"
        
        # å…¨é‡å­ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°ï¼ˆTOP 20ï¼‰
        report += "\n### Top Quantum Events (Detailed v4.0)\n"
        
        quantum_events = sorted([a for a in quantum_assessments if getattr(a, 'is_quantum', False)],
                              key=lambda x: getattr(x, 'confidence', 0), reverse=True)
        
        for i, assessment in enumerate(quantum_events[:20], 1):
            report += f"\n#### Event {i}\n"
            report += f"- **Pattern**: {getattr(assessment, 'pattern').value}\n"
            report += f"- **Signature**: {getattr(assessment, 'signature').value}\n"
            report += f"- **Confidence**: {getattr(assessment, 'confidence', 0):.1%}\n"
            report += f"- **Explanation**: {getattr(assessment, 'explanation', 'N/A')}\n"
            
            if hasattr(assessment, 'criteria_met') and assessment.criteria_met:
                report += f"- **Criteria met** ({len(assessment.criteria_met)}):\n"
                for criterion in assessment.criteria_met[:5]:
                    report += f"  - {criterion}\n"
            
            if hasattr(assessment, 'lambda_anomaly') and assessment.lambda_anomaly:
                la = assessment.lambda_anomaly
                if hasattr(la, 'lambda_zscore') and la.lambda_zscore > 3:
                    report += f"- **Lambda anomaly**: Z-score={la.lambda_zscore:.2f} "
                    if hasattr(la, 'statistical_rarity'):
                        report += f"(p={la.statistical_rarity:.4f})\n"
            
            if hasattr(assessment, 'atomic_evidence') and assessment.atomic_evidence:
                ae = assessment.atomic_evidence
                if hasattr(ae, 'correlation_coefficient') and ae.correlation_coefficient > 0.8:
                    report += f"- **Atomic correlation**: {ae.correlation_coefficient:.3f}\n"
            
            if hasattr(assessment, 'bell_inequality') and assessment.bell_inequality is not None:
                report += f"- **Bell inequality**: S={assessment.bell_inequality:.3f}\n"
    
    # ========================================
    # 4.5. Bootstrapçµ±è¨ˆã®ç·åˆè§£æï¼ˆæ—¢å­˜ï¼‰
    # ========================================
    all_confidence_results = []
    ci_widths = []  # äº‹å‰ã«å®šç¾©
    
    if two_stage_result and hasattr(two_stage_result, 'residue_analyses'):
        for analysis in two_stage_result.residue_analyses.values():
            if hasattr(analysis, 'confidence_results') and analysis.confidence_results:
                all_confidence_results.extend(analysis.confidence_results)
    
    if all_confidence_results:
        if verbose:
            print("\nğŸ“Š Extracting bootstrap statistics...")
        
        report += "\n## ğŸ“Š Bootstrap Statistical Analysis (Complete)\n"
        
        # å…¨ä½“çµ±è¨ˆ
        n_total = len(all_confidence_results)
        n_significant = sum(1 for r in all_confidence_results if r.get('is_significant', False))
        
        percent = n_significant/n_total*100 if n_total > 0 else 0
        report += f"""
### Overall Bootstrap Statistics
- **Total correlations tested**: {n_total}
- **Statistically significant**: {n_significant} ({percent:.1f}%)
- **Bootstrap iterations**: {all_confidence_results[0].get('n_bootstrap', 1000) if all_confidence_results else 'N/A'}
- **Confidence level**: 95%
"""
        
        # ç›¸é–¢ä¿‚æ•°ã®åˆ†å¸ƒ
        correlations = [r.get('correlation', 0) for r in all_confidence_results]
        if correlations:
            report += f"""
### Correlation Distribution
- **Mean correlation**: {np.mean(correlations):.3f}
- **Max correlation**: {np.max(correlations):.3f}
- **Min correlation**: {np.min(correlations):.3f}
- **Std deviation**: {np.std(correlations):.3f}
"""
        
        # ä¿¡é ¼åŒºé–“ã®å¹…ã®åˆ†æ
        ci_widths = [r.get('ci_upper', 0) - r.get('ci_lower', 0) 
                    for r in all_confidence_results if 'ci_upper' in r and 'ci_lower' in r]
        if ci_widths:
            report += f"""
### Confidence Interval Analysis
- **Mean CI width**: {np.mean(ci_widths):.3f}
- **Min CI width**: {np.min(ci_widths):.3f} (most precise)
- **Max CI width**: {np.max(ci_widths):.3f} (least precise)
"""
        
        # æœ€ã‚‚å¼·ã„ç›¸é–¢ã®ãƒˆãƒƒãƒ—10
        sorted_results = sorted(all_confidence_results, 
                              key=lambda x: abs(x.get('correlation', 0)), 
                              reverse=True)
        
        report += "\n### Strongest Correlations (All Events)\n"
        report += "| Rank | Pair | Correlation | 95% CI | Significant | SE |\n"
        report += "|------|------|-------------|---------|-------------|----|\n"
        
        for i, conf in enumerate(sorted_results[:15], 1):
            from_res = conf.get('from_res', 0)
            to_res = conf.get('to_res', 0)
            corr = conf.get('correlation', 0)
            ci_lower = conf.get('ci_lower', 0)
            ci_upper = conf.get('ci_upper', 0)
            is_sig = "âœ“" if conf.get('is_significant', False) else "âœ—"
            se = conf.get('standard_error', 0)
            
            report += f"| {i} | R{from_res+1}-R{to_res+1} | {corr:.3f} | "
            report += f"[{ci_lower:.3f}, {ci_upper:.3f}] | {is_sig} | {se:.3f} |\n"
        
        # ãƒã‚¤ã‚¢ã‚¹åˆ†æ
        biases = [abs(r.get('bias', 0)) for r in all_confidence_results if 'bias' in r]
        if biases:
            report += f"\n### Bootstrap Bias Analysis\n"
            report += f"- **Mean absolute bias**: {np.mean(biases):.4f}\n"
            report += f"- **Max absolute bias**: {np.max(biases):.4f}\n"
            
            high_bias = [r for r in all_confidence_results 
                        if 'bias' in r and abs(r['bias']) > 0.05]
            if high_bias:
                report += f"- **High bias pairs (|bias| > 0.05)**: {len(high_bias)}\n"
    
    # ========================================
    # 5. çµ±åˆçš„æ´å¯Ÿï¼ˆVersion 4.0å¼·åŒ–ç‰ˆï¼‰
    # ========================================
    if verbose:
        print("\nğŸ’¡ Generating integrated insights (v4.0)...")
    
    report += "\n## ğŸ’¡ Integrated Insights v4.0\n"
    
    insights = []
    
    # LambdaÂ³æ§‹é€ ç•°å¸¸
    if all_events:
        total_unique = len(set(e['frame'] for e in all_events))
        insights.append(f"âœ“ {total_unique} unique frames with structural anomalies")
        insights.append(f"âœ“ {len(all_events)} total structural events detected")
    
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ã‚¹ã‚¦ã‚§ã‚¤
    if lambda_result.critical_events:
        n_events = len(lambda_result.critical_events)
        insights.append(f"âœ“ {n_events} critical events with propagation pathways analyzed")
    
    # é‡å­æ€§ã®åˆ†æï¼ˆVersion 4.0ï¼‰
    if quantum_assessments:
        if quantum_count > 0:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®é‡å­æ€§
            for pattern in ['instantaneous', 'transition', 'cascade']:
                pattern_events = [a for a in quantum_assessments if getattr(a, 'pattern').value == pattern]
                if pattern_events:
                    q_count = sum(1 for a in pattern_events if getattr(a, 'is_quantum', False))
                    if q_count > 0:
                        insights.append(f"âœ“ {pattern}: {q_count}/{len(pattern_events)} quantum "
                                      f"({q_count/len(pattern_events)*100:.1f}%)")
            
            # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ¥
            if 'sig_counts' in locals():
                top_sig = sig_counts.most_common(1)[0]
                insights.append(f"âœ“ Most common quantum signature: {top_sig[0]} ({top_sig[1]} events)")
        
        # Lambdaç•°å¸¸ã®é‡è¦æ€§
        if lambda_anomalies:
            high_z = sum(1 for la in lambda_anomalies if hasattr(la, 'lambda_zscore') and la.lambda_zscore > 3)
            if high_z > 0:
                insights.append(f"âœ“ {high_z} events with significant Lambda anomaly (>3Ïƒ)")
        
        # åŸå­ãƒ¬ãƒ™ãƒ«ã®è¨¼æ‹ 
        if atomic_evidences:
            high_corr = sum(1 for ae in atomic_evidences if hasattr(ae, 'correlation_coefficient') and ae.correlation_coefficient > 0.8)
            if high_corr > 0:
                insights.append(f"âœ“ {high_corr} events with high atomic correlation (>0.8)")
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        total_links = (stats.get('total_causal_links', 0) + 
                      stats.get('total_sync_links', 0) +
                      stats.get('total_async_bonds', 0))
        if total_links > 0:
            insights.append(f"âœ“ {total_links} total network connections")
            
            if stats.get('async_to_causal_ratio', 0) > 0.5:
                insights.append(f"âœ“ High async/causal ratio ({stats['async_to_causal_ratio']:.1%})")
    
    # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—çµ±è¨ˆã®æ´å¯Ÿ
    if all_confidence_results:
        n_sig = sum(1 for r in all_confidence_results if r.get('is_significant', False))
        if n_sig > 0:
            insights.append(f"âœ“ {n_sig}/{len(all_confidence_results)} correlations statistically significant (95% CI)")
        
        # é«˜ç›¸é–¢ãƒšã‚¢
        high_corr = [r for r in all_confidence_results if abs(r.get('correlation', 0)) > 0.8]
        if high_corr:
            insights.append(f"âœ“ {len(high_corr)} pairs with |r| > 0.8 (strong correlation)")
        
        # ç‹­ã„ä¿¡é ¼åŒºé–“ï¼ˆç²¾åº¦ã®é«˜ã„æ¨å®šï¼‰
        if ci_widths:
            narrow_ci = sum(1 for w in ci_widths if w < 0.2)
            if narrow_ci > 0:
                insights.append(f"âœ“ {narrow_ci} pairs with narrow CI (width < 0.2, high precision)")
    
    for insight in insights:
        report += f"\n{insight}"
    
    # ========================================
    # 6. å‰µè–¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆææ¡ˆï¼ˆæ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼‰
    # ========================================
    all_hub_residues = []
    
    if two_stage_result and hasattr(two_stage_result, 'residue_analyses'):
        for analysis in two_stage_result.residue_analyses.values():
            if hasattr(analysis, 'initiator_residues'):
                all_hub_residues.extend(analysis.initiator_residues)
    
    if all_hub_residues:
        report += "\n\n## ğŸ’Š Drug Design Recommendations\n"
        
        hub_counts = Counter(all_hub_residues)
        top_targets = hub_counts.most_common(15)
        
        report += "\n### Primary Targets (Top Hub Residues)\n"
        
        for i, (res_id, count) in enumerate(top_targets, 1):
            report += f"\n{i}. **Residue {res_id+1}**\n"
            report += f"   - Hub frequency: {count} events\n"
            
            if hasattr(two_stage_result, 'global_residue_importance'):
                importance = two_stage_result.global_residue_importance.get(res_id, 0)
                report += f"   - Importance score: {importance:.3f}\n"
            
            if i <= 3:
                report += f"   - **Priority: CRITICAL TARGET**\n"
            elif i <= 10:
                report += f"   - Priority: Secondary target\n"
    
    # ========================================
    # 7. æ¨å¥¨äº‹é …ï¼ˆVersion 4.0å¼·åŒ–ç‰ˆï¼‰
    # ========================================
    report += "\n## ğŸ“‹ Recommendations v4.0\n"
    
    recommendations = []
    
    # ãƒãƒ–æ®‹åŸºãƒ™ãƒ¼ã‚¹
    hub_counts = None  # åˆæœŸåŒ–
    if all_hub_residues:
        hub_counts = Counter(all_hub_residues)
        top3 = [f"R{r+1}" for r, _ in hub_counts.most_common(3)]
        recommendations.append(f"Focus on residues {', '.join(top3)} for drug targeting")
    
    # é‡å­ã‚¤ãƒ™ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ï¼ˆVersion 4.0ï¼‰
    if quantum_assessments and quantum_count > 0:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥æ¨å¥¨
        if 'pattern_counts' in locals() and pattern_counts.get('instantaneous', 0) > 5:
            recommendations.append("Instantaneous transitions detected - consider quantum tunneling in drug design")
        if 'pattern_counts' in locals() and pattern_counts.get('cascade', 0) > 10:
            recommendations.append("Network cascades detected - target allosteric communication pathways")
        
        # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ¥æ¨å¥¨
        if 'sig_counts' in locals():
            if sig_counts.get('quantum_entanglement', 0) > 0:
                recommendations.append("Quantum entanglement signatures - non-local correlations present")
            if sig_counts.get('quantum_tunneling', 0) > 0:
                recommendations.append("Tunneling events detected - consider proton transfer mechanisms")
    
    # Lambdaç•°å¸¸ãƒ™ãƒ¼ã‚¹ï¼ˆVersion 4.0ï¼‰
    if lambda_anomalies:
        high_z = [la for la in lambda_anomalies if hasattr(la, 'lambda_zscore') and la.lambda_zscore > 5]
        if high_z:
            recommendations.append(f"Extreme structural anomalies detected ({len(high_z)} events with Z>5Ïƒ)")
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        if stats.get('total_async_bonds', 0) > 100:
            recommendations.append("Strong async bonds indicate allosteric mechanisms")
    
    # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨
    if all_confidence_results:
        # çµ±è¨ˆçš„ã«æœ‰æ„ãªå¼·ã„ç›¸é–¢
        strong_sig = [r for r in all_confidence_results 
                     if r.get('is_significant', False) and abs(r.get('correlation', 0)) > 0.7]
        if strong_sig:
            top_pairs = [(r['from_res'], r['to_res']) for r in strong_sig[:3]]
            pair_str = ', '.join([f"R{f+1}-R{t+1}" for f, t in top_pairs])
            recommendations.append(f"Statistically validated correlations at {pair_str} - potential allosteric pathway")
        
        # ä¿¡é ¼åŒºé–“ãŒç‹­ã„ï¼ˆç²¾åº¦ã®é«˜ã„ï¼‰ãƒšã‚¢
        if ci_widths:
            precise_pairs = [r for r in all_confidence_results 
                           if 'ci_upper' in r and 'ci_lower' in r 
                           and (r['ci_upper'] - r['ci_lower']) < 0.15]
            if precise_pairs:
                recommendations.append(f"High-precision estimates for {len(precise_pairs)} residue pairs - reliable targets")
    
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ã‚¹ã‚¦ã‚§ã‚¤ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨
    if lambda_result.critical_events and len(lambda_result.critical_events) > 3:
        recommendations.append(f"Multiple critical events ({len(lambda_result.critical_events)}) detected - consider multi-state drug design")
    
    for i, rec in enumerate(recommendations, 1):
        report += f"\n{i}. {rec}"
    
    # Version 4.0ã®æ–°ã—ã„æ´å¯Ÿ
    report += "\n\n### Version 4.0.3 Improvements (RESTORED + Bootstrap + Pathways)\n"
    report += "- Lambda structure anomaly as primary quantum indicator\n"
    report += "- 3-pattern classification (instantaneous/transition/cascade)\n"
    report += "- Atomic-level evidence integration\n"
    report += "- Fixed ResidueEvent attribute access (event_score)\n"
    report += "- Corrected Lambda structure key names (lambda_F_mag, rho_T)\n"
    report += "- **RESTORED: Bootstrap confidence intervals for all correlations**\n"
    report += "- **RESTORED: Statistical significance testing (95% CI)**\n"
    report += "- **RESTORED: Bias and standard error estimation**\n"
    report += "- **RESTORED: Event-based propagation pathway analysis**\n"
    report += "- **FIXED: quantum_events â†’ quantum_assessments compatibility**\n"
    report += "- **FIXED: Event key matching (top_XX_score_Y.YY format)**\n"
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    report += f"""

---
*Analysis Complete!*
*Version: 4.0.3 - LambdaÂ³ Integrated Edition (RESTORED)*
*Total report length: {len(report):,} characters*
*NO TIME, NO PHYSICS, ONLY STRUCTURE!*
"""
    
    # ========================================
    # ä¿å­˜ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    # ========================================
    
    # Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_path = output_path / 'maximum_report_v4.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    # JSONå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆãƒ‡ãƒ¼ã‚¿è§£æç”¨ï¼‰
    json_data = {
        'version': '4.0.3',
        'summary': {
            'n_frames': lambda_result.n_frames,
            'n_atoms': lambda_result.n_atoms,
            'computation_time': lambda_result.computation_time,
            'total_lambda_events': len(all_events),
            'event_types': dict(Counter(e['type'] for e in all_events))
        },
        'events': all_events[:100],
        'metadata': metadata if metadata else {}
    }
    
    # Version 4.0ã®é‡å­è©•ä¾¡ã‚µãƒãƒªãƒ¼
    if quantum_assessments:
        json_data['quantum_v4'] = {
            'total': total,
            'quantum': quantum_count,
            'patterns': dict(pattern_counts) if 'pattern_counts' in locals() else {},
            'signatures': dict(sig_counts) if 'sig_counts' in locals() else {},
            'mean_confidence': np.mean(confidences) if confidences else 0
        }
    
    # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—çµ±è¨ˆã‚µãƒãƒªãƒ¼
    if all_confidence_results:
        json_data['bootstrap_statistics'] = {
            'total_pairs': len(all_confidence_results),
            'significant_pairs': sum(1 for r in all_confidence_results if r.get('is_significant', False)),
            'mean_correlation': float(np.mean([r.get('correlation', 0) for r in all_confidence_results])),
            'mean_ci_width': float(np.mean(ci_widths)) if ci_widths else None,
            'n_bootstrap': all_confidence_results[0].get('n_bootstrap', 1000) if all_confidence_results else None
        }
    
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ã‚¹ã‚¦ã‚§ã‚¤ã‚µãƒãƒªãƒ¼
    if lambda_result.critical_events:
        json_data['event_pathways'] = {
            'n_events': len(lambda_result.critical_events),
            'event_frames': [(int(e[0]), int(e[1])) for e in lambda_result.critical_events if isinstance(e, tuple)]
        }
    
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        json_data['network_stats'] = two_stage_result.global_network_stats
    
    json_path = output_path / 'analysis_data_v4.json'
    with open(json_path, 'w') as f:
        json.dump(json_data, f, indent=2, default=float)
    
    if verbose:
        print(f"\nâœ¨ COMPLETE! (Version 4.0.3 - RESTORED + Bootstrap + Pathways)")
        print(f"   ğŸ“„ Report saved to: {report_path}")
        print(f"   ğŸ“Š Data saved to: {json_path}")
        print(f"   ğŸ“ Report length: {len(report):,} characters")
        print(f"   ğŸ¯ Lambda events: {len(all_events)}")
        if quantum_assessments:
            print(f"   âš›ï¸ Quantum events: {quantum_count}/{total}")
        if all_confidence_results:
            n_sig = sum(1 for r in all_confidence_results if r.get('is_significant', False))
            print(f"   ğŸ“Š Bootstrap: {n_sig}/{len(all_confidence_results)} significant correlations")
        if lambda_result.critical_events:
            print(f"   ğŸ”¬ Event pathways: {len(lambda_result.critical_events)} events analyzed")
        if all_hub_residues:
            hub_counts = Counter(all_hub_residues)  # ã“ã“ã§å®šç¾©
            print(f"   ğŸ’Š Drug targets: {len(hub_counts)}")
        print(f"\n   All information extracted with v4.0.3 enhancements!")
    
    return report


# ========================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ========================================

def _build_propagation_paths(causal_network, initiators, max_paths=3, max_hops=6):
    """
    å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰Propagation Pathwaysã‚’æ§‹ç¯‰
    
    Parameters
    ----------
    causal_network : list
        NetworkLinkã®ãƒªã‚¹ãƒˆ
    initiators : list
        é–‹å§‹æ®‹åŸºã®ãƒªã‚¹ãƒˆ
    max_paths : int
        æœ€å¤§ãƒ‘ã‚¹æ•°
    max_hops : int
        æœ€å¤§ãƒ›ãƒƒãƒ—æ•°
        
    Returns
    -------
    list
        ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆå„ãƒ‘ã‚¹ã¯æ®‹åŸºIDã®ãƒªã‚¹ãƒˆï¼‰
    """
    paths = []
    
    for init_res in initiators[:max_paths]:
        # BFSã§ãƒ‘ã‚¹ã‚’æ¢ç´¢
        path = [init_res]
        current = init_res
        visited = {init_res}
        
        for _ in range(max_hops - 1):
            # currentã‹ã‚‰å‡ºã¦ã„ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            next_links = []
            for link in causal_network:
                # NetworkLinkã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å±æ€§ã‚’å®‰å…¨ã«ãƒã‚§ãƒƒã‚¯
                if hasattr(link, 'from_res') and hasattr(link, 'to_res'):
                    if link.from_res == current and link.to_res not in visited:
                        next_links.append(link)
            
            if not next_links:
                break
            
            # æœ€å¼·ã®ãƒªãƒ³ã‚¯ã‚’é¸æŠ
            if hasattr(next_links[0], 'strength'):
                strongest = max(next_links, key=lambda l: l.strength)
            else:
                strongest = next_links[0]  # strengthãŒãªã„å ´åˆã¯æœ€åˆã®ãƒªãƒ³ã‚¯
            
            path.append(strongest.to_res)
            visited.add(strongest.to_res)
            current = strongest.to_res
        
        if len(path) > 1:
            paths.append(path)
    
    return paths

# ========================================
# V3äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°ï¼ˆç°¡ç•¥ç‰ˆï¼‰
# ========================================

def _generate_v3_report(lambda_result, two_stage_result, quantum_events, 
                       metadata, output_dir, verbose):
    """Version 3.0äº’æ›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆç°¡ç•¥å®Ÿè£…ï¼‰"""
    # å®Ÿéš›ã®V3å®Ÿè£…ã¯çœç•¥ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
    if verbose:
        print("Generating V3 compatible report...")
    return "V3 Report (simplified)"
