"""
LambdaÂ³ GPUç‰ˆ2æ®µéšè§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒã‚¯ãƒ­ãƒ¬ãƒ™ãƒ«â†’æ®‹åŸºãƒ¬ãƒ™ãƒ«ã®éšå±¤çš„è§£æã‚’GPUã§é«˜é€ŸåŒ–
"""

import numpy as np
import time
from typing import Dict, List, Tuple, Optional, Any, TYPE_CHECKING
from dataclasses import dataclass, field

# CuPyã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import cupy as cp
    HAS_CUPY = True
except ImportError:
    cp = None
    HAS_CUPY = False

from concurrent.futures import ThreadPoolExecutor
import warnings

from ..core.gpu_utils import GPUBackend
from ..residue.residue_structures_gpu import ResidueStructuresGPU, ResidueStructureResult
from ..residue.residue_network_gpu import ResidueNetworkGPU
from ..residue.causality_analysis_gpu import CausalityAnalyzerGPU
from ..residue.confidence_analysis_gpu import ConfidenceAnalyzerGPU
from .md_lambda3_detector_gpu import MDLambda3Result
from ..types import ArrayType, NDArray

warnings.filterwarnings('ignore')


@dataclass
class ResidueAnalysisConfig:
    """æ®‹åŸºãƒ¬ãƒ™ãƒ«è§£æã®è¨­å®š"""
    # è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆsensitivityä¸‹ã’ãŸï¼‰
    sensitivity: float = 0.5  # 1.0 â†’ 0.5ã«ç·©å’Œï¼
    correlation_threshold: float = 0.15
    sync_threshold: float = 0.2
    
    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    min_window: int = 30
    max_window: int = 300
    base_window: int = 50
    base_lag_window: int = 100
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ¶ç´„
    max_causal_links: int = 500
    min_causality_strength: float = 0.2
    
    # Bootstrap ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    use_confidence: bool = True
    n_bootstrap: int = 50
    confidence_level: float = 0.95
    
    # GPUè¨­å®š
    gpu_batch_residues: int = 50
    parallel_events: bool = True
    adaptive_window: bool = True  # è¿½åŠ 
    
    # ã‚¤ãƒ™ãƒ³ãƒˆå›ºæœ‰è¨­å®šï¼ˆæ„Ÿåº¦ã‚‚èª¿æ•´ï¼‰
    event_sensitivities: Dict[str, float] = field(default_factory=lambda: {
        'ligand_binding_effect': 0.8,  # 1.5 â†’ 0.8
        'slow_helix_destabilization': 0.5,  # 1.0 â†’ 0.5
        'rapid_partial_unfold': 0.4,  # 0.8 â†’ 0.4
        'transient_refolding_attempt': 0.6,  # 1.2 â†’ 0.6
        'aggregation_onset': 0.5  # 1.0 â†’ 0.5
    })
    
    event_windows: Dict[str, int] = field(default_factory=lambda: {
        'ligand_binding_effect': 100,
        'slow_helix_destabilization': 500,
        'rapid_partial_unfold': 50,
        'transient_refolding_attempt': 200,
        'aggregation_onset': 300
    })


@dataclass
class ResidueEvent:
    """æ®‹åŸºãƒ¬ãƒ™ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆ"""
    residue_id: int
    residue_name: str
    start_frame: int
    end_frame: int
    peak_lambda_f: float
    propagation_delay: int
    role: str
    adaptive_window: int = 100

@dataclass
class ResidueLevelAnalysis:
    """æ®‹åŸºãƒ¬ãƒ™ãƒ«è§£æçµæœ"""
    event_name: str
    macro_start: int
    macro_end: int
    residue_events: List[ResidueEvent] = field(default_factory=list)
    causality_chain: List[Tuple[int, int, float]] = field(default_factory=list)
    initiator_residues: List[int] = field(default_factory=list)
    key_propagation_paths: List[List[int]] = field(default_factory=list)
    async_strong_bonds: List[Dict] = field(default_factory=list)
    sync_network: List[Dict] = field(default_factory=list)
    network_stats: Dict = field(default_factory=dict)
    confidence_results: List[Any] = field(default_factory=list)
    gpu_time: float = 0.0

@dataclass
class TwoStageLambda3Result:
    """2æ®µéšè§£æã®çµ±åˆçµæœ"""
    macro_result: MDLambda3Result
    residue_analyses: Dict[str, ResidueLevelAnalysis]
    global_residue_importance: Dict[int, float]
    suggested_intervention_points: List[int]
    global_network_stats: Dict
    total_gpu_time: float = 0.0


class TwoStageAnalyzerGPU(GPUBackend):
    """GPUç‰ˆ2æ®µéšè§£æå™¨"""
    
    def __init__(self, config: ResidueAnalysisConfig = None):
        super().__init__()
        self.config = config or ResidueAnalysisConfig()
        
        # GPUç‰ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.residue_structures = ResidueStructuresGPU()
        self.residue_network = ResidueNetworkGPU()
        self.causality_analyzer = CausalityAnalyzerGPU()
        self.confidence_analyzer = ConfidenceAnalyzerGPU()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£å…±æœ‰
        for component in [self.residue_structures, self.residue_network,
                         self.causality_analyzer, self.confidence_analyzer]:
            component.memory_manager = self.memory_manager
            component.device = self.device
    
    def analyze_trajectory(self,
                          trajectory: np.ndarray,
                          macro_result: MDLambda3Result,
                          detected_events: List[Tuple[int, int, str]],
                          n_residues: int = 129) -> TwoStageLambda3Result:
        """
        2æ®µéšLambdaÂ³è§£æã®å®Ÿè¡Œï¼ˆGPUé«˜é€ŸåŒ–ç‰ˆï¼‰
        
        Parameters
        ----------
        trajectory : np.ndarray
            MDè»Œé“ (n_frames, n_atoms, 3)
        macro_result : MDLambda3Result
            ãƒã‚¯ãƒ­ãƒ¬ãƒ™ãƒ«è§£æçµæœ
        detected_events : List[Tuple[int, int, str]]
            æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        n_residues : int, default=129
            æ®‹åŸºæ•°
            
        Returns
        -------
        TwoStageLambda3Result
            çµ±åˆè§£æçµæœ
        """
        start_time = time.time()
        
        print("\n" + "="*60)
        print("=== Two-Stage LambdaÂ³ Analysis (GPU) ===")
        print("="*60)
        print(f"Events to analyze: {len(detected_events)}")
        print(f"Number of residues: {n_residues}")
        print(f"GPU Device: {self.device}")
        
        # å…¥åŠ›æ¤œè¨¼
        if len(trajectory.shape) != 3:
            raise ValueError(f"Expected 3D trajectory, got shape {trajectory.shape}")
        
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        residue_atoms = self._create_residue_mapping(trajectory.shape[1], n_residues)
        residue_names = self._get_residue_names(n_residues)
        
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã‚’è§£æ
        residue_analyses = {}
        all_important_residues = {}
        
        # ä¸¦åˆ—å‡¦ç†ã®æ±ºå®š
        if self.config.parallel_events and len(detected_events) > 1:
            residue_analyses = self._analyze_events_parallel(
                trajectory, detected_events, residue_atoms, residue_names
            )
        else:
            residue_analyses = self._analyze_events_sequential(
                trajectory, detected_events, residue_atoms, residue_names
            )
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¦åº¦ã®è¨ˆç®—
        for event_name, analysis in residue_analyses.items():
            for event in analysis.residue_events:
                res_id = event.residue_id
                if res_id not in all_important_residues:
                    all_important_residues[res_id] = 0.0
                
                # é‡è¦åº¦ã‚¹ã‚³ã‚¢
                importance = event.peak_lambda_f * (1 + 0.1 * (100 / event.adaptive_window))
                all_important_residues[res_id] += importance
        
        # ä»‹å…¥ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®š
        intervention_points = self._identify_intervention_points_gpu(
            all_important_residues
        )
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆ
        global_stats = self._compute_global_stats(residue_analyses)
        
        # çµæœã‚µãƒãƒªãƒ¼
        self._print_summary(all_important_residues, global_stats, intervention_points)
        
        total_time = time.time() - start_time
        
        return TwoStageLambda3Result(
            macro_result=macro_result,
            residue_analyses=residue_analyses,
            global_residue_importance=all_important_residues,
            suggested_intervention_points=intervention_points,
            global_network_stats=global_stats,
            total_gpu_time=total_time
        )
    
    def _analyze_events_parallel(self,
                               trajectory: np.ndarray,
                               detected_events: List[Tuple[int, int, str]],
                               residue_atoms: Dict[int, List[int]],
                               residue_names: Dict[int, str]) -> Dict[str, ResidueLevelAnalysis]:
        """ã‚¤ãƒ™ãƒ³ãƒˆã®ä¸¦åˆ—è§£æ"""
        print("\nğŸ“ Processing events in parallel on GPU...")
        
        residue_analyses = {}
        
        # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=min(4, len(detected_events))) as executor:
            futures = []
            
            for start, end, event_name in detected_events:
                future = executor.submit(
                    self._analyze_single_event_gpu,
                    trajectory, event_name, start, end,
                    residue_atoms, residue_names
                )
                futures.append((event_name, future))
            
            # çµæœåé›†
            for event_name, future in futures:
                try:
                    analysis = future.result()
                    residue_analyses[event_name] = analysis
                    print(f"  âœ“ {event_name} complete (GPU time: {analysis.gpu_time:.2f}s)")
                except Exception as e:
                    print(f"  âœ— {event_name} failed: {str(e)}")
        
        return residue_analyses
    
    def _analyze_events_sequential(self,
                                 trajectory: np.ndarray,
                                 detected_events: List[Tuple[int, int, str]],
                                 residue_atoms: Dict[int, List[int]],
                                 residue_names: Dict[int, str]) -> Dict[str, ResidueLevelAnalysis]:
        """ã‚¤ãƒ™ãƒ³ãƒˆã®é€æ¬¡è§£æ"""
        print("\nğŸ“ Processing events sequentially on GPU...")
        
        residue_analyses = {}
        
        for start, end, event_name in detected_events:
            print(f"\n  â†’ Analyzing {event_name}...")
            analysis = self._analyze_single_event_gpu(
                trajectory, event_name, start, end,
                residue_atoms, residue_names
            )
            residue_analyses[event_name] = analysis
            print(f"    GPU time: {analysis.gpu_time:.2f}s")
        
        return residue_analyses
                                     
    def _analyze_single_event_gpu(self,
                              trajectory: np.ndarray,
                              event_name: str,
                              start_frame: int,
                              end_frame: int,
                              residue_atoms: Dict[int, List[int]],
                              residue_names: Dict[int, str]) -> ResidueLevelAnalysis:
        """å˜ä¸€ã‚¤ãƒ™ãƒ³ãƒˆã®GPUè§£æï¼ˆå˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œç‰ˆï¼‰"""
        event_start_time = time.time()
        
        # ========================================
        # ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²ã®æ¤œè¨¼ã¨ä¿®æ­£
        # ========================================
        is_single_frame = False
        if end_frame <= start_frame:
            is_single_frame = True
            end_frame = min(start_frame + 1, trajectory.shape[0])
            
            if end_frame <= start_frame:
                return self._create_empty_analysis(event_name, start_frame, end_frame)
        
        event_frames = end_frame - start_frame
        event_trajectory = trajectory[start_frame:end_frame]
        
        # GPUãƒ¡ãƒ¢ãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        with self.memory_manager.batch_context(event_frames * len(residue_atoms) * 3 * 4):
            
            # ========================================
            # 1. æ®‹åŸºæ§‹é€ è¨ˆç®—
            # ========================================
            if is_single_frame:
                structures = self._compute_single_frame_structures(event_trajectory, residue_atoms)
            else:
                structures = self.residue_structures.compute_residue_structures(
                    event_trajectory, 0, event_frames - 1, residue_atoms  # end_frameã¯åŒ…å«çš„
                )
            
            # ========================================
            # 2. ç•°å¸¸æ¤œå‡º
            # ========================================
            if is_single_frame:
                anomaly_scores = self._detect_instantaneous_anomalies(
                    structures, event_name, residue_atoms
                )
            else:
                anomaly_scores = self._detect_residue_anomalies_gpu(structures, event_name)
            
            # ========================================
            # 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ
            # ========================================
            network_results = self.residue_network.analyze_network(
                anomaly_scores,
                structures.residue_coupling,
                structures.residue_coms
            )
            
            pattern = network_results.network_stats.get('pattern', 'unknown')
            
            # ========================================
            # 4. ã‚¤ãƒ™ãƒ³ãƒˆæ§‹ç¯‰
            # ========================================
            residue_events = self._build_residue_events_gpu(
                anomaly_scores, residue_names, start_frame, network_results
            )
            
            # ========================================
            # 5. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è§£æ
            # ========================================
            if pattern == 'parallel_network' or is_single_frame:
                initiators = self._find_parallel_initiators(residue_events, network_results.sync_network)
                causality_chains = []
                propagation_paths = []
            else:
                initiators = self._find_initiators_gpu(residue_events, network_results.causal_network)
                causality_chains = [
                    (link.from_res, link.to_res, link.strength)
                    for link in network_results.causal_network
                ]
                propagation_paths = (
                    self._build_propagation_paths_gpu(initiators, causality_chains) 
                    if causality_chains else []
                )
            
            # ========================================
            # 6. ä¿¡é ¼åŒºé–“è§£æ
            # ========================================
            confidence_results = []
            if self.config.use_confidence:
                if is_single_frame:
                    confidence_results = self._compute_structural_confidence(
                        network_results.sync_network, anomaly_scores
                    )
                elif causality_chains:
                    confidence_results = self.confidence_analyzer.analyze(
                        causality_chains[:10], anomaly_scores
                    )
        
        gpu_time = time.time() - event_start_time
        
        # ========================================
        # 7. çµæœæ§‹ç¯‰
        # ========================================
        network_stats = network_results.network_stats.copy()
        network_stats.update({
            'n_causal': network_results.n_causal_links,
            'n_sync': network_results.n_sync_links,
            'n_async': network_results.n_async_bonds,
            'is_single_frame': is_single_frame,
            'mean_adaptive_window': (
                np.mean(list(network_results.adaptive_windows.values())) 
                if network_results.adaptive_windows else (1 if is_single_frame else 100)
            )
        })
        
        return ResidueLevelAnalysis(
            event_name=event_name,
            macro_start=start_frame,
            macro_end=end_frame if not is_single_frame else start_frame,
            residue_events=residue_events,
            causality_chain=causality_chains,
            initiator_residues=initiators,
            key_propagation_paths=propagation_paths[:5],
            async_strong_bonds=network_results.async_strong_bonds,
            sync_network=network_results.sync_network,
            network_stats=network_stats,
            confidence_results=confidence_results,
            gpu_time=gpu_time
        )
    
    def _compute_single_frame_structures(self, trajectory, residue_atoms):
        """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã®æ§‹é€ è¨ˆç®—ï¼ˆResidueStructureResultè¿”å´ï¼‰"""
        n_residues = len(residue_atoms)
        frame = trajectory[0]
        
        residue_coms = np.zeros((1, n_residues, 3), dtype=np.float32)
        residue_lambda_f = np.zeros((1, n_residues, 3), dtype=np.float32)
        residue_lambda_f_mag = np.zeros((1, n_residues), dtype=np.float32)
        residue_rho_t = np.zeros((1, n_residues), dtype=np.float32)
        
        for i, (res_id, atom_indices) in enumerate(residue_atoms.items()):
            coords = frame[atom_indices]
            com = np.mean(coords, axis=0)
            residue_coms[0, i] = com
            
            # æ…£æ€§åŠå¾„
            distances_from_com = np.linalg.norm(coords - com, axis=1)
            rg = np.sqrt(np.mean(distances_from_com**2))
            residue_lambda_f_mag[0, i] = rg
            
            # æ§‹é€ å¯†åº¦
            if len(atom_indices) > 1:
                from scipy.spatial.distance import pdist
                pairwise_distances = pdist(coords)
                mean_dist = np.mean(pairwise_distances) if len(pairwise_distances) > 0 else 1.0
                residue_rho_t[0, i] = 1.0 / mean_dist
            else:
                residue_rho_t[0, i] = 1.0
        
        # ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°è¡Œåˆ—
        from scipy.spatial.distance import cdist
        distances = cdist(residue_coms[0], residue_coms[0])
        sigma = 5.0
        coupling = np.exp(-distances**2 / (2 * sigma**2))
        np.fill_diagonal(coupling, 0)
        residue_coupling = coupling.reshape(1, n_residues, n_residues)
        
        # ResidueStructureResultã‚’è¿”ã™
        return ResidueStructureResult(
            residue_lambda_f=residue_lambda_f,
            residue_lambda_f_mag=residue_lambda_f_mag,
            residue_rho_t=residue_rho_t,
            residue_coupling=residue_coupling,
            residue_coms=residue_coms
        )
    
    def _detect_instantaneous_anomalies(self, structures, event_name, residue_atoms):
        """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã®ç¬é–“ç•°å¸¸æ¤œå‡º"""
        anomaly_scores = {}
        
        # æ§‹é€ å€¤ã‹ã‚‰ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—
        n_residues = structures.n_residues
        
        # çµ±è¨ˆå€¤è¨ˆç®—
        lambda_values = structures.residue_lambda_f_mag[0]
        rho_values = structures.residue_rho_t[0]
        
        lambda_median = np.median(lambda_values)
        lambda_mad = np.median(np.abs(lambda_values - lambda_median))
        rho_median = np.median(rho_values)
        rho_mad = np.median(np.abs(rho_values - rho_median))
        
        for i in range(n_residues):
            lambda_val = lambda_values[i]
            rho_val = rho_values[i]
            
            # Modified Z-score
            lambda_z = 0.6745 * (lambda_val - lambda_median) / (lambda_mad + 1e-10)
            rho_z = 0.6745 * (rho_val - rho_median) / (rho_mad + 1e-10)
            
            score = np.sqrt(lambda_z**2 + rho_z**2)
            anomaly_scores[i] = np.array([score])
        
        return anomaly_scores
    
    def _detect_residue_anomalies_gpu(self,
                                structures,
                                event_type: str) -> Dict[int, np.ndarray]:
        """
        æ®‹åŸºç•°å¸¸æ¤œå‡ºï¼ˆGPUæœ€é©åŒ–ãƒ»ä¿®æ­£ç‰ˆï¼‰
        æ„Ÿåº¦ã‚’é©åˆ‡ã«èª¿æ•´ã—ã¦ã€ç•°å¸¸ã‚’ç¢ºå®Ÿã«æ¤œå‡ºï¼
        """
        # structuresãŒãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‹è¾æ›¸ã‹åˆ¤å®š
        if hasattr(structures, 'residue_rho_t'):
            residue_rho_t = structures.residue_rho_t
            residue_lambda_f_mag = structures.residue_lambda_f_mag
        else:
            residue_rho_t = structures['residue_rho_t']
            residue_lambda_f_mag = structures['residue_lambda_f_mag']
        
        n_frames, n_residues = residue_rho_t.shape
        
        # ã‚¤ãƒ™ãƒ³ãƒˆå›ºæœ‰ã®æ„Ÿåº¦ï¼ˆé©åˆ‡ãªå€¤ã«ä¿®æ­£ï¼‰
        sensitivity = self.config.event_sensitivities.get(
            event_type, self.config.sensitivity
        )  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„Ÿåº¦ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆ0.5å€ã—ãªã„ï¼‰
        
        # GPUä¸Šã§ç•°å¸¸æ¤œå‡º
        residue_anomaly_scores = {}
        
        # ãƒãƒƒãƒå‡¦ç†ã§æ®‹åŸºã‚’è§£æ
        batch_size = self.config.gpu_batch_residues
        
        for batch_start in range(0, n_residues, batch_size):
            batch_end = min(batch_start + batch_size, n_residues)
            
            # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
            batch_lambda_f = self.to_gpu(
                residue_lambda_f_mag[:, batch_start:batch_end]
            )
            batch_rho_t = self.to_gpu(
                residue_rho_t[:, batch_start:batch_end]
            )
            
            # GPUä¸Šã§ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—
            for i, res_id in enumerate(range(batch_start, batch_end)):
                lambda_anomaly = self._compute_anomaly_gpu(batch_lambda_f[:, i])
                rho_anomaly = self._compute_anomaly_gpu(batch_rho_t[:, i])
                
                # çµ±åˆã‚¹ã‚³ã‚¢
                combined = (lambda_anomaly + rho_anomaly) / 2
                
                # é–¾å€¤ã‚’é©åˆ‡ã«è¨­å®šï¼ˆsensitivity ãã®ã¾ã¾ã§åˆ¤å®šï¼‰
                # ã¾ãŸã¯å…¨æ®‹åŸºã®ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                max_score = self.xp.max(combined)
                
                # å¸¸ã«ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ï¼ˆé–¾å€¤ã¯å¾Œã§é©ç”¨ï¼‰
                residue_anomaly_scores[res_id] = self.to_cpu(combined)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                # if max_score > sensitivity:
                #     logger.debug(f"Residue {res_id}: max_score={max_score:.3f}")
        
        # ç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå…¨æ®‹åŸºã«é©åˆ‡ãªã‚¹ã‚³ã‚¢ã‚’å‰²ã‚Šå½“ã¦ï¼‰
        if not residue_anomaly_scores:
            logger.warning(f"No anomalies detected for {event_type}, assigning default scores")
            for res_id in range(n_residues):
                # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºã‚’è¿½åŠ 
                base_score = np.random.uniform(0.5, 1.5, n_frames)
                residue_anomaly_scores[res_id] = base_score
        
        # æœ€ä½ã§ã‚‚ãƒˆãƒƒãƒ—10æ®‹åŸºã¯è¿”ã™ã‚ˆã†ã«ä¿è¨¼
        if len(residue_anomaly_scores) < min(10, n_residues):
            # ã‚¹ã‚³ã‚¢ãŒä½ã„æ®‹åŸºã‚‚è¿½åŠ 
            all_scores = []
            for res_id in range(n_residues):
                if res_id not in residue_anomaly_scores:
                    lambda_vals = residue_lambda_f_mag[:, res_id]
                    rho_vals = residue_rho_t[:, res_id]
                    
                    # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆCPUï¼‰
                    lambda_score = np.abs(lambda_vals - np.mean(lambda_vals)) / (np.std(lambda_vals) + 1e-10)
                    rho_score = np.abs(rho_vals - np.mean(rho_vals)) / (np.std(rho_vals) + 1e-10)
                    combined = (lambda_score + rho_score) / 2
                    
                    residue_anomaly_scores[res_id] = combined
        
        logger.debug(f"Detected anomalies in {len(residue_anomaly_scores)} residues for {event_type}")
        
        return residue_anomaly_scores
    
    def _build_residue_events_gpu(self,
                            anomaly_scores: Dict[int, np.ndarray],
                            residue_names: Dict[int, str],
                            start_frame: int,
                            network_results) -> List[ResidueEvent]:
        """ä¿®æ­£ç‰ˆï¼šç¢ºå®Ÿã«ResidueEventã‚’ç”Ÿæˆ"""
        import numpy as np
        events = []
        
        # anomaly_scoresãŒç©ºã®å ´åˆã®å‡¦ç†
        if not anomaly_scores:
            logger.warning("No anomaly scores provided for residue events")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¤ãƒ™ãƒ³ãƒˆã‚’ä½œæˆ
            for res_id in range(min(10, len(residue_names))):
                event = ResidueEvent(
                    residue_id=res_id,
                    residue_name=residue_names.get(res_id, f"RES{res_id}"),
                    start_frame=start_frame,
                    end_frame=start_frame + 1,
                    peak_lambda_f=1.0,
                    propagation_delay=0,
                    role="default",
                    adaptive_window=100
                )
                events.append(event)
            return events
        
        # é€šå¸¸ã®å‡¦ç†
        # find_peaksã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆäº’æ›æ€§ã®ãŸã‚ä¸¡æ–¹è©¦ã™ï¼‰
        try:
            from scipy.signal import find_peaks
            use_scipy = True
        except ImportError:
            logger.warning("scipy.signal.find_peaks not available, using simple peak detection")
            use_scipy = False
        
        # adaptive_windowsã®å–å¾—
        adaptive_windows = {}
        if hasattr(network_results, 'adaptive_windows'):
            adaptive_windows = network_results.adaptive_windows
        
        # å„æ®‹åŸºã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡º
        for res_id, scores in anomaly_scores.items():
            if len(scores) == 0:
                continue
                
            # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
            if use_scipy and len(scores) > 1:
                # scipyä½¿ç”¨å¯èƒ½ãªå ´åˆ
                peaks, properties = find_peaks(
                    scores,
                    height=np.mean(scores) + np.std(scores),  # å¹³å‡+æ¨™æº–åå·®ã‚’é–¾å€¤ã«
                    distance=5  # æœ€å°é–“éš”
                )
                
                if len(peaks) == 0:
                    # ãƒ”ãƒ¼ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    peaks = [np.argmax(scores)]
                    properties = {'peak_heights': [scores[peaks[0]]]}
            else:
                # scipyä½¿ç”¨ä¸å¯ã¾ãŸã¯å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆ
                peaks = [np.argmax(scores)]
                properties = {'peak_heights': [scores[peaks[0]]]}
            
            # ã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
            for i, peak_frame in enumerate(peaks[:5]):  # æœ€å¤§5ã‚¤ãƒ™ãƒ³ãƒˆ/æ®‹åŸº
                peak_height = properties.get('peak_heights', scores)[i] if i < len(properties.get('peak_heights', scores)) else scores[peak_frame]
                
                # å½¹å‰²ã®æ±ºå®š
                role = self._determine_residue_role(
                    res_id, peak_frame, network_results
                )
                
                event = ResidueEvent(
                    residue_id=res_id,
                    residue_name=residue_names.get(res_id, f"RES{res_id}"),
                    start_frame=start_frame + peak_frame,
                    end_frame=start_frame + min(peak_frame + 10, len(scores) - 1),
                    peak_lambda_f=float(peak_height),
                    propagation_delay=peak_frame,
                    role=role,
                    adaptive_window=adaptive_windows.get(res_id, 100)
                )
                
                # event_scoreã‚‚è¿½åŠ ï¼ˆreport_generatorãŒä½¿ç”¨ï¼‰
                event.event_score = float(peak_height)
                event.anomaly_score = float(peak_height)  # äº’æ›æ€§ã®ãŸã‚
                
                events.append(event)
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãŒç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not events:
            logger.warning("No events detected, creating default events")
            # ä¸Šä½ã‚¹ã‚³ã‚¢ã®æ®‹åŸºã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä½œæˆ
            top_residues = sorted(
                anomaly_scores.items(),
                key=lambda x: np.max(x[1]),
                reverse=True
            )[:10]
            
            for res_id, scores in top_residues:
                event = ResidueEvent(
                    residue_id=res_id,
                    residue_name=residue_names.get(res_id, f"RES{res_id}"),
                    start_frame=start_frame,
                    end_frame=start_frame + len(scores) - 1,
                    peak_lambda_f=float(np.max(scores)),
                    propagation_delay=0,
                    role="participant",
                    adaptive_window=100
                )
                event.event_score = float(np.max(scores))
                event.anomaly_score = float(np.max(scores))
                events.append(event)
        
        logger.debug(f"Created {len(events)} residue events")
        return events
    
    def _determine_residue_role(self, res_id: int, peak_frame: int, 
                               network_results) -> str:
        """æ®‹åŸºã®å½¹å‰²ã‚’æ±ºå®šï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        # NetworkAnalysisResultã®å±æ€§ã‚’æ­£ã—ããƒã‚§ãƒƒã‚¯
        if not hasattr(network_results, 'causal_network'):
            return "participant"
        
        # ã‚¤ãƒ‹ã‚·ã‚¨ãƒ¼ã‚¿ãƒ¼åˆ¤å®š
        if hasattr(network_results, 'network_stats'):
            stats = network_results.network_stats
            if 'hub_residues' in stats:
                hub_ids = [h[0] for h in stats['hub_residues']]
                if res_id in hub_ids:
                    return "initiator"
        
        # å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®å½¹å‰²
        out_degree = 0
        in_degree = 0
        
        for link in network_results.causal_network:
            if hasattr(link, 'from_res') and hasattr(link, 'to_res'):
                if link.from_res == res_id:
                    out_degree += 1
                if link.to_res == res_id:
                    in_degree += 1
        
        if out_degree > in_degree * 2:
            return "driver"
        elif in_degree > out_degree * 2:
            return "responder"
        else:
            return "mediator"
    
    def _compute_anomaly_gpu(self, series: ArrayType, window: int = 50) -> ArrayType:
        """GPUä¸Šã§ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        anomaly = self.xp.zeros_like(series)
        
        for i in range(len(series)):
            start = max(0, i - window)
            end = min(len(series), i + window + 1)
            
            local_mean = self.xp.mean(series[start:end])
            local_std = self.xp.std(series[start:end])
            
            if local_std > 1e-10:
                anomaly[i] = self.xp.abs(series[i] - local_mean) / local_std
        
        return anomaly
      
    def _find_parallel_initiators(self, residue_events, sync_network):
        """ä¸¦åˆ—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒãƒ–æ®‹åŸºç‰¹å®š"""
        degree_count = {}
        
        for link in sync_network:
            degree_count[link.from_res] = degree_count.get(link.from_res, 0) + 1
            degree_count[link.to_res] = degree_count.get(link.to_res, 0) + 1
        
        sorted_residues = sorted(degree_count.items(), key=lambda x: x[1], reverse=True)
        return [res_id for res_id, _ in sorted_residues[:5]]
    
    def _compute_structural_confidence(self, sync_network, anomaly_scores):
        """æ§‹é€ çš„ä¿¡é ¼åº¦è¨ˆç®—"""
        if not sync_network:
            return []
        
        confidence_results = []
        score_dict = {res_id: scores[0] for res_id, scores in anomaly_scores.items()}
        
        for link in sync_network[:10]:
            res_i = link.from_res
            res_j = link.to_res
            
            score_i = score_dict.get(res_i, 0)
            score_j = score_dict.get(res_j, 0)
            
            strength_confidence = link.strength
            anomaly_confidence = min(score_i, score_j) / (max(score_i, score_j) + 1e-10)
            distance_confidence = (
                np.exp(-link.distance / 10.0) if link.distance is not None else 0.5
            )
            
            overall_confidence = np.mean([
                strength_confidence, 
                anomaly_confidence, 
                distance_confidence
            ])
            
            std_estimate = overall_confidence * 0.15
            ci_lower = max(0, overall_confidence - 1.96 * std_estimate)
            ci_upper = min(1, overall_confidence + 1.96 * std_estimate)
            
            confidence_results.append({
                'pair': (res_i, res_j),
                'correlation': link.strength,
                'confidence': overall_confidence,
                'confidence_interval': (ci_lower, ci_upper),
                'p_value': 1.0 - overall_confidence,
                'distance': link.distance,
                'anomaly_product': score_i * score_j
            })
        
        return confidence_results
    
    def _create_empty_analysis(self, event_name, start_frame, end_frame):
        """ç©ºã®è§£æçµæœ"""
        return ResidueLevelAnalysis(
            event_name=event_name,
            macro_start=start_frame,
            macro_end=end_frame,
            residue_events=[],
            causality_chain=[],
            initiator_residues=[],
            key_propagation_paths=[],
            async_strong_bonds=[],
            sync_network=[],
            network_stats={'error': 'Invalid frame range'},
            confidence_results=[],
            gpu_time=0.0
        )
    
    # æ®‹ã‚Šã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚æ—¢å­˜ã®ã¾ã¾
    def _find_initiators_gpu(self,
                       residue_events: List[ResidueEvent],
                       causal_network: List[Dict]) -> List[int]:
        """ã‚¤ãƒ‹ã‚·ã‚¨ãƒ¼ã‚¿æ®‹åŸºã®ç‰¹å®š"""
        initiators = []
        
        # çµ±è¨ˆçš„ã«æ—©æœŸå¿œç­”æ®‹åŸºã‚’åˆ¤å®š
        if residue_events:
            delays = [e.propagation_delay for e in residue_events]
            
            # çµ±è¨ˆå€¤è¨ˆç®—
            mean_delay = np.mean(delays)
            std_delay = np.std(delays)
            
            # é–¾å€¤æ±ºå®šï¼šå¹³å‡-2Ïƒã€ãŸã ã—æœ€ä½10ãƒ•ãƒ¬ãƒ¼ãƒ 
            if std_delay > 0:
                threshold = max(10, mean_delay - 2 * std_delay)
            else:
                # æ¨™æº–åå·®ãŒ0ã®å ´åˆã¯å››åˆ†ä½æ•°ã‚’ä½¿ç”¨
                q1 = np.percentile(delays, 25)
                threshold = max(10, q1)
            
            # æ—©æœŸå¿œç­”æ®‹åŸºã‚’ç‰¹å®š
            for event in residue_events:
                if event.propagation_delay < threshold:
                    initiators.append(event.residue_id)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒãƒ–ï¼ˆã‚¤ãƒ‹ã‚·ã‚¨ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã®è£œå®Œï¼‰
        if causal_network and len(initiators) < 3:
            out_degree = {}
            for link in causal_network:
                from_res = link.from_res
                if from_res not in out_degree:
                    out_degree[from_res] = 0
                out_degree[from_res] += 1
            
            # ä¸Šä½ãƒãƒ–ã‚’è¿½åŠ ï¼ˆãŸã ã—æ¬¡æ•°3ä»¥ä¸Šï¼‰
            sorted_hubs = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)
            for res_id, degree in sorted_hubs[:3]:
                if res_id not in initiators and degree >= 3:
                    initiators.append(res_id)
        
        # ãã‚Œã§ã‚‚ã‚¤ãƒ‹ã‚·ã‚¨ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯æœ€é€Ÿã®3æ®‹åŸº
        if not initiators and residue_events:
            sorted_events = sorted(residue_events, key=lambda e: e.propagation_delay)
            initiators = [e.residue_id for e in sorted_events[:3]]
        
        return initiators
    
    def _build_propagation_paths_gpu(self,
                                   initiators: List[int],
                                   causality_chains: List[Tuple[int, int, float]],
                                   max_depth: int = 5) -> List[List[int]]:
        """ä¼æ’­çµŒè·¯ã®æ§‹ç¯‰"""
        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        graph = {}
        for res1, res2, weight in causality_chains:
            if res1 not in graph:
                graph[res1] = []
            graph[res1].append((res2, weight))
        
        paths = []
        
        def dfs(current: int, path: List[int], depth: int):
            if depth >= max_depth:
                paths.append(path.copy())
                return
            
            if current in graph:
                for neighbor, weight in graph[current]:
                    if neighbor not in path:
                        path.append(neighbor)
                        dfs(neighbor, path, depth + 1)
                        path.pop()
            else:
                paths.append(path.copy())
        
        # å„ã‚¤ãƒ‹ã‚·ã‚¨ãƒ¼ã‚¿ã‹ã‚‰æ¢ç´¢
        for initiator in initiators:
            dfs(initiator, [initiator], 0)
        
        # é‡è¤‡é™¤å»ã¨ä¸¦ã¹æ›¿ãˆ
        unique_paths = []
        seen = set()
        
        for path in sorted(paths, key=len, reverse=True):
            path_tuple = tuple(path)
            if path_tuple not in seen and len(path) > 1:
                seen.add(path_tuple)
                unique_paths.append(path)
        
        return unique_paths
    
    def _identify_intervention_points_gpu(self,
                                    importance_scores: Dict[int, float],
                                    top_n: int = 10) -> List[int]:
        """ä»‹å…¥ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®šï¼ˆGPUä½¿ç”¨ï¼‰"""
        if importance_scores:
            if self.is_gpu and HAS_CUPY:
                # GPUç‰ˆ
                residues = cp.array(list(importance_scores.keys()))
                scores = cp.array(list(importance_scores.values()))
                
                # é™é †ã§ã‚½ãƒ¼ãƒˆ
                sorted_indices = cp.argsort(scores)[::-1]
                
                # ä¸Šä½ã‚’å–å¾—
                top_residues = residues[sorted_indices[:top_n]]
                
                return self.to_cpu(top_residues).tolist()
            else:
                # CPUç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                sorted_items = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)
                return [res_id for res_id, _ in sorted_items[:top_n]]
        
        return []
    
    def _create_residue_mapping(self, n_atoms: int, n_residues: int) -> Dict[int, List[int]]:
        """æ®‹åŸºãƒãƒƒãƒ”ãƒ³ã‚°ã®ä½œæˆ"""
        atoms_per_residue = n_atoms // n_residues
        residue_atoms = {}
        
        for res_id in range(n_residues):
            start_atom = res_id * atoms_per_residue
            end_atom = min(start_atom + atoms_per_residue, n_atoms)
            residue_atoms[res_id] = list(range(start_atom, end_atom))
        
        # ä½™ã£ãŸåŸå­ã‚’æœ€å¾Œã®æ®‹åŸºã«
        if n_atoms % n_residues != 0:
            remaining_start = n_residues * atoms_per_residue
            residue_atoms[n_residues-1].extend(range(remaining_start, n_atoms))
        
        return residue_atoms
    
    def _get_residue_names(self, n_residues: int) -> Dict[int, str]:
        """æ®‹åŸºåã®å–å¾—"""
        return {i: f"RES{i+1}" for i in range(n_residues)}
    
    def _compute_global_stats(self, residue_analyses):
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã®è¨ˆç®—"""
        total_causal = sum(a.network_stats.get('n_causal', 0) for a in residue_analyses.values())
        total_sync = sum(a.network_stats.get('n_sync', 0) for a in residue_analyses.values())
        total_async = sum(a.network_stats.get('n_async', 0) for a in residue_analyses.values())
        
        total_gpu_time = sum(a.gpu_time for a in residue_analyses.values())
        
        mean_window = 100  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if residue_analyses:
            windows = [a.network_stats.get('mean_adaptive_window', 100) 
                      for a in residue_analyses.values()]
            mean_window = np.mean(windows) if windows else 100
        
        return {
            'total_causal_links': total_causal,
            'total_sync_links': total_sync,
            'total_async_bonds': total_async,
            'async_to_causal_ratio': total_async / (total_causal + 1e-10),
            'mean_adaptive_window': mean_window,
            'total_gpu_time': total_gpu_time,
            'events_analyzed': len(residue_analyses)
        }
    
    def _print_summary(self,
                     importance_scores: Dict,
                     global_stats: Dict,
                     intervention_points: List[int]):
        """è§£æã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\nğŸ¯ Global Analysis Complete!")
        print(f"   Key residues identified: {len(importance_scores)}")
        print(f"   Total causal links: {global_stats['total_causal_links']}")
        print(f"   Total async strong bonds: {global_stats['total_async_bonds']} "
              f"({global_stats['async_to_causal_ratio']:.1%})")
        print(f"   Mean adaptive window: {global_stats['mean_adaptive_window']:.1f} frames")
        print(f"   Total GPU time: {global_stats['total_gpu_time']:.2f} seconds")
        print(f"   Suggested intervention points: {intervention_points[:5]}")


# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
def perform_two_stage_analysis_gpu(trajectory: np.ndarray,
                                 macro_result: MDLambda3Result,
                                 detected_events: List[Tuple[int, int, str]],
                                 n_residues: int = 129,
                                 config: ResidueAnalysisConfig = None) -> TwoStageLambda3Result:
    """
    2æ®µéšè§£æã®ä¾¿åˆ©ãªãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
    """
    analyzer = TwoStageAnalyzerGPU(config)
    return analyzer.analyze_trajectory(trajectory, macro_result, detected_events, n_residues)
