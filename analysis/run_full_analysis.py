#!/usr/bin/env python3
"""
LambdaÂ³ GPU Quantum Validation Pipeline - Version 4.0
======================================================

LambdaÂ³çµ±åˆå‹é‡å­èµ·æºåˆ¤å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
æ§‹é€ å¤‰åŒ–ã®é‡å­æ€§ã‚’æ­£ã—ãè©•ä¾¡ã™ã‚‹å®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ

Version: 4.0.0 - Complete Refactoring
Authors: LambdaÂ³ Team, ç’°ã¡ã‚ƒã‚“ & ã”ä¸»äººã•ã¾ ğŸ’•
Date: 2024
"""

import numpy as np
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# LambdaÂ³ GPU imports
try:
    from lambda3_gpu.analysis.md_lambda3_detector_gpu import (
        MDLambda3DetectorGPU, 
        MDLambda3Result,
        MDConfig
    )
    from lambda3_gpu.analysis.two_stage_analyzer_gpu import (
        TwoStageAnalyzerGPU,
        TwoStageLambda3Result,
        ResidueAnalysisConfig
    )
    # Version 4.0 imports
    from lambda3_gpu.quantum import (
        QuantumValidatorV4,
        StructuralEventPattern,
        QuantumSignature,
        QuantumAssessment
    )
    from lambda3_gpu.visualization import Lambda3VisualizerGPU
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure lambda3_gpu is properly installed")
    raise

# Loggerè¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('quantum_validation_v4')

# ============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆVersion 4.0ï¼‰
# ============================================

def run_quantum_validation_pipeline(
    trajectory_path: str,
    metadata_path: str,
    protein_indices_path: str,
    topology_path: Optional[str] = None,
    enable_two_stage: bool = True,
    enable_visualization: bool = True,
    output_dir: str = './quantum_results_v4',
    verbose: bool = False
) -> Dict:
    """
    å®Œå…¨ãªé‡å­æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆVersion 4.0ï¼‰
    
    Parameters
    ----------
    trajectory_path : str
        ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (.npy)
    metadata_path : str
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (.json)
    protein_indices_path : str
        ã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸå­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (.npy)
    topology_path : str, optional
        ãƒˆãƒãƒ­ã‚¸ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆçµåˆæƒ…å ±ãªã©ï¼‰
    enable_two_stage : bool
        2æ®µéšè§£æï¼ˆæ®‹åŸºãƒ¬ãƒ™ãƒ«ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã‹
    enable_visualization : bool
        å¯è¦–åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ã‹
    output_dir : str
        å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    verbose : bool
        è©³ç´°å‡ºåŠ›
        
    Returns
    -------
    Dict
        è§£æçµæœã®è¾æ›¸
    """
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info("ğŸš€ LAMBDAÂ³ GPU QUANTUM VALIDATION PIPELINE v4.0")
    logger.info("   LambdaÂ³ Integrated Edition")
    logger.info("="*70)
    
    # ========================================
    # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    # ========================================
    logger.info("\nğŸ“ Loading data...")
    
    try:
        # ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªèª­ã¿è¾¼ã¿
        trajectory = np.load(trajectory_path)
        logger.info(f"   Trajectory shape: {trajectory.shape}")
        n_frames, n_atoms, n_dims = trajectory.shape
        
        if n_dims != 3:
            raise ValueError(f"Trajectory must be 3D, got {n_dims}D")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        logger.info("   Metadata loaded successfully")
        
        # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
        if not Path(protein_indices_path).exists():
            raise FileNotFoundError(f"Protein indices file not found: {protein_indices_path}")
            
        protein_indices = np.load(protein_indices_path)
        logger.info(f"   Protein indices loaded: {len(protein_indices)} atoms")
        
        # ãƒˆãƒãƒ­ã‚¸ãƒ¼èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        topology = None
        if topology_path and Path(topology_path).exists():
            # ãƒˆãƒãƒ­ã‚¸ãƒ¼å½¢å¼ã«å¿œã˜ã¦èª­ã¿è¾¼ã¿
            # ã“ã“ã§ã¯ç°¡ç•¥åŒ–
            logger.info(f"   Topology loaded from {topology_path}")
        
        # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæƒ…å ±ã®è¡¨ç¤º
        if 'protein' in metadata:
            protein_info = metadata['protein']
            logger.info(f"   Protein: {protein_info.get('n_residues', 'N/A')} residues")
            logger.info(f"   Sequence: {protein_info.get('sequence', 'N/A')[:20]}...")
        
        logger.info(f"   âœ… Data validation passed")
            
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise
    
    # ========================================
    # Step 2: LambdaÂ³ GPUè§£æ
    # ========================================
    logger.info("\nğŸ”¬ Running LambdaÂ³ GPU Analysis...")
    
    try:
        # MDConfigè¨­å®š
        config = MDConfig()
        config.use_extended_detection = True
        config.use_phase_space = True
        config.use_periodic = True
        config.use_gradual = True
        config.use_drift = True
        config.adaptive_window = True
        config.window_scale = 0.005
        config.gpu_batch_size = 5000
        config.verbose = verbose
        config.sensitivity = 2.0
        config.use_hierarchical = True
        
        logger.info("   Config optimized for structure detection")
        
        # LambdaÂ³æ¤œå‡ºå™¨åˆæœŸåŒ–
        detector = MDLambda3DetectorGPU(config)
        logger.info("   Detector initialized on GPU")
        
        # è§£æå®Ÿè¡Œ
        lambda_result = detector.analyze(trajectory, protein_indices)
        
        logger.info(f"   âœ… LambdaÂ³ analysis complete")
        logger.info(f"   Critical events detected: {len(lambda_result.critical_events)}")
        
        # çµæœã®ä¿å­˜
        result_summary = {
            'n_frames': lambda_result.n_frames,
            'n_atoms': lambda_result.n_atoms,
            'n_protein_atoms': len(protein_indices),
            'n_critical_events': len(lambda_result.critical_events),
            'computation_time': lambda_result.computation_time,
            'analysis_version': '4.0.0'
        }
        
        with open(output_path / 'lambda_result_summary.json', 'w') as f:
            json.dump(result_summary, f, indent=2)
            
    except Exception as e:
        logger.error(f"LambdaÂ³ analysis failed: {e}")
        raise
  
    # ========================================
    # Step 3: Two-Stageè©³ç´°è§£æ
    # ========================================
    two_stage_result = None
    network_results = []
    
    if enable_two_stage and len(lambda_result.critical_events) > 0:
        logger.info("\\nğŸ”¬ Running Two-Stage Residue-Level Analysis...")
        
        try:
            # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ®‹åŸºæ•°ã®å–å¾—
            n_protein_residues = metadata.get('protein', {}).get('n_residues', 10)
            logger.info(f"   Protein residues: {n_protein_residues}")
            
            # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªéƒ¨åˆ†ã®ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒª
            protein_trajectory = trajectory[:, protein_indices, :]
            
            # TOP50ã‚¤ãƒ™ãƒ³ãƒˆé¸æŠ
            MAX_EVENTS = 50
            MIN_WINDOW_SIZE = 10
            
            # ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚¹ã‚³ã‚¢ä»˜ãã‚½ãƒ¼ãƒˆ
            sorted_events = []
            for event in lambda_result.critical_events:
                if isinstance(event, dict):
                    score = event.get('anomaly_score', 0)
                    start = event.get('start', event.get('frame', 0))
                    end = event.get('end', start)
                elif isinstance(event, (tuple, list)) and len(event) >= 2:
                    start = int(event[0])
                    end = int(event[1])
                    
                    # âš¡ ä¿®æ­£: anomaly_scoresã‹ã‚‰å®Ÿéš›ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼ˆæœ€å¤§å€¤ï¼‰
                    if len(event) > 2:
                        score = event[2]  # æ—¢ã«ã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆ
                    else:
                        # anomaly_scoresã‹ã‚‰è©²å½“ç¯„å›²ã®æœ€å¤§å€¤ã‚’å–å¾—
                        if 'combined' in lambda_result.anomaly_scores:
                            # ç¯„å›²å†…ã®æœ€å¤§å€¤ï¼ˆé‡å­çš„ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ‰ãˆã‚‹ï¼‰
                            score = float(np.max(lambda_result.anomaly_scores['combined'][start:end+1]))
                        elif 'final_combined' in lambda_result.anomaly_scores:
                            score = float(np.max(lambda_result.anomaly_scores['final_combined'][start:end+1]))
                        elif 'global' in lambda_result.anomaly_scores:
                            score = float(np.max(lambda_result.anomaly_scores['global'][start:end+1]))
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šä½•ã‹ã—ã‚‰ã®ã‚¹ã‚³ã‚¢ã‚’æ¢ã™
                            for key in ['local', 'extended']:
                                if key in lambda_result.anomaly_scores:
                                    score = float(np.max(lambda_result.anomaly_scores[key][start:end+1]))
                                    break
                            else:
                                score = 0.0
                                logger.warning(f"   âš ï¸ No anomaly scores found for event at frames {start}-{end}")
                else:
                    continue
                sorted_events.append((start, end, score))
            
            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
            sorted_events.sort(key=lambda x: x[2], reverse=True)
            selected_events = sorted_events[:min(MAX_EVENTS, len(sorted_events))]
            logger.info(f"   Selected TOP {len(selected_events)} events")
            
            # ãƒ‡ãƒãƒƒã‚°ï¼šä¸Šä½ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
            if verbose:
                logger.debug("   Top event scores:")
                for i, (start, end, score) in enumerate(selected_events[:10]):
                    logger.debug(f"     Event {i}: frames {start}-{end}, score={score:.3f}")
            
            # =========== ã“ã“ã‹ã‚‰è¿½åŠ ï¼ï¼ ===========
            # TwoStageAnalyzerã®è¨­å®š
            residue_config = ResidueAnalysisConfig()
            residue_config.n_residues = n_protein_residues
            residue_config.min_window_size = MIN_WINDOW_SIZE
            residue_config.use_gpu = True
            residue_config.verbose = verbose
            
            # TwoStageAnalyzeråˆæœŸåŒ–
            logger.info("   Initializing Two-Stage Analyzer...")
            two_stage_analyzer = TwoStageAnalyzerGPU(residue_config)
            
            # è§£æå®Ÿè¡Œï¼
            logger.info(f"   Analyzing {len(selected_events)} events...")
            two_stage_result = two_stage_analyzer.analyze_trajectory(  # â† analyze_trajectoryï¼
                trajectory=protein_trajectory,  # â† trajectoryã¨ã„ã†å¼•æ•°å
                macro_result=lambda_result,     # â† macro_resultã¨ã„ã†å¼•æ•°å  
                detected_events=selected_events, # â† detected_eventsã¨ã„ã†å¼•æ•°å
                n_residues=n_protein_residues
            )
            
            logger.info(f"   âœ… Two-stage analysis complete")
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµæœã®æŠ½å‡º
            if hasattr(two_stage_result, 'residue_analyses'):
                network_results = []
                for analysis in two_stage_result.residue_analyses.values():
                    if hasattr(analysis, 'network_result'):
                        network_results.append(analysis.network_result)
                logger.info(f"   Extracted {len(network_results)} network results")
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã®è¡¨ç¤º
            if hasattr(two_stage_result, 'global_network_stats'):
                stats = two_stage_result.global_network_stats
                logger.info(f"   Total causal links: {stats.get('total_causal_links', 0)}")
                logger.info(f"   Total async bonds: {stats.get('total_async_bonds', 0)}")
            
        except Exception as e:
            logger.error(f"Two-stage analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            two_stage_result = None
            network_results = []   
          
    # ========================================
    # Step 4: é‡å­æ¤œè¨¼ï¼ˆVersion 4.0ï¼‰
    # ========================================
    logger.info("\nâš›ï¸ Running Quantum Validation v4.0...")
    
    quantum_assessments = []
    
    try:
        # ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        temperature = metadata.get('temperature', 300.0)
        dt_ps = metadata.get('time_step_ps', 100.0)
        
        # é‡å­æ¤œè¨¼å™¨åˆæœŸåŒ–ï¼ˆVersion 4.0ï¼‰
        quantum_validator = QuantumValidatorV4(
            trajectory=trajectory[:, protein_indices, :],
            topology=topology,
            dt_ps=dt_ps,
            temperature_K=temperature
        )
        
        logger.info(f"   Quantum validator v4.0 initialized")
        logger.info(f"   Temperature: {temperature:.1f} K")
        logger.info(f"   Time step: {dt_ps:.1f} ps")
        
        # LambdaÂ³ã‚¤ãƒ™ãƒ³ãƒˆã‚’å¤‰æ›
        events = []
        for e in lambda_result.critical_events[:100]:  # æœ€å¤§100ã‚¤ãƒ™ãƒ³ãƒˆ
            if isinstance(e, (tuple, list)) and len(e) >= 2:
                events.append({
                    'frame_start': int(e[0]),
                    'frame_end': int(e[1]),
                    'type': 'critical'
                })
            elif isinstance(e, dict):
                events.append({
                    'frame_start': e.get('start', e.get('frame', 0)),
                    'frame_end': e.get('end', e.get('frame', 0)),
                    'type': e.get('type', 'critical')
                })
        
        if events:
            # ãƒãƒƒãƒæ¤œè¨¼
            quantum_assessments = quantum_validator.validate_events(
                events,
                lambda_result,
                network_results if network_results else None
            )
            
            logger.info(f"   âœ… Quantum validation complete")
            logger.info(f"   Events analyzed: {len(quantum_assessments)}")
            
            # çµ±è¨ˆè¡¨ç¤º
            quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
            logger.info(f"   Quantum events: {quantum_count}/{len(quantum_assessments)}")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆ
            pattern_counts = Counter(a.pattern.value for a in quantum_assessments)
            logger.info("\n   ğŸ“Š Pattern Distribution:")
            for pattern, count in pattern_counts.items():
                logger.info(f"     {pattern}: {count}")
            
            # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ¥çµ±è¨ˆ
            sig_counts = Counter(a.signature.value for a in quantum_assessments 
                               if a.signature != QuantumSignature.NONE)
            if sig_counts:
                logger.info("\n   âš›ï¸ Quantum Signatures:")
                for sig, count in sig_counts.items():
                    logger.info(f"     {sig}: {count}")
            
            # ä¿¡é ¼åº¦çµ±è¨ˆ
            confidences = [a.confidence for a in quantum_assessments if a.is_quantum]
            if confidences:
                logger.info(f"\n   ğŸ“ˆ Confidence Statistics:")
                logger.info(f"     Mean: {np.mean(confidences):.3f}")
                logger.info(f"     Max: {np.max(confidences):.3f}")
                logger.info(f"     Min: {np.min(confidences):.3f}")
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            quantum_validator.print_summary(quantum_assessments)
            
            # çµæœä¿å­˜
            save_quantum_assessments_v4(quantum_assessments, output_path, metadata)
            
        else:
            logger.warning("   No events to validate")
            
    except Exception as e:
        logger.error(f"Quantum validation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
    
    # ========================================
    # Step 5: å¯è¦–åŒ–ï¼ˆVersion 4.0å¯¾å¿œï¼‰
    # ========================================
    if enable_visualization:
        logger.info("\nğŸ“Š Creating visualizations...")
        
        try:
            visualizer = Lambda3VisualizerGPU()
            
            # LambdaÂ³çµæœã®å¯è¦–åŒ–
            fig_lambda = visualizer.visualize_results(
                lambda_result,
                save_path=str(output_path / 'lambda_results.png')
            )
            logger.info("   LambdaÂ³ results visualized")
            
            # é‡å­è©•ä¾¡ã®å¯è¦–åŒ–
            if quantum_assessments:
                fig_quantum = visualize_quantum_assessments_v4(
                    quantum_assessments,
                    save_path=str(output_path / 'quantum_assessments.png')
                )
                logger.info("   Quantum assessments visualized")
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯è¦–åŒ–
            if two_stage_result:
                fig_network = visualize_residue_network(
                    two_stage_result,
                    save_path=str(output_path / 'residue_network.png')
                )
                logger.info("   Residue network visualized")
            
            logger.info("   âœ… All visualizations saved")
            
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
    
    # ========================================
    # Step 6: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆVersion 4.0ï¼‰
    # ========================================
    logger.info("\nğŸ“ Generating comprehensive report...")
    
    report = generate_comprehensive_report_v4(
        lambda_result,
        quantum_assessments,
        two_stage_result,
        metadata,
        protein_indices
    )
    
    report_path = output_path / 'analysis_report_v4.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    logger.info(f"   Report saved to {report_path}")
    
    # ========================================
    # Step 7: Maximum Report Generation
    # ========================================
    try:
        logger.info("\nğŸ“š Generating maximum report...")
        
        # maximum_report_generatorã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè©¦è¡Œ
        try:
            from .maximum_report_generator import generate_maximum_report_from_results_v4
        except ImportError:
            # ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆ
            from lambda3_gpu.analysis.maximum_report_generator import generate_maximum_report_from_results_v4
        
        # Version 4.0å¯¾å¿œã®å‘¼ã³å‡ºã—
        max_report = generate_maximum_report_from_results_v4(
            lambda_result=lambda_result,
            two_stage_result=two_stage_result if enable_two_stage else None,
            quantum_assessments=quantum_assessments,  # v4.0: quantum_eventsã§ã¯ãªãassessments
            metadata=metadata,
            output_dir=str(output_path),
            verbose=verbose
        )
        logger.info(f"   âœ… Maximum report generated: {len(max_report):,} characters")
        
    except ImportError as e:
        logger.warning(f"   âš ï¸ Maximum report generator not found: {e}")
    except Exception as e:
        logger.warning(f"   âš ï¸ Maximum report generation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
    
    # ========================================
    # å®Œäº†
    # ========================================
    logger.info("\n" + "="*70)
    logger.info("âœ… PIPELINE COMPLETE!")
    logger.info(f"   Results directory: {output_path}")
    logger.info(f"   Key findings:")
    
    if lambda_result:
        logger.info(f"     - {len(lambda_result.critical_events)} critical events")
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        logger.info(f"     - {two_stage_result.global_network_stats.get('total_causal_links', 0)} causal links")
    if quantum_assessments:
        quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
        logger.info(f"     - {quantum_count} quantum events confirmed")
    
    logger.info("="*70)
    
    return {
        'lambda_result': lambda_result,
        'quantum_assessments': quantum_assessments,
        'two_stage_result': two_stage_result,
        'output_dir': output_path,
        'success': True
    }


# ============================================
# è£œåŠ©é–¢æ•°ç¾¤ï¼ˆVersion 4.0ï¼‰
# ============================================

def save_quantum_assessments_v4(assessments: List[QuantumAssessment], 
                               output_path: Path, metadata: Dict):
    """é‡å­è©•ä¾¡çµæœã®ä¿å­˜ï¼ˆVersion 4.0ï¼‰"""
    
    assessment_data = []
    
    for assessment in assessments:
        data = {
            'pattern': assessment.pattern.value,
            'signature': assessment.signature.value,
            'is_quantum': assessment.is_quantum,
            'confidence': float(assessment.confidence),
            'explanation': assessment.explanation,
            'criteria_met': assessment.criteria_met
        }
        
        # Lambdaç•°å¸¸æ€§
        if assessment.lambda_anomaly:
            data['lambda_anomaly'] = {
                'lambda_jump': float(assessment.lambda_anomaly.lambda_jump),
                'lambda_zscore': float(assessment.lambda_anomaly.lambda_zscore),
                'rho_t_spike': float(assessment.lambda_anomaly.rho_t_spike),
                'sigma_s_value': float(assessment.lambda_anomaly.sigma_s_value),
                'coordination': float(assessment.lambda_anomaly.coordination),
                'statistical_rarity': float(assessment.lambda_anomaly.statistical_rarity),
                'thermal_comparison': float(assessment.lambda_anomaly.thermal_comparison)
            }
        
        # åŸå­ãƒ¬ãƒ™ãƒ«è¨¼æ‹ 
        if assessment.atomic_evidence:
            data['atomic_evidence'] = {
                'max_velocity': float(assessment.atomic_evidence.max_velocity),
                'max_acceleration': float(assessment.atomic_evidence.max_acceleration),
                'correlation_coefficient': float(assessment.atomic_evidence.correlation_coefficient),
                'n_bond_anomalies': len(assessment.atomic_evidence.bond_anomalies),
                'n_dihedral_flips': len(assessment.atomic_evidence.dihedral_flips)
            }
        
        # Bellä¸ç­‰å¼ï¼ˆã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®å ´åˆï¼‰
        if assessment.bell_inequality is not None:
            data['bell_inequality'] = float(assessment.bell_inequality)
        
        # async bondsï¼ˆã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®å ´åˆï¼‰
        if assessment.async_bonds_used:
            data['async_bonds'] = assessment.async_bonds_used[:5]
        
        assessment_data.append(data)
    
    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    total = len(assessments)
    quantum_count = sum(1 for a in assessments if a.is_quantum)
    
    output_data = {
        'metadata': {
            'system_name': metadata.get('system_name', 'Unknown'),
            'temperature_K': metadata.get('temperature', 300.0),
            'dt_ps': metadata.get('time_step_ps', 100.0),
            'analysis_version': '4.0.0'
        },
        'summary': {
            'total_events': total,
            'quantum_events': quantum_count,
            'quantum_ratio': quantum_count / total if total > 0 else 0,
            'pattern_distribution': dict(Counter(a.pattern.value for a in assessments)),
            'signature_distribution': dict(Counter(a.signature.value for a in assessments 
                                                  if a.signature != QuantumSignature.NONE))
        },
        'assessments': assessment_data
    }
    
    with open(output_path / 'quantum_assessments_v4.json', 'w') as f:
        json.dump(output_data, f, indent=2)
    
    logger.info(f"   ğŸ’¾ Saved {len(assessment_data)} quantum assessments")


def visualize_quantum_assessments_v4(assessments: List[QuantumAssessment],
                                    save_path: Optional[str] = None) -> plt.Figure:
    """é‡å­è©•ä¾¡çµæœã®å¯è¦–åŒ–ï¼ˆVersion 4.0ï¼‰"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    if not assessments:
        fig.text(0.5, 0.5, 'No Assessments Available', 
                ha='center', va='center', fontsize=20)
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        return fig
    
    # 1. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ
    ax1 = axes[0, 0]
    pattern_counts = Counter(a.pattern.value for a in assessments)
    if pattern_counts:
        ax1.pie(pattern_counts.values(), 
               labels=pattern_counts.keys(),
               autopct='%1.1f%%',
               startangle=90)
        ax1.set_title('Event Pattern Distribution')
    
    # 2. ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ†å¸ƒ
    ax2 = axes[0, 1]
    sig_counts = Counter(a.signature.value for a in assessments)
    # CLASSICALã‚’é™¤å¤–
    sig_counts.pop('classical', None)
    if sig_counts:
        ax2.pie(sig_counts.values(),
               labels=sig_counts.keys(),
               autopct='%1.1f%%',
               startangle=90)
        ax2.set_title('Quantum Signature Distribution')
    
    # 3. é‡å­åˆ¤å®šç‡
    ax3 = axes[0, 2]
    n_quantum = sum(1 for a in assessments if a.is_quantum)
    n_classical = len(assessments) - n_quantum
    ax3.pie([n_quantum, n_classical],
           labels=[f'Quantum ({n_quantum})', f'Classical ({n_classical})'],
           colors=['red', 'blue'],
           autopct='%1.1f%%',
           startangle=90)
    ax3.set_title('Quantum vs Classical')
    
    # 4. ä¿¡é ¼åº¦åˆ†å¸ƒ
    ax4 = axes[1, 0]
    confidences = [a.confidence for a in assessments if a.is_quantum]
    if confidences:
        ax4.hist(confidences, bins=20, alpha=0.7, color='purple', edgecolor='black')
        ax4.set_xlabel('Confidence')
        ax4.set_ylabel('Count')
        ax4.set_title('Quantum Confidence Distribution')
        ax4.grid(True, alpha=0.3)
    
    # 5. Lambdaç•°å¸¸æ€§
    ax5 = axes[1, 1]
    lambda_zscores = [a.lambda_anomaly.lambda_zscore for a in assessments 
                      if a.lambda_anomaly and a.lambda_anomaly.lambda_zscore > 0]
    if lambda_zscores:
        ax5.hist(lambda_zscores, bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax5.axvline(x=3.0, color='red', linestyle='--', label='3Ïƒ threshold')
        ax5.set_xlabel('Lambda Z-score')
        ax5.set_ylabel('Count')
        ax5.set_title('Lambda Anomaly Distribution')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
    
    # 6. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é‡å­åˆ¤å®šç‡
    ax6 = axes[1, 2]
    pattern_quantum = {}
    for pattern in StructuralEventPattern:
        pattern_assessments = [a for a in assessments if a.pattern == pattern]
        if pattern_assessments:
            quantum_ratio = sum(1 for a in pattern_assessments if a.is_quantum) / len(pattern_assessments)
            pattern_quantum[pattern.value] = quantum_ratio * 100
    
    if pattern_quantum:
        ax6.bar(range(len(pattern_quantum)), list(pattern_quantum.values()), 
               alpha=0.7, color='steelblue')
        ax6.set_xticks(range(len(pattern_quantum)))
        ax6.set_xticklabels(list(pattern_quantum.keys()), rotation=45, ha='right')
        ax6.set_ylabel('Quantum Rate (%)')
        ax6.set_title('Quantum Rate by Pattern')
        ax6.grid(True, alpha=0.3)
    
    plt.suptitle('Quantum Assessment Results v4.0', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def visualize_residue_network(two_stage_result: Any,
                             save_path: Optional[str] = None) -> plt.Figure:
    """æ®‹åŸºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯è¦–åŒ–"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. æ®‹åŸºé‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    ax1 = axes[0, 0]
    if hasattr(two_stage_result, 'global_residue_importance'):
        top_residues = sorted(
            two_stage_result.global_residue_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        if top_residues:
            residue_ids = [f"R{r[0]+1}" for r in top_residues]
            scores = [r[1] for r in top_residues]
            
            ax1.barh(residue_ids, scores, alpha=0.7, color='steelblue')
            ax1.set_xlabel('Importance Score')
            ax1.set_title('Top 20 Important Residues')
            ax1.invert_yaxis()
    
    # 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ
    ax2 = axes[0, 1]
    if hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        labels = ['Causal', 'Sync', 'Async']
        values = [
            stats.get('total_causal_links', 0),
            stats.get('total_sync_links', 0),
            stats.get('total_async_bonds', 0)
        ]
        
        if sum(values) > 0:
            ax2.pie(values, labels=labels, autopct='%1.1f%%',
                   startangle=90, colors=['red', 'green', 'blue'])
            ax2.set_title('Network Link Distribution')
    
    # 3. ã‚¤ãƒ™ãƒ³ãƒˆåˆ¥çµ±è¨ˆ
    ax3 = axes[1, 0]
    if hasattr(two_stage_result, 'residue_analyses'):
        event_names = list(two_stage_result.residue_analyses.keys())
        n_residues = []
        
        for analysis in two_stage_result.residue_analyses.values():
            if hasattr(analysis, 'residue_events'):
                n_residues.append(len(analysis.residue_events))
            else:
                n_residues.append(0)
        
        if event_names and n_residues:
            # ã‚¤ãƒ™ãƒ³ãƒˆåãŒé•·ã„å ´åˆã¯çŸ­ç¸®
            short_names = []
            for name in event_names:
                if len(name) > 10:
                    # top_XX_score_Y.YYå½¢å¼ã‹ã‚‰ç•ªå·ã ã‘æŠ½å‡º
                    import re
                    match = re.search(r'top_(\d+)', name)
                    if match:
                        short_names.append(f"E{match.group(1)}")
                    else:
                        short_names.append(name[:10])
                else:
                    short_names.append(name)
            
            ax3.bar(range(len(event_names)), n_residues, alpha=0.7, color='orange')
            ax3.set_xticks(range(len(event_names)))
            ax3.set_xticklabels(short_names, rotation=45, ha='right')
            ax3.set_ylabel('Number of Residues Involved')
            ax3.set_title('Residues per Event')
            ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. GPUæ€§èƒ½
    ax4 = axes[1, 1]
    if hasattr(two_stage_result, 'residue_analyses'):
        gpu_times = []
        event_names = list(two_stage_result.residue_analyses.keys())
        
        for analysis in two_stage_result.residue_analyses.values():
            if hasattr(analysis, 'gpu_time'):
                gpu_times.append(analysis.gpu_time)
            else:
                gpu_times.append(0)
        
        if gpu_times and event_names:
            # ã‚¤ãƒ™ãƒ³ãƒˆåçŸ­ç¸®ï¼ˆä¸Šã¨åŒã˜ï¼‰
            short_names = []
            for name in event_names:
                if len(name) > 10:
                    import re
                    match = re.search(r'top_(\d+)', name)
                    if match:
                        short_names.append(f"E{match.group(1)}")
                    else:
                        short_names.append(name[:10])
                else:
                    short_names.append(name)
            
            ax4.plot(range(len(event_names)), gpu_times, 'go-', 
                    linewidth=2, markersize=8, alpha=0.7)
            ax4.set_xticks(range(len(event_names)))
            ax4.set_xticklabels(short_names, rotation=45, ha='right')
            ax4.set_ylabel('GPU Time (seconds)')
            ax4.set_title('GPU Processing Time per Event')
            ax4.grid(True, alpha=0.3)
            
            # å¹³å‡æ™‚é–“ã‚’è¡¨ç¤º
            if gpu_times:
                avg_time = np.mean(gpu_times)
                ax4.axhline(y=avg_time, color='red', linestyle='--', 
                           alpha=0.5, label=f'Avg: {avg_time:.2f}s')
                ax4.legend()
    
    plt.suptitle('Residue-Level Analysis Summary', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def generate_comprehensive_report_v4(
    lambda_result: Any,
    quantum_assessments: List[QuantumAssessment],
    two_stage_result: Optional[Any],
    metadata: Dict,
    protein_indices: np.ndarray
) -> str:
    """åŒ…æ‹¬çš„ãªè§£æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆï¼ˆVersion 4.0ï¼‰"""
    
    system_name = metadata.get('system_name', 'Unknown System')
    
    report = f"""# LambdaÂ³ GPU Analysis Report v4.0

## System Information
- **System**: {system_name}
- **Temperature**: {metadata.get('temperature', 300)} K
- **Time step**: {metadata.get('time_step_ps', 100)} ps
- **Frames analyzed**: {lambda_result.n_frames}
- **Total atoms**: {lambda_result.n_atoms}
- **Protein atoms**: {len(protein_indices)}
- **Analysis version**: 4.0.0 (LambdaÂ³ Integrated)

## LambdaÂ³ Analysis Results
- **Critical events**: {len(lambda_result.critical_events)}
- **Computation time**: {lambda_result.computation_time:.2f} seconds
"""
    
    # Two-Stageè§£æçµæœ
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        report += f"""
## Residue Network Analysis
- **Total causal links**: {stats.get('total_causal_links', 0)}
- **Total async bonds**: {stats.get('total_async_bonds', 0)}
- **Async/Causal ratio**: {stats.get('async_to_causal_ratio', 0):.2%}
"""
    
    # é‡å­æ¤œè¨¼çµæœï¼ˆVersion 4.0ï¼‰
    if quantum_assessments:
        total = len(quantum_assessments)
        quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
        
        report += f"""
## Quantum Validation Results v4.0

### Overview
- **Total events analyzed**: {total}
- **Quantum events confirmed**: {quantum_count} ({quantum_count/total*100:.1f}%)

### Pattern Analysis
"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆ
        for pattern in StructuralEventPattern:
            pattern_assessments = [a for a in quantum_assessments if a.pattern == pattern]
            if pattern_assessments:
                n_pattern = len(pattern_assessments)
                n_quantum = sum(1 for a in pattern_assessments if a.is_quantum)
                report += f"- **{pattern.value}**: {n_quantum}/{n_pattern} quantum ({n_quantum/n_pattern*100:.1f}%)\n"
        
        # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ†å¸ƒ
        sig_counts = Counter(a.signature.value for a in quantum_assessments 
                           if a.signature != QuantumSignature.NONE)
        if sig_counts:
            report += f"""
### Quantum Signatures Detected
"""
            for sig, count in sorted(sig_counts.items()):
                report += f"- **{sig}**: {count}\n"
        
        # ä¿¡é ¼åº¦çµ±è¨ˆ
        confidences = [a.confidence for a in quantum_assessments if a.is_quantum]
        if confidences:
            report += f"""
### Confidence Statistics
- **Mean confidence**: {np.mean(confidences):.3f}
- **Max confidence**: {np.max(confidences):.3f}
- **Min confidence**: {np.min(confidences):.3f}
"""
        
        # Lambdaç•°å¸¸æ€§çµ±è¨ˆ
        lambda_zscores = [a.lambda_anomaly.lambda_zscore for a in quantum_assessments 
                         if a.lambda_anomaly and a.lambda_anomaly.lambda_zscore > 0]
        if lambda_zscores:
            report += f"""
### Lambda Anomaly Statistics
- **Mean Z-score**: {np.mean(lambda_zscores):.2f}
- **Max Z-score**: {np.max(lambda_zscores):.2f}
- **Significant anomalies (>3Ïƒ)**: {sum(1 for z in lambda_zscores if z > 3)}
"""
    
    # çµè«–
    report += f"""
## Conclusions

The LambdaÂ³ GPU v4.0 analysis successfully identified structural changes and evaluated their quantum origins:

1. **Structural dynamics**: {len(lambda_result.critical_events)} critical structural events detected
"""
    
    if quantum_assessments:
        quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
        report += f"""2. **Quantum validation**: {quantum_count} events showed quantum signatures
3. **Pattern distribution**: Events classified into instantaneous, transition, and cascade patterns
"""
    
    if two_stage_result:
        report += f"""4. **Network analysis**: Complex residue interaction networks identified
"""
    
    report += """
## Key Improvements in v4.0

- LambdaÂ³ structure anomaly as primary input for quantum validation
- Clear 3-pattern classification (instantaneous/transition/cascade)
- Trajectory-based atomic evidence integration
- Realistic and adjustable quantum criteria
- No automatic classical assignment for long events

---
*Generated by LambdaÂ³ GPU Quantum Validation Pipeline v4.0*
*LambdaÂ³ Integrated Edition - Complete Refactoring*
"""
    
    return report


# ============================================
# CLI Interface
# ============================================

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆVersion 4.0ï¼‰"""
    
    parser = argparse.ArgumentParser(
        description='LambdaÂ³ GPU Quantum Validation Pipeline v4.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic run
  %(prog)s trajectory.npy metadata.json protein.npy
  
  # With topology
  %(prog)s trajectory.npy metadata.json protein.npy --topology topology.pdb
  
  # Custom output directory
  %(prog)s trajectory.npy metadata.json protein.npy --output ./results_v4
  
  # Skip visualizations
  %(prog)s trajectory.npy metadata.json protein.npy --no-viz
        """
    )
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('trajectory', 
                       help='Path to trajectory file (.npy)')
    parser.add_argument('metadata', 
                       help='Path to metadata file (.json)')
    parser.add_argument('protein',
                       help='Path to protein indices file (.npy)')
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°
    parser.add_argument('--topology', '-t',
                       help='Path to topology file (optional)')
    parser.add_argument('--output', '-o', 
                       default='./quantum_results_v4',
                       help='Output directory (default: ./quantum_results_v4)')
    parser.add_argument('--no-two-stage', 
                       action='store_true',
                       help='Skip two-stage residue analysis')
    parser.add_argument('--no-viz', 
                       action='store_true',
                       help='Skip visualization')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='Verbose output')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    try:
        results = run_quantum_validation_pipeline(
            trajectory_path=args.trajectory,
            metadata_path=args.metadata,
            protein_indices_path=args.protein,
            topology_path=args.topology,
            enable_two_stage=not args.no_two_stage,
            enable_visualization=not args.no_viz,
            output_dir=args.output,
            verbose=args.verbose
        )
        
        if results and results.get('success'):
            print(f"\nâœ… Success! Results saved to: {results['output_dir']}")
            return 0
        else:
            print("\nâŒ Pipeline failed. Check logs for details.")
            return 1
            
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
