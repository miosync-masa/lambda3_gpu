#!/usr/bin/env python3
"""
LambdaÂ³ GPU Quantum Validation Pipeline - Version 3.0 Complete
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

æŸ»èª­è€æ€§ï¼†å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œã®å®Œå…¨ç‰ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ã‚¿ãƒ³ãƒ‘ã‚¯è³ªå…¨åŸå­ã‚’ä½¿ã£ãŸè©³ç´°ãªæ®‹åŸºãƒ¬ãƒ™ãƒ«è§£æã«æœ€é©åŒ–

Version: 3.0.0 - Publication Ready
Authors: LambdaÂ³ Team, ç’°ã¡ã‚ƒã‚“ & ã”ä¸»äººã•ã¾ ğŸ’•
Date: 2025-08-18
"""

import numpy as np
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# LambdaÂ³ GPU imports
try:
    from lambda3_gpu.analysis.md_lambda3_detector_gpu import (
        MDLambda3DetectorGPU, 
        MDLambda3Result,
        MDConfig
    )
    from lambda3_gpu.analysis.two_stage_analyzer_gpu import (
        TwoStageAnalyzerGPU,
        TwoStageLambda3Result,
        ResidueAnalysisConfig
    )
    from lambda3_gpu.quantum import (
        QuantumValidationGPU,
        QuantumEventType,
        ValidationCriterion,
        generate_quantum_report
    )
    from lambda3_gpu.visualization import Lambda3VisualizerGPU
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure lambda3_gpu is properly installed")
    raise

# Loggerè¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('quantum_validation')

# ============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ============================================

def run_quantum_validation_pipeline(
    trajectory_path: str,
    metadata_path: str,
    protein_indices_path: str,  # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªå…¨åŸå­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå¿…é ˆï¼‰
    enable_two_stage: bool = True,
    enable_visualization: bool = True,
    output_dir: str = './quantum_results',
    verbose: bool = False
) -> Dict:
    """
    å®Œå…¨ãªé‡å­æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆVersion 3.0ï¼‰
    
    Parameters
    ----------
    trajectory_path : str
        ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (.npy)
    metadata_path : str
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (.json)
    protein_indices_path : str
        ã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸå­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (.npy) - Two-Stageã§ã¯å…¨åŸå­æ¨å¥¨
    enable_two_stage : bool
        2æ®µéšè§£æï¼ˆæ®‹åŸºãƒ¬ãƒ™ãƒ«ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã‹
    enable_visualization : bool
        å¯è¦–åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ã‹
    output_dir : str
        å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    verbose : bool
        è©³ç´°å‡ºåŠ›
        
    Returns
    -------
    Dict
        è§£æçµæœã®è¾æ›¸
    """
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info("ğŸš€ LAMBDAÂ³ GPU QUANTUM VALIDATION PIPELINE v3.0")
    logger.info("   Publication Ready Edition")
    logger.info("="*70)
    
    # ========================================
    # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
    # ========================================
    logger.info("\nğŸ“ Loading data...")
    
    try:
        # ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªèª­ã¿è¾¼ã¿
        trajectory = np.load(trajectory_path)
        logger.info(f"   Trajectory shape: {trajectory.shape}")
        n_frames, n_atoms, n_dims = trajectory.shape
        
        if n_dims != 3:
            raise ValueError(f"Trajectory must be 3D, got {n_dims}D")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        logger.info("   Metadata loaded successfully")
        
        # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ï¼ˆå¿…é ˆï¼‰
        if not Path(protein_indices_path).exists():
            raise FileNotFoundError(f"Protein indices file not found: {protein_indices_path}")
            
        protein_indices = np.load(protein_indices_path)
        logger.info(f"   Protein indices loaded: {len(protein_indices)} atoms")
        
        # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæƒ…å ±ã®è¡¨ç¤º
        if 'protein' in metadata:
            protein_info = metadata['protein']
            logger.info(f"   Protein: {protein_info.get('n_residues', 'N/A')} residues")
            logger.info(f"   Sequence length: {len(protein_info.get('sequence', ''))}")
        
        # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        max_idx = np.max(protein_indices)
        if max_idx >= n_atoms:
            raise ValueError(f"Protein index {max_idx} exceeds atom count {n_atoms}")
        
        logger.info(f"   âœ… Data validation passed")
            
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise
    
    # ========================================
    # Step 2: LambdaÂ³ GPUè§£æï¼ˆé«˜é€ŸåˆæœŸæ¢ç´¢ï¼‰
    # ========================================
    logger.info("\nğŸ”¬ Running LambdaÂ³ GPU Analysis...")
    logger.info(f"   Using {'full protein' if len(protein_indices) > 100 else 'backbone'} atoms")
    
    try:
        # MDConfigè¨­å®šï¼ˆTwo-Stageç”¨ã«æœ€é©åŒ–ï¼‰
        config = MDConfig()
        config.use_extended_detection = True
        config.use_phase_space = True
        config.use_periodic = True
        config.use_gradual = True
        config.use_drift = True
        config.adaptive_window = True
        config.window_scale = 0.005
        config.gpu_batch_size = 5000
        config.verbose = verbose
        
        # æ„Ÿåº¦ã‚’ä¸Šã’ã¦è©³ç´°ãªã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡º
        config.sensitivity = 2.0
        config.use_hierarchical = True  # éšå±¤çš„æ¤œå‡º
        
        logger.info("   Config optimized for Two-Stage analysis")
        
        # LambdaÂ³æ¤œå‡ºå™¨åˆæœŸåŒ–
        detector = MDLambda3DetectorGPU(config)
        logger.info("   Detector initialized on GPU")
        
        # è§£æå®Ÿè¡Œï¼ˆã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸå­ã®ã¿ä½¿ç”¨ï¼‰
        lambda_result = detector.analyze(trajectory, protein_indices)
        
        logger.info(f"   âœ… LambdaÂ³ analysis complete")
        logger.info(f"   Critical events detected: {len(lambda_result.critical_events)}")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°è¡¨ç¤º
        if verbose and lambda_result.critical_events:
            for i, event in enumerate(lambda_result.critical_events[:5]):
                logger.info(f"     Event {i}: frames {event[0]}-{event[1]}")
        
        # çµæœã®ä¿å­˜
        result_summary = {
            'n_frames': lambda_result.n_frames,
            'n_atoms': lambda_result.n_atoms,
            'n_protein_atoms': len(protein_indices),
            'n_critical_events': len(lambda_result.critical_events),
            'computation_time': lambda_result.computation_time,
            'gpu_info': getattr(lambda_result, 'gpu_info', {})
        }
        
        with open(output_path / 'lambda_result_summary.json', 'w') as f:
            json.dump(result_summary, f, indent=2)
            
    except Exception as e:
        logger.error(f"LambdaÂ³ analysis failed: {e}")
        raise
    
    # ========================================
    # Step 3: Two-Stageè©³ç´°è§£æï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
    # ========================================
    two_stage_result = None
    
    if enable_two_stage and len(lambda_result.critical_events) > 0:
        logger.info("\nğŸ”¬ Running Two-Stage Residue-Level Analysis...")
        logger.info("   This is the main analysis for protein dynamics")
        
        try:
            # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ®‹åŸºæ•°ã®å–å¾—
            if 'protein' in metadata and 'n_residues' in metadata['protein']:
                n_protein_residues = metadata['protein']['n_residues']
            elif 'n_protein_residues' in metadata:
                n_protein_residues = metadata['n_protein_residues']
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆTrpCageã®å ´åˆï¼‰
                n_protein_residues = 20
                logger.warning(f"   Using default n_residues: {n_protein_residues}")
            
            logger.info(f"   Protein residues: {n_protein_residues}")
            
            # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªéƒ¨åˆ†ã®ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚’æŠ½å‡º
            protein_trajectory = trajectory[:, protein_indices, :]
            logger.info(f"   Protein trajectory: {protein_trajectory.shape}")
            
            # ã‚¤ãƒ™ãƒ³ãƒˆçª“ã®ä½œæˆï¼ˆæ‹¡å¼µç‰ˆ - è§£æã«ååˆ†ãªå¹…ã‚’ç¢ºä¿ï¼‰
            events = []
            MIN_WINDOW_SIZE = 50  # æœ€å°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            CONTEXT_FRAMES = 100  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            
            for i, event in enumerate(lambda_result.critical_events[:10]):  # æœ€å¤§10ã‚¤ãƒ™ãƒ³ãƒˆ
                # ã‚¤ãƒ™ãƒ³ãƒˆã®ä¸­å¿ƒãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç‰¹å®š
                if isinstance(event, (tuple, list)) and len(event) >= 2:
                    center = (int(event[0]) + int(event[1])) // 2
                    event_width = int(event[1]) - int(event[0]) + 1
                elif isinstance(event, dict):
                    center = event.get('frame', event.get('start', 0))
                    event_width = event.get('end', center) - event.get('start', center) + 1
                elif hasattr(event, 'frame'):
                    center = event.frame
                    event_width = 1
                else:
                    continue
                
                # è§£æã«ååˆ†ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’ç¢ºä¿
                window_size = max(MIN_WINDOW_SIZE, event_width + CONTEXT_FRAMES)
                half_window = window_size // 2
                
                # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ç¯„å›²ã‚’è¨ˆç®—ï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                start = max(0, center - half_window)
                end = min(n_frames - 1, center + half_window)
                
                # æœ€å°ã‚µã‚¤ã‚ºã‚’ç¢ºä¿
                if end - start < MIN_WINDOW_SIZE:
                    if start == 0:
                        end = min(n_frames - 1, start + MIN_WINDOW_SIZE)
                    elif end == n_frames - 1:
                        start = max(0, end - MIN_WINDOW_SIZE)
                
                # ã‚¤ãƒ™ãƒ³ãƒˆåã‚’ä»˜ä¸
                event_name = f'critical_{i}'
                events.append((start, end, event_name))
                logger.info(f"     Event {event_name}: frames {start}-{end} ({end-start+1} frames)")
            
            if events:
                logger.info(f"   Processing {len(events)} events for residue analysis")
                logger.info(f"   Average window size: {np.mean([e[1]-e[0]+1 for e in events]):.1f} frames")
                
                # æ®‹åŸºè§£æè¨­å®šï¼ˆæ„Ÿåº¦ã‚’ä¸Šã’ã¦è©³ç´°ã«è§£æï¼‰
                residue_config = ResidueAnalysisConfig()
                residue_config.sensitivity = 1.5  # æ„Ÿåº¦ã‚’é©åº¦ã«
                residue_config.correlation_threshold = 0.10  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
                residue_config.use_confidence = True
                residue_config.n_bootstrap = 100
                residue_config.parallel_events = True
                residue_config.min_window_size = 20  # æœ€å°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                residue_config.max_window_size = 200  # æœ€å¤§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                residue_config.adaptive_window = True
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                logger.info(f"   Config: sensitivity={residue_config.sensitivity}, "
                          f"threshold={residue_config.correlation_threshold}")
                
                # TwoStageAnalyzerGPUåˆæœŸåŒ–
                analyzer = TwoStageAnalyzerGPU(residue_config)
                logger.info("   Two-stage analyzer initialized")
                
                # Anomaly scoresã‚’æº–å‚™ï¼ˆLambdaÂ³çµæœã‹ã‚‰ï¼‰
                anomaly_scores = None
                if hasattr(lambda_result, 'anomaly_scores'):
                    # å„ã‚¤ãƒ™ãƒ³ãƒˆã®anomaly scoresã‚’æŠ½å‡º
                    anomaly_scores = {}
                    for start, end, name in events:
                        if 'structural' in lambda_result.anomaly_scores:
                            scores = lambda_result.anomaly_scores['structural'][start:end+1]
                            anomaly_scores[name] = scores
                        elif 'combined' in lambda_result.anomaly_scores:
                            scores = lambda_result.anomaly_scores['combined'][start:end+1]
                            anomaly_scores[name] = scores
                    
                    if anomaly_scores:
                        logger.info(f"   Anomaly scores prepared for {len(anomaly_scores)} events")
                
                # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ã¿ã®ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã§è§£æ
                two_stage_result = analyzer.analyze_trajectory(
                    protein_trajectory,      # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ã¿
                    lambda_result,          # ãƒã‚¯ãƒ­çµæœï¼ˆanomaly_scoreså«ã‚€ï¼‰
                    events,                 # ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µæ¸ˆã¿ï¼‰
                    n_protein_residues,     # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ®‹åŸºæ•°
                    anomaly_scores=anomaly_scores  # æ˜ç¤ºçš„ã«æ¸¡ã™
                )
                
                logger.info("   âœ… Two-stage analysis complete")
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆã‚’è¡¨ç¤º
                if hasattr(two_stage_result, 'global_network_stats'):
                    stats = two_stage_result.global_network_stats
                    logger.info("\n   ğŸŒ Global Network Statistics:")
                    logger.info(f"     Total causal links: {stats.get('total_causal_links', 0)}")
                    logger.info(f"     Total sync links: {stats.get('total_sync_links', 0)}")
                    logger.info(f"     Total async bonds: {stats.get('total_async_bonds', 0)}")
                    
                    # çµæœãŒ0ã®å ´åˆã®è¨ºæ–­
                    if stats.get('total_causal_links', 0) == 0:
                        logger.warning("   âš ï¸ No causal links detected. Possible causes:")
                        logger.warning("     - Correlation threshold too high")
                        logger.warning("     - Window size too small")
                        logger.warning("     - Insufficient anomaly in the data")
                        
                        # å†è©¦è¡Œã®ææ¡ˆ
                        logger.info("\n   ğŸ”„ Attempting re-analysis with adjusted parameters...")
                        residue_config.correlation_threshold = 0.05  # ã•ã‚‰ã«ä¸‹ã’ã‚‹
                        residue_config.sensitivity = 1.0
                        residue_config.min_lag = 1
                        residue_config.max_lag = 20
                        
                        # å†è§£æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        # two_stage_result = analyzer.analyze_trajectory(...)
                    else:
                        logger.info(f"     Async/Causal ratio: {stats.get('async_to_causal_ratio', 0):.2%}")
                        logger.info(f"     Mean adaptive window: {stats.get('mean_adaptive_window', 0):.1f} frames")
                
                # é‡è¦æ®‹åŸºã®è¡¨ç¤º
                if hasattr(two_stage_result, 'global_residue_importance'):
                    importance_scores = two_stage_result.global_residue_importance
                    if importance_scores:
                        top_residues = sorted(
                            importance_scores.items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:5]
                        
                        if top_residues:
                            logger.info("\n   ğŸ¯ Top Important Residues:")
                            for res_id, score in top_residues:
                                if 'protein' in metadata and 'residue_mapping' in metadata['protein']:
                                    res_name = metadata['protein']['residue_mapping'].get(
                                        str(res_id), {}
                                    ).get('name', f'RES{res_id+1}')
                                else:
                                    res_name = f'RES{res_id+1}'
                                logger.info(f"     {res_name}: {score:.3f}")
                    else:
                        logger.warning("   No residue importance scores calculated")
                
                # ä»‹å…¥ãƒã‚¤ãƒ³ãƒˆã®ææ¡ˆ
                if hasattr(two_stage_result, 'suggested_intervention_points'):
                    points = two_stage_result.suggested_intervention_points[:3]
                    if points:
                        logger.info(f"\n   ğŸ’¡ Suggested intervention points: {points}")
                    else:
                        logger.info("\n   ğŸ’¡ No specific intervention points identified")
                        
                # è©³ç´°è¨ºæ–­æƒ…å ±
                if verbose:
                    logger.info("\n   ğŸ“Š Detailed Diagnostics:")
                    if hasattr(two_stage_result, 'residue_analyses'):
                        for event_name, analysis in two_stage_result.residue_analyses.items():
                            logger.info(f"     {event_name}:")
                            if hasattr(analysis, 'residue_events'):
                                logger.info(f"       - Residue events: {len(analysis.residue_events)}")
                            if hasattr(analysis, 'network_result'):
                                network = analysis.network_result
                                if hasattr(network, 'causal_network'):
                                    logger.info(f"       - Causal links: {len(network.causal_network)}")
                                if hasattr(network, 'async_strong_bonds'):
                                    logger.info(f"       - Async bonds: {len(network.async_strong_bonds)}")
                    
            else:
                logger.warning("   No events suitable for Two-Stage analysis")
                
        except Exception as e:
            logger.error(f"Two-stage analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            # ç¶šè¡Œï¼ˆé‡å­æ¤œè¨¼ã¯å¯èƒ½ï¼‰
    
    # ========================================
    # Step 4: é‡å­æ¤œè¨¼ï¼ˆVersion 3.0å¯¾å¿œç‰ˆï¼‰
    # ========================================
    logger.info("\nâš›ï¸ Running Quantum Validation (v3.0)...")
    
    quantum_events = []
    
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        temperature = metadata.get('temperature', 
                                  metadata.get('simulation', {}).get('temperature_K', 310.0))
        dt_ps = metadata.get('time_step_ps', 
                           metadata.get('simulation', {}).get('dt_ps', 2.0))
        
        # é‡å­æ¤œè¨¼å™¨åˆæœŸåŒ–ï¼ˆVersion 3.0ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        quantum_validator = QuantumValidationGPU(
            trajectory=trajectory[:, protein_indices, :],  # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ã¿
            metadata=metadata,
            dt_ps=dt_ps,                      # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
            temperature_K=temperature,         # æ¸©åº¦
            bootstrap_iterations=1000,         # Bootstrapåå¾©æ•°
            significance_level=0.01,           # æœ‰æ„æ°´æº–
            force_cpu=False                    # GPUä½¿ç”¨
        )
        
        logger.info(f"   Quantum validator v3.0 initialized")
        logger.info(f"   Temperature: {temperature:.1f} K")
        logger.info(f"   Time step: {dt_ps:.1f} ps")
        logger.info(f"   Thermal decoherence: {quantum_validator.thermal_decoherence_ps:.3e} ps")
        
        # é‡å­ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰è§£æï¼ˆVersion 3.0ï¼‰
        # two_stage_resultã‚’ãã®ã¾ã¾æ¸¡ã™ï¼ˆå†…éƒ¨ã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
        quantum_events = quantum_validator.analyze_quantum_cascade(
            lambda_result,
            two_stage_result  # Version 3.0: ç›´æ¥æ¸¡ã™
        )
        
        logger.info(f"   âœ… Quantum validation complete")
        logger.info(f"   Quantum events detected: {len(quantum_events)}")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆï¼ˆVersion 3.0ã®æ–°æ©Ÿèƒ½ï¼‰
        event_types = Counter(e.event_type.value for e in quantum_events)
        
        logger.info("\n   ğŸ“Š Event Type Distribution:")
        for event_type, count in event_types.items():
            logger.info(f"     {event_type}: {count}")
        
        # é‡å­ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        quantum_only = [e for e in quantum_events if e.quantum_metrics.is_quantum]
        logger.info(f"   Confirmed quantum events: {len(quantum_only)}/{len(quantum_events)}")
        
        # åˆ¤å®šåŸºæº–ã®çµ±è¨ˆï¼ˆVersion 3.0ã®æ–°æ©Ÿèƒ½ï¼‰
        criterion_stats = {}
        for event in quantum_events:
            for criterion in event.quantum_metrics.criteria_passed:
                name = criterion.criterion.value
                if name not in criterion_stats:
                    criterion_stats[name] = {'passed': 0, 'total': 0}
                criterion_stats[name]['total'] += 1
                if criterion.passed:
                    criterion_stats[name]['passed'] += 1
        
        if criterion_stats:
            logger.info("\n   ğŸ“ˆ Validation Criteria Statistics:")
            for name, stats in sorted(criterion_stats.items()):
                if stats['total'] > 0:
                    rate = stats['passed'] / stats['total'] * 100
                    logger.info(f"     {name}: {stats['passed']}/{stats['total']} ({rate:.1f}%)")
        
        # è‡¨ç•Œã‚¤ãƒ™ãƒ³ãƒˆã®ç‰¹å®šï¼ˆVersion 3.0ï¼‰
        critical_quantum = [e for e in quantum_events if e.is_critical]
        if critical_quantum:
            logger.info(f"\n   ğŸ’« Critical quantum events: {len(critical_quantum)}")
            for i, event in enumerate(critical_quantum[:3]):
                qm = event.quantum_metrics
                logger.info(f"     {i+1}. Frame {event.frame_start}-{event.frame_end}")
                logger.info(f"        Type: {event.event_type.value}")
                logger.info(f"        Quantum confidence: {qm.quantum_confidence:.3f}")
                if qm.bell_violated:
                    logger.info(f"        CHSH: {qm.chsh_value:.3f} (p={qm.chsh_p_value:.4f})")
                logger.info(f"        Reasons: {', '.join(event.critical_reasons)}")
        
        # Bellé•åã®è©³ç´°çµ±è¨ˆ
        bell_violations = [e for e in quantum_events 
                          if e.quantum_metrics.bell_violated]
        
        if bell_violations:
            violation_rate = 100 * len(bell_violations) / len(quantum_events)
            logger.info(f"\n   ğŸ”” Bell violations: {len(bell_violations)}/{len(quantum_events)} "
                       f"({violation_rate:.1f}%)")
            
            # CHSHå€¤ã®çµ±è¨ˆ
            chsh_values = [e.quantum_metrics.chsh_value for e in bell_violations]
            raw_values = [e.quantum_metrics.chsh_raw_value for e in bell_violations]
            
            logger.info(f"   CHSH statistics:")
            logger.info(f"     Corrected: mean={np.mean(chsh_values):.3f}, "
                       f"max={np.max(chsh_values):.3f}")
            logger.info(f"     Raw: mean={np.mean(raw_values):.3f}, "
                       f"max={np.max(raw_values):.3f}")
            logger.info(f"     Tsirelson bound: {2*np.sqrt(2):.3f}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆVersion 3.0ã®æ‹¡å¼µç‰ˆï¼‰
        quantum_validator.print_validation_summary(quantum_events)
        
        # é‡å­ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°ä¿å­˜ï¼ˆVersion 3.0å¯¾å¿œï¼‰
        save_quantum_events_v3(quantum_events, output_path, metadata)
        
        # æŸ»èª­ç”¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆVersion 3.0ã®æ–°æ©Ÿèƒ½ï¼‰
        report = generate_quantum_report(quantum_events)
        
        with open(output_path / 'quantum_validation_report.txt', 'w') as f:
            f.write(report)
        logger.info(f"   ğŸ“„ Validation report saved")
        
    except Exception as e:
        logger.error(f"Quantum validation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
    
    # ========================================
    # Step 5: å¯è¦–åŒ–ï¼ˆè©³ç´°ç‰ˆï¼‰
    # ========================================
    if enable_visualization:
        logger.info("\nğŸ“Š Creating detailed visualizations...")
        
        try:
            visualizer = Lambda3VisualizerGPU()
            
            # LambdaÂ³çµæœã®å¯è¦–åŒ–
            fig_lambda = visualizer.visualize_results(
                lambda_result,
                save_path=str(output_path / 'lambda_results.png')
            )
            logger.info("   LambdaÂ³ results visualized")
            
            # é‡å­ã‚¤ãƒ™ãƒ³ãƒˆã®å¯è¦–åŒ–
            if quantum_events:
                fig_quantum = visualize_quantum_results(
                    quantum_events,
                    save_path=str(output_path / 'quantum_events.png')
                )
                logger.info("   Quantum events visualized")
            
            # Two-Stageçµæœã®å¯è¦–åŒ–
            if two_stage_result:
                fig_network = visualize_residue_network(
                    two_stage_result,
                    save_path=str(output_path / 'residue_network.png')
                )
                logger.info("   Residue network visualized")
            
            logger.info("   âœ… All visualizations saved")
            
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
    
    # ========================================
    # Step 6: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    # ========================================
    logger.info("\nğŸ“ Generating comprehensive report...")
    
    report = generate_comprehensive_report(
        lambda_result,
        quantum_events,
        two_stage_result,
        metadata,
        protein_indices
    )
    
    report_path = output_path / 'analysis_report.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    logger.info(f"   Report saved to {report_path}")
    
    # ========================================
    # å®Œäº†
    # ========================================
    logger.info("\n" + "="*70)
    logger.info("âœ… PIPELINE COMPLETE!")
    logger.info(f"   Results directory: {output_path}")
    logger.info(f"   Key findings:")
    
    if lambda_result:
        logger.info(f"     - {len(lambda_result.critical_events)} critical events")
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        logger.info(f"     - {two_stage_result.global_network_stats.get('total_causal_links', 0)} causal links")
    if quantum_events:
        logger.info(f"     - {len(quantum_events)} quantum signatures")
        logger.info(f"     - {len(quantum_only)} confirmed quantum events")
        logger.info(f"     - {len(bell_violations)} Bell violations")
    
    logger.info("="*70)
    
    return {
        'lambda_result': lambda_result,
        'quantum_events': quantum_events,
        'two_stage_result': two_stage_result,
        'output_dir': output_path,
        'success': True
    }


# ============================================
# è£œåŠ©é–¢æ•°ç¾¤
# ============================================

def save_quantum_events_v3(quantum_events: List, output_path: Path, metadata: Dict):
    """é‡å­ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°ä¿å­˜ï¼ˆVersion 3.0å¯¾å¿œï¼‰"""
    quantum_data = []
    
    for event in quantum_events:
        event_dict = {
            'frame_start': event.frame_start,
            'frame_end': event.frame_end,
            'event_type': event.event_type.value,
            'is_critical': event.is_critical,
            'critical_reasons': event.critical_reasons,
            'validation_window': list(event.validation_window)
        }
        
        # é‡å­ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆVersion 3.0ã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
        qm = event.quantum_metrics
        event_dict['quantum_metrics'] = {
            # åŸºæœ¬åˆ†é¡
            'event_type': qm.event_type.value,
            'duration_frames': qm.duration_frames,
            'duration_ps': qm.duration_ps,
            
            # é‡å­åˆ¤å®š
            'is_quantum': qm.is_quantum,
            'quantum_confidence': float(qm.quantum_confidence),
            'quantum_score': float(qm.quantum_score),
            
            # CHSHæ¤œè¨¼
            'bell_violated': qm.bell_violated,
            'chsh_value': float(qm.chsh_value),
            'chsh_raw_value': float(qm.chsh_raw_value),
            'chsh_confidence': float(qm.chsh_confidence),
            'chsh_p_value': float(qm.chsh_p_value),
            
            # ç‰©ç†æŒ‡æ¨™
            'coherence_time_ps': float(qm.coherence_time_ps),
            'thermal_ratio': float(qm.thermal_ratio),
            'tunneling_probability': float(qm.tunneling_probability),
            'energy_barrier_kT': float(qm.energy_barrier_kT),
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŒ‡æ¨™
            'n_async_bonds': qm.n_async_bonds,
            'max_causality': float(qm.max_causality),
            'min_sync_rate': float(qm.min_sync_rate),
            'mean_lag_frames': float(qm.mean_lag_frames),
            
            # çµ±è¨ˆæƒ…å ±
            'n_samples_used': qm.n_samples_used,
            'data_quality': float(qm.data_quality),
            'bootstrap_iterations': qm.bootstrap_iterations
        }
        
        # åˆ¤å®šåŸºæº–ã®è©³ç´°ï¼ˆVersion 3.0ï¼‰
        event_dict['criteria_passed'] = []
        for criterion in qm.criteria_passed:
            event_dict['criteria_passed'].append({
                'criterion': criterion.criterion.value,
                'reference': criterion.reference,
                'value': float(criterion.value),
                'threshold': float(criterion.threshold),
                'passed': criterion.passed,
                'p_value': float(criterion.p_value) if criterion.p_value else None,
                'description': criterion.description
            })
        
        # æ®‹åŸºæƒ…å ±ï¼ˆå¯¾å¿œã™ã‚‹æ®‹åŸºIDã‚’è¨˜éŒ²ï¼‰
        event_dict['residue_ids'] = event.residue_ids
        
        # async bondsæƒ…å ±ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æã‹ã‚‰ï¼‰
        if event.async_bonds_used:
            event_dict['async_bonds'] = []
            for bond in event.async_bonds_used[:10]:  # æœ€å¤§10å€‹
                bond_dict = {
                    'residue_pair': bond.get('residue_pair', []),
                    'strength': float(bond.get('strength', 0)),
                    'lag': int(bond.get('lag', 0)),
                    'sync_rate': float(bond.get('sync_rate', 0)),
                    'type': bond.get('type', 'unknown')
                }
                event_dict['async_bonds'].append(bond_dict)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ
        if event.network_stats:
            event_dict['network_stats'] = {
                'n_async_bonds': len(event.network_stats.get('async_bonds', [])),
                'n_causal_links': len(event.network_stats.get('causal_links', [])),
                'network_type': event.network_stats.get('network_type')
            }
        
        # çµ±è¨ˆçš„æ¤œå®šçµæœï¼ˆã‚ã‚Œã°ï¼‰
        if event.statistical_tests:
            event_dict['statistical_tests'] = event.statistical_tests
        
        quantum_data.append(event_dict)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã¦ä¿å­˜
    output_data = {
        'metadata': {
            'system_name': metadata.get('system_name', 'Unknown'),
            'temperature_K': metadata.get('temperature', 310.0),
            'dt_ps': metadata.get('time_step_ps', 2.0),
            'n_protein_residues': metadata.get('protein', {}).get('n_residues', 0),
            'analysis_version': '3.0.0'
        },
        'summary': {
            'total_events': len(quantum_events),
            'quantum_events': sum(1 for e in quantum_events if e.quantum_metrics.is_quantum),
            'bell_violations': sum(1 for e in quantum_events if e.quantum_metrics.bell_violated),
            'critical_events': sum(1 for e in quantum_events if e.is_critical),
            'event_type_distribution': dict(Counter(e.event_type.value for e in quantum_events))
        },
        'events': quantum_data
    }
    
    with open(output_path / 'quantum_events_v3.json', 'w') as f:
        json.dump(output_data, f, indent=2)
    
    logger.info(f"   ğŸ’¾ Saved {len(quantum_data)} quantum events with full v3.0 metrics")


def visualize_quantum_results(quantum_events: List, 
                             save_path: Optional[str] = None) -> plt.Figure:
    """é‡å­æ¤œè¨¼çµæœã®å¯è¦–åŒ–ï¼ˆVersion 3.0å¯¾å¿œï¼‰"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    if not quantum_events:
        fig.text(0.5, 0.5, 'No Quantum Events Detected', 
                ha='center', va='center', fontsize=20)
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        return fig
    
    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    frames = []
    chsh_values = []
    quantum_scores = []
    confidences = []
    event_type_list = []
    
    for e in quantum_events:
        if hasattr(e, 'frame_start'):
            frames.append(e.frame_start)
        if hasattr(e, 'quantum_metrics'):
            chsh_values.append(e.quantum_metrics.chsh_value)
            quantum_scores.append(e.quantum_metrics.quantum_score)
            confidences.append(e.quantum_metrics.chsh_confidence)
        if hasattr(e, 'event_type'):
            event_type_list.append(e.event_type.value)
    
    # 1. CHSHå€¤ã®æ™‚ç³»åˆ—
    ax1 = axes[0, 0]
    if frames and chsh_values:
        ax1.scatter(frames[:len(chsh_values)], chsh_values[:len(frames)], 
                   c='blue', alpha=0.6, s=50)
        ax1.axhline(y=2.0, color='red', linestyle='--', label='Classical Bound')
        ax1.axhline(y=2*np.sqrt(2), color='green', linestyle='--', 
                   label=f'Tsirelson Bound ({2*np.sqrt(2):.3f})')
        ax1.set_xlabel('Frame')
        ax1.set_ylabel('CHSH Value')
        ax1.set_title('CHSH Inequality Timeline')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # 2. ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ†å¸ƒï¼ˆVersion 3.0ï¼‰
    ax2 = axes[0, 1]
    event_counts = Counter(event_type_list)
    if event_counts:
        ax2.pie(event_counts.values(), 
               labels=event_counts.keys(),
               autopct='%1.1f%%',
               startangle=90)
        ax2.set_title('Event Type Distribution')
    
    # 3. Bellé•åã®åˆ†å¸ƒ
    ax3 = axes[0, 2]
    n_violated = sum(1 for e in quantum_events 
                    if hasattr(e, 'quantum_metrics') 
                    and e.quantum_metrics.bell_violated)
    n_classical = len(quantum_events) - n_violated
    
    ax3.pie([n_violated, n_classical], 
           labels=[f'Violated ({n_violated})', f'Classical ({n_classical})'],
           colors=['red', 'blue'],
           autopct='%1.1f%%',
           startangle=90)
    ax3.set_title('Bell Violation Distribution')
    
    # 4. é‡å­ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    ax4 = axes[1, 0]
    if quantum_scores:
        ax4.hist(quantum_scores, bins=20, alpha=0.7, color='purple', edgecolor='black')
        ax4.set_xlabel('Quantum Score')
        ax4.set_ylabel('Count')
        ax4.set_title('Quantum Score Distribution')
        ax4.grid(True, alpha=0.3)
    
    # 5. ä¿¡é ¼åº¦vs CHSHå€¤
    ax5 = axes[1, 1]
    if confidences and chsh_values and quantum_scores:
        scatter = ax5.scatter(confidences[:len(chsh_values)], 
                            chsh_values[:len(confidences)], 
                            c=quantum_scores[:min(len(confidences), len(chsh_values))], 
                            cmap='viridis',
                            s=50, alpha=0.6)
        plt.colorbar(scatter, ax=ax5, label='Quantum Score')
        ax5.set_xlabel('Confidence')
        ax5.set_ylabel('CHSH Value')
        ax5.set_title('Confidence vs CHSH Value')
        ax5.grid(True, alpha=0.3)
    
    # 6. åˆ¤å®šåŸºæº–é€šéç‡ï¼ˆVersion 3.0ï¼‰
    ax6 = axes[1, 2]
    criterion_counts = {}
    for e in quantum_events:
        if hasattr(e, 'quantum_metrics'):
            for criterion in e.quantum_metrics.criteria_passed:
                name = criterion.criterion.value
                if name not in criterion_counts:
                    criterion_counts[name] = {'passed': 0, 'total': 0}
                criterion_counts[name]['total'] += 1
                if criterion.passed:
                    criterion_counts[name]['passed'] += 1
    
    if criterion_counts:
        names = list(criterion_counts.keys())
        pass_rates = [c['passed']/c['total']*100 if c['total'] > 0 else 0 
                     for c in criterion_counts.values()]
        ax6.bar(range(len(names)), pass_rates, alpha=0.7, color='steelblue')
        ax6.set_xticks(range(len(names)))
        ax6.set_xticklabels(names, rotation=45, ha='right')
        ax6.set_ylabel('Pass Rate (%)')
        ax6.set_title('Validation Criteria Pass Rates')
        ax6.grid(True, alpha=0.3)
    
    plt.suptitle('Quantum Validation Results v3.0', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def visualize_residue_network(two_stage_result: Any,
                             save_path: Optional[str] = None) -> plt.Figure:
    """æ®‹åŸºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯è¦–åŒ–"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. æ®‹åŸºé‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    ax1 = axes[0, 0]
    if hasattr(two_stage_result, 'global_residue_importance'):
        top_residues = sorted(
            two_stage_result.global_residue_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        if top_residues:
            residue_ids = [f"R{r[0]+1}" for r in top_residues]
            scores = [r[1] for r in top_residues]
            
            ax1.barh(residue_ids, scores, alpha=0.7, color='steelblue')
            ax1.set_xlabel('Importance Score')
            ax1.set_title('Top 20 Important Residues')
            ax1.invert_yaxis()
    
    # 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ
    ax2 = axes[0, 1]
    if hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        labels = ['Causal', 'Sync', 'Async']
        values = [
            stats.get('total_causal_links', 0),
            stats.get('total_sync_links', 0),
            stats.get('total_async_bonds', 0)
        ]
        
        if sum(values) > 0:
            ax2.pie(values, labels=labels, autopct='%1.1f%%',
                   startangle=90, colors=['red', 'green', 'blue'])
            ax2.set_title('Network Link Distribution')
    
    # 3. ã‚¤ãƒ™ãƒ³ãƒˆåˆ¥çµ±è¨ˆ
    ax3 = axes[1, 0]
    if hasattr(two_stage_result, 'residue_analyses'):
        event_names = list(two_stage_result.residue_analyses.keys())
        n_residues = [len(a.residue_events) 
                      for a in two_stage_result.residue_analyses.values()]
        
        if event_names and n_residues:
            ax3.bar(range(len(event_names)), n_residues, alpha=0.7, color='orange')
            ax3.set_xticks(range(len(event_names)))
            ax3.set_xticklabels(event_names, rotation=45, ha='right')
            ax3.set_ylabel('Number of Residues Involved')
            ax3.set_title('Residues per Event')
    
    # 4. GPUæ€§èƒ½
    ax4 = axes[1, 1]
    if hasattr(two_stage_result, 'residue_analyses'):
        gpu_times = [a.gpu_time for a in two_stage_result.residue_analyses.values()]
        event_names = list(two_stage_result.residue_analyses.keys())
        
        if gpu_times and event_names:
            ax4.plot(range(len(event_names)), gpu_times, 'go-', 
                    linewidth=2, markersize=8)
            ax4.set_xticks(range(len(event_names)))
            ax4.set_xticklabels(event_names, rotation=45, ha='right')
            ax4.set_ylabel('GPU Time (seconds)')
            ax4.set_title('GPU Processing Time per Event')
            ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Residue-Level Analysis Summary', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def generate_comprehensive_report(
    lambda_result: Any,
    quantum_events: List,
    two_stage_result: Optional[Any],
    metadata: Dict,
    protein_indices: np.ndarray
) -> str:
    """åŒ…æ‹¬çš„ãªè§£æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆï¼ˆVersion 3.0å¯¾å¿œï¼‰"""
    
    # ã‚·ã‚¹ãƒ†ãƒ åã®å–å¾—
    system_name = metadata.get('system_name', 'Unknown System')
    
    report = f"""# LambdaÂ³ GPU Analysis Report v3.0

## System Information
- **System**: {system_name}
- **Temperature**: {metadata.get('temperature', metadata.get('simulation', {}).get('temperature_K', 310))} K
- **Time step**: {metadata.get('time_step_ps', metadata.get('simulation', {}).get('dt_ps', 2.0))} ps
- **Frames analyzed**: {lambda_result.n_frames}
- **Total atoms**: {lambda_result.n_atoms}
- **Protein atoms**: {len(protein_indices)}
"""
    
    # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæƒ…å ±
    if 'protein' in metadata:
        protein_info = metadata['protein']
        report += f"""- **Protein residues**: {protein_info.get('n_residues', 'N/A')}
- **Sequence length**: {len(protein_info.get('sequence', ''))}
"""
    
    report += f"""- **Computation time**: {lambda_result.computation_time:.2f} seconds
- **Analysis version**: 3.0.0 (Publication Ready)

## LambdaÂ³ Analysis Results
- **Critical events**: {len(lambda_result.critical_events)}
"""
    
    # æ§‹é€ å¢ƒç•Œã¨ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ç ´ã‚Œ
    if hasattr(lambda_result, 'structural_boundaries'):
        report += f"- **Structural boundaries**: {len(lambda_result.structural_boundaries)}\n"
    
    if hasattr(lambda_result, 'topological_breaks'):
        report += f"- **Topological breaks**: {len(lambda_result.topological_breaks)}\n"
    
    # Two-Stageè§£æçµæœ
    if two_stage_result:
        report += f"""
## Two-Stage Residue-Level Analysis

### Overview
"""
        if hasattr(two_stage_result, 'residue_analyses'):
            report += f"- **Events analyzed**: {len(two_stage_result.residue_analyses)}\n"
        
        if hasattr(two_stage_result, 'global_network_stats'):
            stats = two_stage_result.global_network_stats
            report += f"""
### Network Statistics
- **Total causal links**: {stats.get('total_causal_links', 0)}
- **Total sync links**: {stats.get('total_sync_links', 0)}
- **Total async bonds**: {stats.get('total_async_bonds', 0)}
- **Async/Causal ratio**: {stats.get('async_to_causal_ratio', 0):.2%}
- **Mean adaptive window**: {stats.get('mean_adaptive_window', 0):.1f} frames
"""
        
        # é‡è¦æ®‹åŸº
        if hasattr(two_stage_result, 'global_residue_importance'):
            top_residues = sorted(
                two_stage_result.global_residue_importance.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]
            
            if top_residues:
                report += """
### Top Important Residues
| Rank | Residue | Importance Score |
|------|---------|-----------------|
"""
                for i, (res_id, score) in enumerate(top_residues, 1):
                    # æ®‹åŸºåã®å–å¾—
                    if 'protein' in metadata and 'residue_mapping' in metadata['protein']:
                        res_name = metadata['protein']['residue_mapping'].get(
                            str(res_id), {}
                        ).get('name', f'RES{res_id+1}')
                    else:
                        res_name = f'RES{res_id+1}'
                    report += f"| {i} | {res_name} | {score:.3f} |\n"
        
        # ä»‹å…¥ãƒã‚¤ãƒ³ãƒˆ
        if hasattr(two_stage_result, 'suggested_intervention_points'):
            points = two_stage_result.suggested_intervention_points[:5]
            if points:
                report += f"\n### Suggested Intervention Points\n"
                report += f"Residues: {points}\n"
    
    # é‡å­æ¤œè¨¼çµæœï¼ˆVersion 3.0æ‹¡å¼µï¼‰
    report += f"""
## Quantum Validation Results (v3.0)
- **Total events analyzed**: {len(quantum_events)}
"""
    
    if quantum_events:
        # åŸºæœ¬çµ±è¨ˆ
        n_quantum = sum(1 for e in quantum_events if e.quantum_metrics.is_quantum)
        n_bell = sum(1 for e in quantum_events if e.quantum_metrics.bell_violated)
        n_critical = sum(1 for e in quantum_events if e.is_critical)
        
        report += f"""- **Confirmed quantum events**: {n_quantum} ({100*n_quantum/len(quantum_events):.1f}%)
- **Bell violations**: {n_bell} ({100*n_bell/len(quantum_events):.1f}%)
- **Critical quantum events**: {n_critical}

### Event Type Distribution
"""
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
        event_types = Counter(e.event_type.value for e in quantum_events)
        for event_type, count in sorted(event_types.items()):
            report += f"- **{event_type}**: {count}\n"
        
        # åˆ¤å®šåŸºæº–çµ±è¨ˆ
        criterion_stats = {}
        for event in quantum_events:
            for criterion in event.quantum_metrics.criteria_passed:
                name = criterion.criterion.value
                if name not in criterion_stats:
                    criterion_stats[name] = {'passed': 0, 'total': 0, 'refs': set()}
                criterion_stats[name]['total'] += 1
                if criterion.passed:
                    criterion_stats[name]['passed'] += 1
                criterion_stats[name]['refs'].add(criterion.reference)
        
        if criterion_stats:
            report += """
### Validation Criteria Statistics
| Criterion | Pass Rate | References |
|-----------|-----------|------------|
"""
            for name, stats in sorted(criterion_stats.items()):
                rate = stats['passed'] / stats['total'] * 100 if stats['total'] > 0 else 0
                refs = list(stats['refs'])[0] if stats['refs'] else 'N/A'
                report += f"| {name} | {rate:.1f}% ({stats['passed']}/{stats['total']}) | {refs} |\n"
        
        # CHSHçµ±è¨ˆ
        chsh_values = [e.quantum_metrics.chsh_value 
                      for e in quantum_events 
                      if hasattr(e, 'quantum_metrics')]
        if chsh_values:
            report += f"""
### CHSH Statistics
- **Average CHSH value**: {np.mean(chsh_values):.3f}
- **Max CHSH value**: {np.max(chsh_values):.3f}
- **Min CHSH value**: {np.min(chsh_values):.3f}
- **Standard deviation**: {np.std(chsh_values):.3f}
- **Classical bound**: 2.000
- **Tsirelson bound**: {2*np.sqrt(2):.3f}

### Interpretation
"""
            if np.max(chsh_values) > 2.0:
                report += "âš›ï¸ **Quantum correlations detected**: CHSH values exceed classical bound\n"
            if np.max(chsh_values) > 2.4:
                report += "ğŸŒŸ **Strong quantum signatures**: Significant Bell inequality violations\n"
            if n_quantum > len(quantum_events) * 0.1:
                report += "ğŸ’« **Quantum prevalence**: >10% of events show quantum characteristics\n"
    
    # çµè«–
    report += f"""
## Conclusions

The LambdaÂ³ GPU v3.0 analysis of {system_name} successfully completed with the following key findings:

1. **Structural dynamics**: {len(lambda_result.critical_events)} critical events identified
"""
    
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        report += f"""2. **Residue interactions**: {stats.get('total_causal_links', 0)} causal relationships detected
3. **Async coupling**: {stats.get('total_async_bonds', 0)} async strong bonds identified
"""
    
    if quantum_events:
        n_quantum = sum(1 for e in quantum_events if e.quantum_metrics.is_quantum)
        n_bell = sum(1 for e in quantum_events if e.quantum_metrics.bell_violated)
        if n_bell > 0:
            report += f"""4. **Quantum signatures**: {n_bell} Bell violations confirm non-classical correlations
5. **Quantum events**: {n_quantum} events passed quantum validation criteria
"""
    
    report += """
## Recommendations

Based on the analysis results:
"""
    
    # æ¨å¥¨äº‹é …
    if two_stage_result and hasattr(two_stage_result, 'suggested_intervention_points'):
        points = two_stage_result.suggested_intervention_points[:3]
        if points:
            report += f"""
1. **Target residues for intervention**: Focus on residues {points} for potential drug targeting or mutation studies
"""
    
    if quantum_events and n_quantum > 0:
        report += """
2. **Quantum effects**: Consider quantum mechanical effects in protein dynamics modeling
3. **Non-classical correlations**: Account for Bell violations in theoretical models
"""
    
    report += """
---
*Generated by LambdaÂ³ GPU Quantum Validation Pipeline v3.0*
*Publication Ready - Peer Review Compatible*
*NO TIME, NO PHYSICS, ONLY STRUCTURE!*
"""
    
    return report


# ============================================
# CLI Interface
# ============================================

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    parser = argparse.ArgumentParser(
        description='LambdaÂ³ GPU Quantum Validation Pipeline v3.0 - Publication Ready',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic run with protein atoms
  %(prog)s trajectory.npy metadata.json protein.npy
  
  # With custom output directory
  %(prog)s trajectory.npy metadata.json protein.npy --output ./my_results
  
  # Skip visualizations for faster processing
  %(prog)s trajectory.npy metadata.json protein.npy --no-viz
  
  # Verbose mode for debugging
  %(prog)s trajectory.npy metadata.json protein.npy --verbose
        """
    )
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('trajectory', 
                       help='Path to trajectory file (.npy)')
    parser.add_argument('metadata', 
                       help='Path to metadata file (.json)')
    parser.add_argument('protein',
                       help='Path to protein indices file (.npy) - use protein.npy for detailed analysis')
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°
    parser.add_argument('--output', '-o', 
                       default='./quantum_results',
                       help='Output directory (default: ./quantum_results)')
    parser.add_argument('--no-two-stage', 
                       action='store_true',
                       help='Skip two-stage residue analysis')
    parser.add_argument('--no-viz', 
                       action='store_true',
                       help='Skip visualization')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='Verbose output for debugging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    try:
        results = run_quantum_validation_pipeline(
            trajectory_path=args.trajectory,
            metadata_path=args.metadata,
            protein_indices_path=args.protein,
            enable_two_stage=not args.no_two_stage,
            enable_visualization=not args.no_viz,
            output_dir=args.output,
            verbose=args.verbose
        )
        
        if results and results.get('success'):
            print(f"\nâœ… Success! Results saved to: {results['output_dir']}")
            return 0
        else:
            print("\nâŒ Pipeline failed. Check logs for details.")
            return 1
            
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
