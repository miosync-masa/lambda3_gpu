#!/usr/bin/env python3
"""
Lambda¬≥ GPU Quantum Validation Pipeline - Version 4.0
======================================================

Lambda¬≥Áµ±ÂêàÂûãÈáèÂ≠êËµ∑Ê∫êÂà§ÂÆö„Éë„Ç§„Éó„É©„Ç§„É≥
ÊßãÈÄ†Â§âÂåñ„ÅÆÈáèÂ≠êÊÄß„ÇíÊ≠£„Åó„ÅèË©ï‰æ°„Åô„ÇãÂÆåÂÖ®„É™„Éï„Ç°„ÇØ„Çø„É™„É≥„Ç∞Áâà

Version: 4.0.0 - Complete Refactoring
Authors: Lambda¬≥ Team, Áí∞„Å°„ÇÉ„Çì & „Åî‰∏ª‰∫∫„Åï„Åæ üíï
Date: 2024
"""

import numpy as np
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Lambda¬≥ GPU imports
try:
    from lambda3_gpu.analysis.md_lambda3_detector_gpu import (
        MDLambda3DetectorGPU, 
        MDLambda3Result,
        MDConfig
    )
    from lambda3_gpu.analysis.two_stage_analyzer_gpu import (
        TwoStageAnalyzerGPU,
        TwoStageLambda3Result,
        ResidueAnalysisConfig
    )
    # Third Impact Analytics
    from lambda3_gpu.analysis.third_impact_analytics import (
        run_third_impact_analysis,
        ThirdImpactAnalyzer,
        ThirdImpactResult
    )
    # Version 4.0 imports
    from lambda3_gpu.quantum import (
        QuantumValidatorV4,
        StructuralEventPattern,
        QuantumSignature,
        QuantumAssessment
    )
    from lambda3_gpu.visualization import Lambda3VisualizerGPU
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure lambda3_gpu is properly installed")
    raise
    
# LoggerË®≠ÂÆö
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('quantum_validation_v4')

# ============================================
# „É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞ÔºàVersion 4.0Ôºâ
# ============================================
def run_quantum_validation_pipeline(
    trajectory_path: str,
    metadata_path: str,
    protein_indices_path: str,
    topology_path: Optional[str] = None,
    enable_two_stage: bool = True,
    enable_third_impact: bool = True,
    enable_visualization: bool = True,
    output_dir: str = './quantum_results_v4',
    verbose: bool = False,
    atom_mapping_path: Optional[str] = None,  # ËøΩÂä†ÔºÅ
    third_impact_top_n: int = 10,            # ËøΩÂä†ÔºÅ
    **kwargs  # „Åù„ÅÆ‰ªñ„ÅÆ„Éë„É©„É°„Éº„ÇøÁî®
) -> Dict:
    """
    ÂÆåÂÖ®„Å™ÈáèÂ≠êÊ§úË®º„Éë„Ç§„Éó„É©„Ç§„É≥ÔºàVersion 4.0Ôºâ
    
    Parameters
    ----------
    trajectory_path : str
        „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ (.npy)
    metadata_path : str
        „É°„Çø„Éá„Éº„Çø„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ (.json)
    protein_indices_path : str
        „Çø„É≥„Éë„ÇØË≥™ÂéüÂ≠ê„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ (.npy)
    topology_path : str, optional
        „Éà„Éù„É≠„Ç∏„Éº„Éï„Ç°„Ç§„É´„ÅÆ„Éë„ÇπÔºàÁµêÂêàÊÉÖÂ†±„Å™„Å©Ôºâ
    enable_two_stage : bool
        2ÊÆµÈöéËß£ÊûêÔºàÊÆãÂü∫„É¨„Éô„É´Ôºâ„ÇíÂÆüË°å„Åô„Çã„Åã
    enable_visualization : bool
        ÂèØË¶ñÂåñ„ÇíÂÆüË°å„Åô„Çã„Åã
    output_dir : str
        Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™
    verbose : bool
        Ë©≥Á¥∞Âá∫Âäõ
        
    Returns
    -------
    Dict
        Ëß£ÊûêÁµêÊûú„ÅÆËæûÊõ∏
    """
    
    # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info("üöÄ LAMBDA¬≥ GPU QUANTUM VALIDATION PIPELINE v4.0")
    logger.info("   Lambda¬≥ Integrated Edition")
    logger.info("="*70)
    
    # ========================================
    # Step 1: „Éá„Éº„ÇøË™≠„ÅøËæº„Åø
    # ========================================
    logger.info("\nüìÅ Loading data...")
    
    try:
        # „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™Ë™≠„ÅøËæº„Åø
        trajectory = np.load(trajectory_path)
        logger.info(f"   Trajectory shape: {trajectory.shape}")
        n_frames, n_atoms, n_dims = trajectory.shape
        
        if n_dims != 3:
            raise ValueError(f"Trajectory must be 3D, got {n_dims}D")
        
        # „É°„Çø„Éá„Éº„ÇøË™≠„ÅøËæº„Åø
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        logger.info("   Metadata loaded successfully")
        
        # „Çø„É≥„Éë„ÇØË≥™„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπË™≠„ÅøËæº„Åø
        if not Path(protein_indices_path).exists():
            raise FileNotFoundError(f"Protein indices file not found: {protein_indices_path}")
            
        protein_indices = np.load(protein_indices_path)
        logger.info(f"   Protein indices loaded: {len(protein_indices)} atoms")
        
        # „Éà„Éù„É≠„Ç∏„ÉºË™≠„ÅøËæº„ÅøÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
        topology = None
        if topology_path and Path(topology_path).exists():
            # „Éà„Éù„É≠„Ç∏„ÉºÂΩ¢Âºè„Å´Âøú„Åò„Å¶Ë™≠„ÅøËæº„Åø
            # „Åì„Åì„Åß„ÅØÁ∞°Áï•Âåñ
            logger.info(f"   Topology loaded from {topology_path}")
        
        # „Çø„É≥„Éë„ÇØË≥™ÊÉÖÂ†±„ÅÆË°®Á§∫
        if 'protein' in metadata:
            protein_info = metadata['protein']
            logger.info(f"   Protein: {protein_info.get('n_residues', 'N/A')} residues")
            logger.info(f"   Sequence: {protein_info.get('sequence', 'N/A')[:20]}...")
        
        logger.info(f"   ‚úÖ Data validation passed")
            
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise
    
    # ========================================
    # Step 2: Lambda¬≥ GPUËß£Êûê
    # ========================================
    logger.info("\nüî¨ Running Lambda¬≥ GPU Analysis...")
    
    try:
        # MDConfigË®≠ÂÆö
        config = MDConfig()
        config.use_extended_detection = True
        config.use_phase_space = True
        config.use_periodic = True
        config.use_gradual = True
        config.use_drift = True
        config.adaptive_window = True
        config.window_scale = 0.005
        config.gpu_batch_size = 30000
        config.verbose = verbose
        config.sensitivity = 2.0
        config.use_hierarchical = True
        
        logger.info("   Config optimized for structure detection")
        
        # Lambda¬≥Ê§úÂá∫Âô®ÂàùÊúüÂåñ
        detector = MDLambda3DetectorGPU(config)
        logger.info("   Detector initialized on GPU")
        
        # Ëß£ÊûêÂÆüË°å
        lambda_result = detector.analyze(trajectory, protein_indices)
        
        logger.info(f"   ‚úÖ Lambda¬≥ analysis complete")
        logger.info(f"   Critical events detected: {len(lambda_result.critical_events)}")
        
        # ÁµêÊûú„ÅÆ‰øùÂ≠ò
        result_summary = {
            'n_frames': lambda_result.n_frames,
            'n_atoms': lambda_result.n_atoms,
            'n_protein_atoms': len(protein_indices),
            'n_critical_events': len(lambda_result.critical_events),
            'computation_time': lambda_result.computation_time,
            'analysis_version': '4.0.0'
        }
        
        with open(output_path / 'lambda_result_summary.json', 'w') as f:
            json.dump(result_summary, f, indent=2)
            
    except Exception as e:
        logger.error(f"Lambda¬≥ analysis failed: {e}")
        raise
  
    # ========================================
    # Step 3: Two-StageË©≥Á¥∞Ëß£Êûê
    # ========================================
    two_stage_result = None
    network_results = []
    
    if enable_two_stage and len(lambda_result.critical_events) > 0:
        logger.info("\nüî¨ Running Two-Stage Residue-Level Analysis...")
        
        try:
            # „Çø„É≥„Éë„ÇØË≥™ÊÆãÂü∫Êï∞„ÅÆÂèñÂæó
            n_protein_residues = metadata.get('protein', {}).get('n_residues', 10)
            logger.info(f"   Protein residues: {n_protein_residues}")
            
            # „Çø„É≥„Éë„ÇØË≥™ÈÉ®ÂàÜ„ÅÆ„Éà„É©„Ç∏„Çß„ÇØ„Éà„É™
            protein_trajectory = trajectory[:, protein_indices, :]
            
            # TOP50„Ç§„Éô„É≥„ÉàÈÅ∏Êäû
            MAX_EVENTS = 50
            MIN_WINDOW_SIZE = 10
            
            # „Ç§„Éô„É≥„Éà„ÅÆ„Çπ„Ç≥„Ç¢‰ªò„Åç„ÇΩ„Éº„Éà
            sorted_events = []
            for event in lambda_result.critical_events:
                if isinstance(event, dict):
                    score = event.get('anomaly_score', 0)
                    start = event.get('start', event.get('frame', 0))
                    end = event.get('end', start)
                elif isinstance(event, (tuple, list)) and len(event) >= 2:
                    start = int(event[0])
                    end = int(event[1])
                    
                    # anomaly_scores„Åã„ÇâÂÆüÈöõ„ÅÆ„Çπ„Ç≥„Ç¢„ÇíÂèñÂæóÔºàÊúÄÂ§ßÂÄ§Ôºâ
                    if len(event) > 2:
                        score = event[2]  # Êó¢„Å´„Çπ„Ç≥„Ç¢„Åå„ÅÇ„ÇãÂ†¥Âêà
                    else:
                        # anomaly_scores„Åã„ÇâË©≤ÂΩìÁØÑÂõ≤„ÅÆÊúÄÂ§ßÂÄ§„ÇíÂèñÂæó
                        if 'combined' in lambda_result.anomaly_scores:
                            score = float(np.max(lambda_result.anomaly_scores['combined'][start:end+1]))
                        elif 'final_combined' in lambda_result.anomaly_scores:
                            score = float(np.max(lambda_result.anomaly_scores['final_combined'][start:end+1]))
                        elif 'global' in lambda_result.anomaly_scores:
                            score = float(np.max(lambda_result.anomaly_scores['global'][start:end+1]))
                        else:
                            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
                            for key in ['local', 'extended']:
                                if key in lambda_result.anomaly_scores:
                                    score = float(np.max(lambda_result.anomaly_scores[key][start:end+1]))
                                    break
                            else:
                                score = 0.0
                                logger.warning(f"   ‚ö†Ô∏è No anomaly scores found for event at frames {start}-{end}")
                else:
                    continue
                sorted_events.append((start, end, score))
            
            # „Çπ„Ç≥„Ç¢„Åß„ÇΩ„Éº„ÉàÔºàÈôçÈ†ÜÔºâ
            sorted_events.sort(key=lambda x: x[2], reverse=True)
            selected_events = sorted_events[:min(MAX_EVENTS, len(sorted_events))]
            logger.info(f"   Selected TOP {len(selected_events)} events")
            
            # „Éá„Éê„ÉÉ„Ç∞Ôºö‰∏ä‰Ωç„Ç§„Éô„É≥„Éà„ÅÆ„Çπ„Ç≥„Ç¢„ÇíË°®Á§∫
            if verbose:
                logger.debug("   Top event scores:")
                for i, (start, end, score) in enumerate(selected_events[:10]):
                    logger.debug(f"     Event {i}: frames {start}-{end}, score={score:.3f}")
            
            # =========== ‰øÆÊ≠£ÈÉ®ÂàÜÔºÅÔºÅ ===========
            # detected_events„Çítop_XX_score_Y.YYÂΩ¢Âºè„Å´Â§âÊèõ
            detected_events = []
            for i, (start, end, score) in enumerate(selected_events):
                # maximum_report_generator„ÅåÊúüÂæÖ„Åô„ÇãÂΩ¢Âºè
                event_name = f"top_{i:02d}_score_{score:.3f}"
                detected_events.append((start, end, event_name))
            
            # TwoStageAnalyzer„ÅÆË®≠ÂÆö
            residue_config = ResidueAnalysisConfig()
            residue_config.n_residues = n_protein_residues
            residue_config.min_window_size = MIN_WINDOW_SIZE
            residue_config.use_gpu = True
            residue_config.verbose = verbose
            
            # TwoStageAnalyzerÂàùÊúüÂåñ
            logger.info("   Initializing Two-Stage Analyzer...")
            two_stage_analyzer = TwoStageAnalyzerGPU(residue_config)
            
            # Ëß£ÊûêÂÆüË°åÔºÅÔºàdetected_events„ÅØÂêçÂâç‰ªò„Åç„Å´„Å™„Å£„ÅüÔºÅÔºâ
            logger.info(f"   Analyzing {len(detected_events)} events...")
            two_stage_result = two_stage_analyzer.analyze_trajectory(
                trajectory=protein_trajectory,
                macro_result=lambda_result,
                detected_events=detected_events,  # top_XX_score_Y.YYÂΩ¢ÂºèÔºÅ
                n_residues=n_protein_residues
            )
            
            logger.info(f"   ‚úÖ Two-stage analysis complete")
            
            # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁµêÊûú„ÅÆÊäΩÂá∫
            if hasattr(two_stage_result, 'residue_analyses'):
                network_results = []
                for analysis in two_stage_result.residue_analyses.values():
                    if hasattr(analysis, 'network_result'):
                        network_results.append(analysis.network_result)
                logger.info(f"   Extracted {len(network_results)} network results")
            
            # „Ç∞„É≠„Éº„Éê„É´Áµ±Ë®à„ÅÆË°®Á§∫
            if hasattr(two_stage_result, 'global_network_stats'):
                stats = two_stage_result.global_network_stats
                logger.info(f"   Total causal links: {stats.get('total_causal_links', 0)}")
                logger.info(f"   Total async bonds: {stats.get('total_async_bonds', 0)}")
            
        except Exception as e:
            logger.error(f"Two-stage analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            two_stage_result = None
            network_results = []
          
    # ========================================
    # Step 4: ÈáèÂ≠êÊ§úË®ºÔºàVersion 4.0Ôºâ
    # ========================================
    logger.info("\n‚öõÔ∏è Running Quantum Validation v4.0...")
    
    quantum_assessments = []
    
    try:
        # Áâ©ÁêÜ„Éë„É©„É°„Éº„ÇøÂèñÂæó
        temperature = metadata.get('temperature', 300.0)
        dt_ps = metadata.get('time_step_ps', 100.0)
        
        # ÈáèÂ≠êÊ§úË®ºÂô®ÂàùÊúüÂåñÔºàVersion 4.0Ôºâ
        quantum_validator = QuantumValidatorV4(
            trajectory=trajectory[:, protein_indices, :],
            topology=topology,
            dt_ps=dt_ps,
            temperature_K=temperature
        )
        
        logger.info(f"   Quantum validator v4.0 initialized")
        logger.info(f"   Temperature: {temperature:.1f} K")
        logger.info(f"   Time step: {dt_ps:.1f} ps")
        
        # Lambda¬≥„Ç§„Éô„É≥„Éà„ÇíÂ§âÊèõ
        events = []
        for e in lambda_result.critical_events[:100]:  # ÊúÄÂ§ß100„Ç§„Éô„É≥„Éà
            if isinstance(e, (tuple, list)) and len(e) >= 2:
                events.append({
                    'frame_start': int(e[0]),
                    'frame_end': int(e[1]),
                    'type': 'critical'
                })
            elif isinstance(e, dict):
                events.append({
                    'frame_start': e.get('start', e.get('frame', 0)),
                    'frame_end': e.get('end', e.get('frame', 0)),
                    'type': e.get('type', 'critical')
                })
        
        if events:
            # „Éê„ÉÉ„ÉÅÊ§úË®º
            quantum_assessments = quantum_validator.validate_events(
                events,
                lambda_result,
                network_results if network_results else None
            )
            
            logger.info(f"   ‚úÖ Quantum validation complete")
            logger.info(f"   Events analyzed: {len(quantum_assessments)}")
            
            # Áµ±Ë®àË°®Á§∫
            quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
            logger.info(f"   Quantum events: {quantum_count}/{len(quantum_assessments)}")
            
            # „Éë„Çø„Éº„É≥Âà•Áµ±Ë®à
            pattern_counts = Counter(a.pattern.value for a in quantum_assessments)
            logger.info("\n   üìä Pattern Distribution:")
            for pattern, count in pattern_counts.items():
                logger.info(f"     {pattern}: {count}")
            
            # „Ç∑„Ç∞„Éç„ÉÅ„É£„ÉºÂà•Áµ±Ë®à
            sig_counts = Counter(a.signature.value for a in quantum_assessments 
                               if a.signature != QuantumSignature.NONE)
            if sig_counts:
                logger.info("\n   ‚öõÔ∏è Quantum Signatures:")
                for sig, count in sig_counts.items():
                    logger.info(f"     {sig}: {count}")
            
            # ‰ø°È†ºÂ∫¶Áµ±Ë®à
            confidences = [a.confidence for a in quantum_assessments if a.is_quantum]
            if confidences:
                logger.info(f"\n   üìà Confidence Statistics:")
                logger.info(f"     Mean: {np.mean(confidences):.3f}")
                logger.info(f"     Max: {np.max(confidences):.3f}")
                logger.info(f"     Min: {np.min(confidences):.3f}")
            
            # „Çµ„Éû„É™„ÉºË°®Á§∫
            quantum_validator.print_summary(quantum_assessments)
            
            # ÁµêÊûú‰øùÂ≠ò
            save_quantum_assessments_v4(quantum_assessments, output_path, metadata)
            
        else:
            logger.warning("   No events to validate")
            
    except Exception as e:
        logger.error(f"Quantum validation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
    
    # ========================================
    # Step 4.5: Third Impact AnalysisÔºàv3.0ÂØæÂøúÔºâ
    # ========================================
    third_impact_results = None
    
    if enable_third_impact and two_stage_result is not None:
        logger.info("\nüî∫ Running Third Impact Analysis v3.0...")
        
        try:
            # atom_mapping„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„ÅøÔºàNEW!Ôºâ
            atom_mapping = None
            atom_mapping_path = kwargs.get('atom_mapping_path')
            
            if atom_mapping_path and Path(atom_mapping_path).exists():
                if atom_mapping_path.endswith('.json'):
                    with open(atom_mapping_path, 'r') as f:
                        raw_mapping = json.load(f)
                        # ÊñáÂ≠óÂàó„Ç≠„Éº„ÇíÊï¥Êï∞„Å´Â§âÊèõ
                        atom_mapping = {int(k): v for k, v in raw_mapping.items()}
                elif atom_mapping_path.endswith('.npy'):
                    # NumPyÂΩ¢Âºè„ÅÆÂ†¥Âêà
                    atom_mapping = np.load(atom_mapping_path, allow_pickle=True).item()
                logger.info(f"   Atom mapping loaded: {len(atom_mapping)} residues")
            else:
                logger.warning("   ‚ö†Ô∏è No atom mapping provided, using fallback (15 atoms/residue)")
            
            # Third ImpactËß£ÊûêÂÆüË°åÔºàv3.0„Éë„É©„É°„Éº„ÇøÔºâ
            third_impact_results = run_third_impact_analysis(
                lambda_result=lambda_result,
                two_stage_result=two_stage_result,
                trajectory=trajectory[:, protein_indices, :],
                residue_mapping=atom_mapping,  # v3.0: residue_mapping
                output_dir=output_path / 'third_impact',
                use_network_analysis=True,     # v3.0: ÂéüÂ≠ê„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØËß£ÊûêON
                use_gpu=True,                   # GPUÂä†ÈÄü
                top_n=kwargs.get('third_impact_top_n', 10),  # „Ç´„Çπ„Çø„Éû„Ç§„Ç∫ÂèØËÉΩ
                sigma_threshold=kwargs.get('sigma_threshold', 3.0)
            )
            
            # Áµ±Ë®àË°®Á§∫Ôºàv3.0„ÅÆÊñ∞„Åó„ÅÑÊßãÈÄ†„Å´ÂØæÂøúÔºâ
            if third_impact_results:
                # v3.0: origin.genesis_atoms„Çí‰ΩøÁî®
                total_genesis = sum(len(r.origin.genesis_atoms) for r in third_impact_results.values())
                total_quantum_atoms = sum(r.n_quantum_atoms for r in third_impact_results.values())
                
                # v3.0: „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁµ±Ë®à„ÇÇËøΩÂä†
                total_network_links = sum(r.n_network_links for r in third_impact_results.values())
                total_bridges = sum(r.n_residue_bridges for r in third_impact_results.values())
                
                logger.info(f"   ‚úÖ Third Impact v3.0 analysis complete")
                logger.info(f"   Genesis atoms identified: {total_genesis}")
                logger.info(f"   Total quantum atoms: {total_quantum_atoms}")
                logger.info(f"   Network links: {total_network_links}")
                logger.info(f"   Residue bridges: {total_bridges}")
                
                # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Éë„Çø„Éº„É≥„ÅÆÂàÜÂ∏É
                patterns = [r.atomic_network.network_pattern 
                           for r in third_impact_results.values() 
                           if r.atomic_network]
                if patterns:
                    from collections import Counter
                    pattern_counts = Counter(patterns)
                    logger.info(f"   Network patterns: {dict(pattern_counts)}")
                
                # ÂâµËñ¨„Çø„Éº„Ç≤„ÉÉ„ÉàË°®Á§∫Ôºàv3.0: bridge_target_atoms„ÇÇËøΩÂä†Ôºâ
                drug_targets = []
                bridge_targets = []
                
                for result in third_impact_results.values():
                    drug_targets.extend(result.drug_target_atoms)
                    if hasattr(result, 'bridge_target_atoms'):
                        bridge_targets.extend(result.bridge_target_atoms)
                
                if drug_targets:
                    unique_drug_targets = list(set(drug_targets[:10]))
                    logger.info(f"   Drug target atoms: {unique_drug_targets}")
                
                if bridge_targets:
                    unique_bridge_targets = list(set(bridge_targets[:10]))
                    logger.info(f"   Bridge target atoms: {unique_bridge_targets}")
                
                # „Éè„ÉñÂéüÂ≠ê„ÅÆË°®Á§∫Ôºàv3.0Êñ∞Ê©üËÉΩÔºâ
                hub_atoms = []
                for result in third_impact_results.values():
                    if result.atomic_network and result.atomic_network.hub_atoms:
                        hub_atoms.extend(result.atomic_network.hub_atoms[:3])
                
                if hub_atoms:
                    unique_hubs = list(set(hub_atoms[:10]))
                    logger.info(f"   Hub atoms (network centers): {unique_hubs}")
            
        except Exception as e:
            logger.error(f"Third Impact v3.0 analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()

    # ========================================
    # Step 5: ÂèØË¶ñÂåñÔºàVersion 4.0ÂØæÂøúÔºâ
    # ========================================
    if enable_visualization:
        logger.info("\nüìä Creating visualizations...")
        
        try:
            visualizer = Lambda3VisualizerGPU()
            
            # Lambda¬≥ÁµêÊûú„ÅÆÂèØË¶ñÂåñ
            fig_lambda = visualizer.visualize_results(
                lambda_result,
                save_path=str(output_path / 'lambda_results.png')
            )
            logger.info("   Lambda¬≥ results visualized")
            
            # ÈáèÂ≠êË©ï‰æ°„ÅÆÂèØË¶ñÂåñ
            if quantum_assessments:
                fig_quantum = visualize_quantum_assessments_v4(
                    quantum_assessments,
                    save_path=str(output_path / 'quantum_assessments.png')
                )
                logger.info("   Quantum assessments visualized")
            
            # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂèØË¶ñÂåñ
            if two_stage_result:
                fig_network = visualize_residue_network(
                    two_stage_result,
                    save_path=str(output_path / 'residue_network.png')
                )
                logger.info("   Residue network visualized")
            
            logger.info("   ‚úÖ All visualizations saved")
            
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
    
    # ========================================
    # Step 6: ÂåÖÊã¨ÁöÑ„É¨„Éù„Éº„ÉàÁîüÊàêÔºàVersion 4.0Ôºâ
    # ========================================
    logger.info("\nüìù Generating comprehensive report...")
    
    report = generate_comprehensive_report_v4(
        lambda_result,
        quantum_assessments,
        two_stage_result,
        metadata,
        protein_indices
    )
    
    report_path = output_path / 'analysis_report_v4.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    logger.info(f"   Report saved to {report_path}")
    
    # ========================================
    # Step 7: Maximum Report Generation
    # ========================================
    try:
        logger.info("\nüìö Generating maximum report...")
        
        # maximum_report_generator„ÅÆ„Ç§„É≥„Éù„Éº„ÉàË©¶Ë°å
        try:
            from .maximum_report_generator import generate_maximum_report_from_results_v4
        except ImportError:
            # Áõ∏ÂØæ„Ç§„É≥„Éù„Éº„Éà„ÅåÂ§±Êïó„Åó„ÅüÂ†¥Âêà
            from lambda3_gpu.analysis.maximum_report_generator import generate_maximum_report_from_results_v4
        
        # Version 4.0ÂØæÂøú„ÅÆÂëº„Å≥Âá∫„Åó
        max_report = generate_maximum_report_from_results_v4(
            lambda_result=lambda_result,
            sorted_events=sorted_events,  
            two_stage_result=two_stage_result if enable_two_stage else None,
            quantum_assessments=quantum_assessments,  # v4.0: quantum_events„Åß„ÅØ„Å™„Åèassessments
            metadata=metadata,
            output_dir=str(output_path),
            verbose=verbose
        )
        logger.info(f"   ‚úÖ Maximum report generated: {len(max_report):,} characters")
        
    except ImportError as e:
        logger.warning(f"   ‚ö†Ô∏è Maximum report generator not found: {e}")
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Maximum report generation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
    
    # ========================================
    # ÂÆå‰∫Ü
    # ========================================
    logger.info("\n" + "="*70)
    logger.info("‚úÖ PIPELINE COMPLETE!")
    logger.info(f"   Results directory: {output_path}")
    logger.info(f"   Key findings:")
    
    if lambda_result:
        logger.info(f"     - {len(lambda_result.critical_events)} critical events")
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        logger.info(f"     - {two_stage_result.global_network_stats.get('total_causal_links', 0)} causal links")
    if quantum_assessments:
        quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
        logger.info(f"     - {quantum_count} quantum events confirmed")
    
    logger.info("="*70)
    
    return {
        'lambda_result': lambda_result,
        'quantum_assessments': quantum_assessments,
        'two_stage_result': two_stage_result,
        'output_dir': output_path,
        'success': True
    }

# ============================================
# Ë£úÂä©Èñ¢Êï∞Áæ§ÔºàVersion 4.0Ôºâ
# ============================================

def save_quantum_assessments_v4(assessments: List[QuantumAssessment], 
                               output_path: Path, metadata: Dict):
    """ÈáèÂ≠êË©ï‰æ°ÁµêÊûú„ÅÆ‰øùÂ≠òÔºàVersion 4.0Ôºâ"""
    
    assessment_data = []
    
    for assessment in assessments:
        data = {
            'pattern': assessment.pattern.value,
            'signature': assessment.signature.value,
            'is_quantum': assessment.is_quantum,
            'confidence': float(assessment.confidence),
            'explanation': assessment.explanation,
            'criteria_met': assessment.criteria_met
        }
        
        # LambdaÁï∞Â∏∏ÊÄß
        if assessment.lambda_anomaly:
            data['lambda_anomaly'] = {
                'lambda_jump': float(assessment.lambda_anomaly.lambda_jump),
                'lambda_zscore': float(assessment.lambda_anomaly.lambda_zscore),
                'rho_t_spike': float(assessment.lambda_anomaly.rho_t_spike),
                'sigma_s_value': float(assessment.lambda_anomaly.sigma_s_value)
            }
        
        # ÂéüÂ≠ê„É¨„Éô„É´Ë®ºÊã†
        if assessment.atomic_evidence:
            data['atomic_evidence'] = {
                'max_velocity': float(assessment.atomic_evidence.max_velocity),
                'max_acceleration': float(assessment.atomic_evidence.max_acceleration),
                'correlation_coefficient': float(assessment.atomic_evidence.correlation_coefficient),
                'n_bond_anomalies': len(assessment.atomic_evidence.bond_anomalies),
                'n_dihedral_flips': len(assessment.atomic_evidence.dihedral_flips)
            }
        
        # Bell‰∏çÁ≠âÂºèÔºà„Ç´„Çπ„Ç±„Éº„Éâ„ÅÆÂ†¥ÂêàÔºâ
        if assessment.bell_inequality is not None:
            data['bell_inequality'] = float(assessment.bell_inequality)
        
        # async bondsÔºà„Ç´„Çπ„Ç±„Éº„Éâ„ÅÆÂ†¥ÂêàÔºâ
        if assessment.async_bonds_used:
            data['async_bonds'] = assessment.async_bonds_used[:5]
        
        assessment_data.append(data)
    
    # „Çµ„Éû„É™„ÉºÁµ±Ë®à
    total = len(assessments)
    quantum_count = sum(1 for a in assessments if a.is_quantum)
    
    output_data = {
        'metadata': {
            'system_name': metadata.get('system_name', 'Unknown'),
            'temperature_K': metadata.get('temperature', 300.0),
            'dt_ps': metadata.get('time_step_ps', 100.0),
            'analysis_version': '4.0.0'
        },
        'summary': {
            'total_events': total,
            'quantum_events': quantum_count,
            'quantum_ratio': quantum_count / total if total > 0 else 0,
            'pattern_distribution': dict(Counter(a.pattern.value for a in assessments)),
            'signature_distribution': dict(Counter(a.signature.value for a in assessments 
                                                  if a.signature != QuantumSignature.NONE))
        },
        'assessments': assessment_data
    }
    
    with open(output_path / 'quantum_assessments_v4.json', 'w') as f:
        json.dump(output_data, f, indent=2)
    
    logger.info(f"   üíæ Saved {len(assessment_data)} quantum assessments")


def visualize_quantum_assessments_v4(assessments: List[QuantumAssessment],
                                    save_path: Optional[str] = None) -> plt.Figure:
    """ÈáèÂ≠êË©ï‰æ°ÁµêÊûú„ÅÆÂèØË¶ñÂåñÔºàVersion 4.0Ôºâ"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    if not assessments:
        fig.text(0.5, 0.5, 'No Assessments Available', 
                ha='center', va='center', fontsize=20)
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        return fig
    
    # 1. „Éë„Çø„Éº„É≥ÂàÜÂ∏É
    ax1 = axes[0, 0]
    pattern_counts = Counter(a.pattern.value for a in assessments)
    if pattern_counts:
        ax1.pie(pattern_counts.values(), 
               labels=pattern_counts.keys(),
               autopct='%1.1f%%',
               startangle=90)
        ax1.set_title('Event Pattern Distribution')
    
    # 2. „Ç∑„Ç∞„Éç„ÉÅ„É£„ÉºÂàÜÂ∏É
    ax2 = axes[0, 1]
    sig_counts = Counter(a.signature.value for a in assessments)
    # CLASSICAL„ÇíÈô§Â§ñ
    sig_counts.pop('classical', None)
    if sig_counts:
        ax2.pie(sig_counts.values(),
               labels=sig_counts.keys(),
               autopct='%1.1f%%',
               startangle=90)
        ax2.set_title('Quantum Signature Distribution')
    
    # 3. ÈáèÂ≠êÂà§ÂÆöÁéá
    ax3 = axes[0, 2]
    n_quantum = sum(1 for a in assessments if a.is_quantum)
    n_classical = len(assessments) - n_quantum
    ax3.pie([n_quantum, n_classical],
           labels=[f'Quantum ({n_quantum})', f'Classical ({n_classical})'],
           colors=['red', 'blue'],
           autopct='%1.1f%%',
           startangle=90)
    ax3.set_title('Quantum vs Classical')
    
    # 4. ‰ø°È†ºÂ∫¶ÂàÜÂ∏É
    ax4 = axes[1, 0]
    confidences = [a.confidence for a in assessments if a.is_quantum]
    if confidences:
        ax4.hist(confidences, bins=20, alpha=0.7, color='purple', edgecolor='black')
        ax4.set_xlabel('Confidence')
        ax4.set_ylabel('Count')
        ax4.set_title('Quantum Confidence Distribution')
        ax4.grid(True, alpha=0.3)
    
    # 5. LambdaÁï∞Â∏∏ÊÄß
    ax5 = axes[1, 1]
    lambda_zscores = [a.lambda_anomaly.lambda_zscore for a in assessments 
                      if a.lambda_anomaly and a.lambda_anomaly.lambda_zscore > 0]
    if lambda_zscores:
        ax5.hist(lambda_zscores, bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax5.axvline(x=3.0, color='red', linestyle='--', label='3œÉ threshold')
        ax5.set_xlabel('Lambda Z-score')
        ax5.set_ylabel('Count')
        ax5.set_title('Lambda Anomaly Distribution')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
    
    # 6. „Éë„Çø„Éº„É≥Âà•ÈáèÂ≠êÂà§ÂÆöÁéá
    ax6 = axes[1, 2]
    pattern_quantum = {}
    for pattern in StructuralEventPattern:
        pattern_assessments = [a for a in assessments if a.pattern == pattern]
        if pattern_assessments:
            quantum_ratio = sum(1 for a in pattern_assessments if a.is_quantum) / len(pattern_assessments)
            pattern_quantum[pattern.value] = quantum_ratio * 100
    
    if pattern_quantum:
        ax6.bar(range(len(pattern_quantum)), list(pattern_quantum.values()), 
               alpha=0.7, color='steelblue')
        ax6.set_xticks(range(len(pattern_quantum)))
        ax6.set_xticklabels(list(pattern_quantum.keys()), rotation=45, ha='right')
        ax6.set_ylabel('Quantum Rate (%)')
        ax6.set_title('Quantum Rate by Pattern')
        ax6.grid(True, alpha=0.3)
    
    plt.suptitle('Quantum Assessment Results v4.0', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def visualize_residue_network(two_stage_result: Any,
                             save_path: Optional[str] = None) -> plt.Figure:
    """ÊÆãÂü∫„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂèØË¶ñÂåñ"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. ÊÆãÂü∫ÈáçË¶ÅÂ∫¶„É©„É≥„Ç≠„É≥„Ç∞
    ax1 = axes[0, 0]
    if hasattr(two_stage_result, 'global_residue_importance'):
        top_residues = sorted(
            two_stage_result.global_residue_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        if top_residues:
            residue_ids = [f"R{r[0]+1}" for r in top_residues]
            scores = [r[1] for r in top_residues]
            
            ax1.barh(residue_ids, scores, alpha=0.7, color='steelblue')
            ax1.set_xlabel('Importance Score')
            ax1.set_title('Top 20 Important Residues')
            ax1.invert_yaxis()
    
    # 2. „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁµ±Ë®à
    ax2 = axes[0, 1]
    if hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        labels = ['Causal', 'Sync', 'Async']
        values = [
            stats.get('total_causal_links', 0),
            stats.get('total_sync_links', 0),
            stats.get('total_async_bonds', 0)
        ]
        
        if sum(values) > 0:
            ax2.pie(values, labels=labels, autopct='%1.1f%%',
                   startangle=90, colors=['red', 'green', 'blue'])
            ax2.set_title('Network Link Distribution')
    
    # 3. „Ç§„Éô„É≥„ÉàÂà•Áµ±Ë®à
    ax3 = axes[1, 0]
    if hasattr(two_stage_result, 'residue_analyses'):
        event_names = list(two_stage_result.residue_analyses.keys())
        n_residues = []
        
        for analysis in two_stage_result.residue_analyses.values():
            if hasattr(analysis, 'residue_events'):
                n_residues.append(len(analysis.residue_events))
            else:
                n_residues.append(0)
        
        if event_names and n_residues:
            # „Ç§„Éô„É≥„ÉàÂêç„ÅåÈï∑„ÅÑÂ†¥Âêà„ÅØÁü≠Á∏Æ
            short_names = []
            for name in event_names:
                if len(name) > 10:
                    # top_XX_score_Y.YYÂΩ¢Âºè„Åã„ÇâÁï™Âè∑„Å†„ÅëÊäΩÂá∫
                    import re
                    match = re.search(r'top_(\d+)', name)
                    if match:
                        short_names.append(f"E{match.group(1)}")
                    else:
                        short_names.append(name[:10])
                else:
                    short_names.append(name)
            
            ax3.bar(range(len(event_names)), n_residues, alpha=0.7, color='orange')
            ax3.set_xticks(range(len(event_names)))
            ax3.set_xticklabels(short_names, rotation=45, ha='right')
            ax3.set_ylabel('Number of Residues Involved')
            ax3.set_title('Residues per Event')
            ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. GPUÊÄßËÉΩ
    ax4 = axes[1, 1]
    if hasattr(two_stage_result, 'residue_analyses'):
        gpu_times = []
        event_names = list(two_stage_result.residue_analyses.keys())
        
        for analysis in two_stage_result.residue_analyses.values():
            if hasattr(analysis, 'gpu_time'):
                gpu_times.append(analysis.gpu_time)
            else:
                gpu_times.append(0)
        
        if gpu_times and event_names:
            # „Ç§„Éô„É≥„ÉàÂêçÁü≠Á∏ÆÔºà‰∏ä„Å®Âêå„ÅòÔºâ
            short_names = []
            for name in event_names:
                if len(name) > 10:
                    import re
                    match = re.search(r'top_(\d+)', name)
                    if match:
                        short_names.append(f"E{match.group(1)}")
                    else:
                        short_names.append(name[:10])
                else:
                    short_names.append(name)
            
            ax4.plot(range(len(event_names)), gpu_times, 'go-', 
                    linewidth=2, markersize=8, alpha=0.7)
            ax4.set_xticks(range(len(event_names)))
            ax4.set_xticklabels(short_names, rotation=45, ha='right')
            ax4.set_ylabel('GPU Time (seconds)')
            ax4.set_title('GPU Processing Time per Event')
            ax4.grid(True, alpha=0.3)
            
            # Âπ≥ÂùáÊôÇÈñì„ÇíË°®Á§∫
            if gpu_times:
                avg_time = np.mean(gpu_times)
                ax4.axhline(y=avg_time, color='red', linestyle='--', 
                           alpha=0.5, label=f'Avg: {avg_time:.2f}s')
                ax4.legend()
    
    plt.suptitle('Residue-Level Analysis Summary', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def generate_comprehensive_report_v4(
    lambda_result: Any,
    quantum_assessments: List[QuantumAssessment],
    two_stage_result: Optional[Any],
    metadata: Dict,
    protein_indices: np.ndarray
) -> str:
    """ÂåÖÊã¨ÁöÑ„Å™Ëß£Êûê„É¨„Éù„Éº„Éà„ÅÆÁîüÊàêÔºàVersion 4.0Ôºâ"""
    
    system_name = metadata.get('system_name', 'Unknown System')
    
    report = f"""# Lambda¬≥ GPU Analysis Report v4.0

## System Information
- **System**: {system_name}
- **Temperature**: {metadata.get('temperature', 300)} K
- **Time step**: {metadata.get('time_step_ps', 100)} ps
- **Frames analyzed**: {lambda_result.n_frames}
- **Total atoms**: {lambda_result.n_atoms}
- **Protein atoms**: {len(protein_indices)}
- **Analysis version**: 4.0.0 (Lambda¬≥ Integrated)

## Lambda¬≥ Analysis Results
- **Critical events**: {len(lambda_result.critical_events)}
- **Computation time**: {lambda_result.computation_time:.2f} seconds
"""
    
    # Two-StageËß£ÊûêÁµêÊûú
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        report += f"""
## Residue Network Analysis
- **Total causal links**: {stats.get('total_causal_links', 0)}
- **Total async bonds**: {stats.get('total_async_bonds', 0)}
- **Async/Causal ratio**: {stats.get('async_to_causal_ratio', 0):.2%}
"""
    
    # ÈáèÂ≠êÊ§úË®ºÁµêÊûúÔºàVersion 4.0Ôºâ
    if quantum_assessments:
        total = len(quantum_assessments)
        quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
        
        report += f"""
## Quantum Validation Results v4.0

### Overview
- **Total events analyzed**: {total}
- **Quantum events confirmed**: {quantum_count} ({quantum_count/total*100:.1f}%)

### Pattern Analysis
"""
        # „Éë„Çø„Éº„É≥Âà•Áµ±Ë®à
        for pattern in StructuralEventPattern:
            pattern_assessments = [a for a in quantum_assessments if a.pattern == pattern]
            if pattern_assessments:
                n_pattern = len(pattern_assessments)
                n_quantum = sum(1 for a in pattern_assessments if a.is_quantum)
                report += f"- **{pattern.value}**: {n_quantum}/{n_pattern} quantum ({n_quantum/n_pattern*100:.1f}%)\n"
        
        # „Ç∑„Ç∞„Éç„ÉÅ„É£„ÉºÂàÜÂ∏É
        sig_counts = Counter(a.signature.value for a in quantum_assessments 
                           if a.signature != QuantumSignature.NONE)
        if sig_counts:
            report += f"""
### Quantum Signatures Detected
"""
            for sig, count in sorted(sig_counts.items()):
                report += f"- **{sig}**: {count}\n"
        
        # ‰ø°È†ºÂ∫¶Áµ±Ë®à
        confidences = [a.confidence for a in quantum_assessments if a.is_quantum]
        if confidences:
            report += f"""
### Confidence Statistics
- **Mean confidence**: {np.mean(confidences):.3f}
- **Max confidence**: {np.max(confidences):.3f}
- **Min confidence**: {np.min(confidences):.3f}
"""
        
        # LambdaÁï∞Â∏∏ÊÄßÁµ±Ë®à
        lambda_zscores = [a.lambda_anomaly.lambda_zscore for a in quantum_assessments 
                         if a.lambda_anomaly and a.lambda_anomaly.lambda_zscore > 0]
        if lambda_zscores:
            report += f"""
### Lambda Anomaly Statistics
- **Mean Z-score**: {np.mean(lambda_zscores):.2f}
- **Max Z-score**: {np.max(lambda_zscores):.2f}
- **Significant anomalies (>3œÉ)**: {sum(1 for z in lambda_zscores if z > 3)}
"""
    
    # ÁµêË´ñ
    report += f"""
## Conclusions

The Lambda¬≥ GPU v4.0 analysis successfully identified structural changes and evaluated their quantum origins:

1. **Structural dynamics**: {len(lambda_result.critical_events)} critical structural events detected
"""
    
    if quantum_assessments:
        quantum_count = sum(1 for a in quantum_assessments if a.is_quantum)
        report += f"""2. **Quantum validation**: {quantum_count} events showed quantum signatures
3. **Pattern distribution**: Events classified into instantaneous, transition, and cascade patterns
"""
    
    if two_stage_result:
        report += f"""4. **Network analysis**: Complex residue interaction networks identified
"""
    
    report += """
## Key Improvements in v4.0

- Lambda¬≥ structure anomaly as primary input for quantum validation
- Clear 3-pattern classification (instantaneous/transition/cascade)
- Trajectory-based atomic evidence integration
- Realistic and adjustable quantum criteria
- No automatic classical assignment for long events

---
*Generated by Lambda¬≥ GPU Quantum Validation Pipeline v4.0*
*Lambda¬≥ Integrated Edition - Complete Refactoring*
"""
    
    return report


# ============================================
# CLI Interface
# ============================================

def main():
    """„Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„ÇπÔºàVersion 4.0Ôºâ"""
    
    parser = argparse.ArgumentParser(
        description='Lambda¬≥ GPU Quantum Validation Pipeline v4.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic run
  %(prog)s trajectory.npy metadata.json protein.npy
  
  # With topology
  %(prog)s trajectory.npy metadata.json protein.npy --topology topology.pdb
  
  # Custom output directory
  %(prog)s trajectory.npy metadata.json protein.npy --output ./results_v4
  
  # Skip visualizations
  %(prog)s trajectory.npy metadata.json protein.npy --no-viz
        """
    )
    
    # ÂøÖÈ†àÂºïÊï∞
    parser.add_argument('trajectory', 
                       help='Path to trajectory file (.npy)')
    parser.add_argument('metadata', 
                       help='Path to metadata file (.json)')
    parser.add_argument('protein',
                       help='Path to protein indices file (.npy)')
    
    # „Ç™„Éó„Ç∑„Éß„É≥ÂºïÊï∞
    parser.add_argument('--atom-mapping',
                   help='Path to atom mapping file (residue->atoms JSON)')
    parser.add_argument('--third-impact-top-n',
                       type=int, default=10,
                       help='Number of top residues for Third Impact analysis'))
    parser.add_argument('--topology', '-t',
                       help='Path to topology file (optional)')
    parser.add_argument('--output', '-o', 
                       default='./quantum_results_v4',
                       help='Output directory (default: ./quantum_results_v4)')
    parser.add_argument('--no-two-stage', 
                       action='store_true',
                       help='Skip two-stage residue analysis')
    parser.add_argument('--no-viz', 
                       action='store_true',
                       help='Skip visualization')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='Verbose output')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # „Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
    try:
        results = run_quantum_validation_pipeline(
            trajectory_path=args.trajectory,
            metadata_path=args.metadata,
            protein_indices_path=args.protein,
            topology_path=args.topology,
            enable_two_stage=not args.no_two_stage,
            enable_third_impact=args.enable_third_impact,  # „Åì„Çå„ÇÇÂøÖË¶ÅÔºÅ
            enable_visualization=not args.no_viz,
            output_dir=args.output,
            verbose=args.verbose,
            # Third ImpactÁî®„ÅÆËøΩÂä†„Éë„É©„É°„Éº„Çø
            atom_mapping_path=args.atom_mapping,
            third_impact_top_n=args.third_impact_top_n
        )
        
        if results and results.get('success'):
            print(f"\n‚úÖ Success! Results saved to: {results['output_dir']}")
            return 0
        else:
            print("\n‚ùå Pipeline failed. Check logs for details.")
            return 1
            
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
