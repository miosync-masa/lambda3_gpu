#!/usr/bin/env python3
"""
Lambda¬≥ GPU Quantum Validation Pipeline - Version 3.0 Complete
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ÊüªË™≠ËÄêÊÄßÔºÜÂçò‰∏Ä„Éï„É¨„Éº„É†ÂØæÂøú„ÅÆÂÆåÂÖ®Áâà„Éë„Ç§„Éó„É©„Ç§„É≥
„Çø„É≥„Éë„ÇØË≥™ÂÖ®ÂéüÂ≠ê„Çí‰Ωø„Å£„ÅüË©≥Á¥∞„Å™ÊÆãÂü∫„É¨„Éô„É´Ëß£Êûê„Å´ÊúÄÈÅ©Âåñ

Version: 3.0.0 - Publication Ready
Authors: Lambda¬≥ Team, Áí∞„Å°„ÇÉ„Çì & „Åî‰∏ª‰∫∫„Åï„Åæ üíï
Date: 2025-08-18
"""

import numpy as np
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Lambda¬≥ GPU imports
try:
    from lambda3_gpu.analysis.md_lambda3_detector_gpu import (
        MDLambda3DetectorGPU, 
        MDLambda3Result,
        MDConfig
    )
    from lambda3_gpu.analysis.two_stage_analyzer_gpu import (
        TwoStageAnalyzerGPU,
        TwoStageLambda3Result,
        ResidueAnalysisConfig
    )
    from lambda3_gpu.quantum import (
        QuantumValidationGPU,
        QuantumEventType,
        ValidationCriterion,
        generate_quantum_report
    )
    from lambda3_gpu.visualization import Lambda3VisualizerGPU
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure lambda3_gpu is properly installed")
    raise

# LoggerË®≠ÂÆö
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('quantum_validation')

# ============================================
# „É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞
# ============================================

def run_quantum_validation_pipeline(
    trajectory_path: str,
    metadata_path: str,
    protein_indices_path: str,  # „Çø„É≥„Éë„ÇØË≥™ÂÖ®ÂéüÂ≠ê„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÔºàÂøÖÈ†àÔºâ
    enable_two_stage: bool = True,
    enable_visualization: bool = True,
    output_dir: str = './quantum_results',
    verbose: bool = False
) -> Dict:
    """
    ÂÆåÂÖ®„Å™ÈáèÂ≠êÊ§úË®º„Éë„Ç§„Éó„É©„Ç§„É≥ÔºàVersion 3.0Ôºâ
    
    Parameters
    ----------
    trajectory_path : str
        „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ (.npy)
    metadata_path : str
        „É°„Çø„Éá„Éº„Çø„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ (.json)
    protein_indices_path : str
        „Çø„É≥„Éë„ÇØË≥™ÂéüÂ≠ê„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ (.npy) - Two-Stage„Åß„ÅØÂÖ®ÂéüÂ≠êÊé®Â•®
    enable_two_stage : bool
        2ÊÆµÈöéËß£ÊûêÔºàÊÆãÂü∫„É¨„Éô„É´Ôºâ„ÇíÂÆüË°å„Åô„Çã„Åã
    enable_visualization : bool
        ÂèØË¶ñÂåñ„ÇíÂÆüË°å„Åô„Çã„Åã
    output_dir : str
        Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™
    verbose : bool
        Ë©≥Á¥∞Âá∫Âäõ
        
    Returns
    -------
    Dict
        Ëß£ÊûêÁµêÊûú„ÅÆËæûÊõ∏
    """
    
    # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info("üöÄ LAMBDA¬≥ GPU QUANTUM VALIDATION PIPELINE v3.0")
    logger.info("   Publication Ready Edition")
    logger.info("="*70)
    
    # ========================================
    # Step 1: „Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÔºà„Ç∑„É≥„Éó„É´ÁâàÔºâ
    # ========================================
    logger.info("\nüìÅ Loading data...")
    
    try:
        # „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™Ë™≠„ÅøËæº„Åø
        trajectory = np.load(trajectory_path)
        logger.info(f"   Trajectory shape: {trajectory.shape}")
        n_frames, n_atoms, n_dims = trajectory.shape
        
        if n_dims != 3:
            raise ValueError(f"Trajectory must be 3D, got {n_dims}D")
        
        # „É°„Çø„Éá„Éº„ÇøË™≠„ÅøËæº„Åø
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        logger.info("   Metadata loaded successfully")
        
        # „Çø„É≥„Éë„ÇØË≥™„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπË™≠„ÅøËæº„ÅøÔºàÂøÖÈ†àÔºâ
        if not Path(protein_indices_path).exists():
            raise FileNotFoundError(f"Protein indices file not found: {protein_indices_path}")
            
        protein_indices = np.load(protein_indices_path)
        logger.info(f"   Protein indices loaded: {len(protein_indices)} atoms")
        
        # „Çø„É≥„Éë„ÇØË≥™ÊÉÖÂ†±„ÅÆË°®Á§∫
        if 'protein' in metadata:
            protein_info = metadata['protein']
            logger.info(f"   Protein: {protein_info.get('n_residues', 'N/A')} residues")
            logger.info(f"   Sequence length: {len(protein_info.get('sequence', ''))}")
        
        # Â¶•ÂΩìÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
        max_idx = np.max(protein_indices)
        if max_idx >= n_atoms:
            raise ValueError(f"Protein index {max_idx} exceeds atom count {n_atoms}")
        
        logger.info(f"   ‚úÖ Data validation passed")
            
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise
    
    # ========================================
    # Step 2: Lambda¬≥ GPUËß£ÊûêÔºàÈ´òÈÄüÂàùÊúüÊé¢Á¥¢Ôºâ
    # ========================================
    logger.info("\nüî¨ Running Lambda¬≥ GPU Analysis...")
    logger.info(f"   Using {'full protein' if len(protein_indices) > 100 else 'backbone'} atoms")
    
    try:
        # MDConfigË®≠ÂÆöÔºàTwo-StageÁî®„Å´ÊúÄÈÅ©ÂåñÔºâ
        config = MDConfig()
        config.use_extended_detection = True
        config.use_phase_space = True
        config.use_periodic = True
        config.use_gradual = True
        config.use_drift = True
        config.adaptive_window = True
        config.window_scale = 0.005
        config.gpu_batch_size = 5000
        config.verbose = verbose
        
        # ÊÑüÂ∫¶„Çí‰∏ä„Åí„Å¶Ë©≥Á¥∞„Å™„Ç§„Éô„É≥„Éà„ÇíÊ§úÂá∫
        config.sensitivity = 2.0
        config.use_hierarchical = True  # ÈöéÂ±§ÁöÑÊ§úÂá∫
        
        logger.info("   Config optimized for Two-Stage analysis")
        
        # Lambda¬≥Ê§úÂá∫Âô®ÂàùÊúüÂåñ
        detector = MDLambda3DetectorGPU(config)
        logger.info("   Detector initialized on GPU")
        
        # Ëß£ÊûêÂÆüË°åÔºà„Çø„É≥„Éë„ÇØË≥™ÂéüÂ≠ê„ÅÆ„Åø‰ΩøÁî®Ôºâ
        lambda_result = detector.analyze(trajectory, protein_indices)
        
        logger.info(f"   ‚úÖ Lambda¬≥ analysis complete")
        logger.info(f"   Critical events detected: {len(lambda_result.critical_events)}")
        
        # „Ç§„Éô„É≥„Éà„ÅÆË©≥Á¥∞Ë°®Á§∫
        if verbose and lambda_result.critical_events:
            for i, event in enumerate(lambda_result.critical_events[:5]):
                logger.info(f"     Event {i}: frames {event[0]}-{event[1]}")
        
        # ÁµêÊûú„ÅÆ‰øùÂ≠ò
        result_summary = {
            'n_frames': lambda_result.n_frames,
            'n_atoms': lambda_result.n_atoms,
            'n_protein_atoms': len(protein_indices),
            'n_critical_events': len(lambda_result.critical_events),
            'computation_time': lambda_result.computation_time,
            'gpu_info': getattr(lambda_result, 'gpu_info', {})
        }
        
        with open(output_path / 'lambda_result_summary.json', 'w') as f:
            json.dump(result_summary, f, indent=2)
            
    except Exception as e:
        logger.error(f"Lambda¬≥ analysis failed: {e}")
        raise
    
    # ========================================
    # Step 3: Two-StageË©≥Á¥∞Ëß£ÊûêÔºà„É°„Ç§„É≥Ôºâ
    # ========================================
    two_stage_result = None
    
    if enable_two_stage and len(lambda_result.critical_events) > 0:
        logger.info("\nüî¨ Running Two-Stage Residue-Level Analysis...")
        logger.info("   This is the main analysis for protein dynamics")
        
        try:
            # „Çø„É≥„Éë„ÇØË≥™ÊÆãÂü∫Êï∞„ÅÆÂèñÂæó
            if 'protein' in metadata and 'n_residues' in metadata['protein']:
                n_protein_residues = metadata['protein']['n_residues']
            elif 'n_protein_residues' in metadata:
                n_protein_residues = metadata['n_protein_residues']
            else:
                # „Éá„Éï„Ç©„É´„ÉàÂÄ§ÔºàTrpCage„ÅÆÂ†¥ÂêàÔºâ
                n_protein_residues = 20
                logger.warning(f"   Using default n_residues: {n_protein_residues}")
            
            logger.info(f"   Protein residues: {n_protein_residues}")
            
            # „Çø„É≥„Éë„ÇØË≥™ÈÉ®ÂàÜ„ÅÆ„Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„ÇíÊäΩÂá∫
            protein_trajectory = trajectory[:, protein_indices, :]
            logger.info(f"   Protein trajectory: {protein_trajectory.shape}")
            
            # „Ç§„Éô„É≥„ÉàÁ™ì„ÅÆ‰ΩúÊàêÔºàcritical events„Åã„ÇâÔºâ
            events = []
            for i, event in enumerate(lambda_result.critical_events[:10]):  # ÊúÄÂ§ß10„Ç§„Éô„É≥„Éà
                if isinstance(event, (tuple, list)) and len(event) >= 2:
                    start = int(event[0])
                    end = int(event[1])
                elif hasattr(event, 'frame'):
                    frame = event.frame
                    start = max(0, frame - 100)
                    end = min(n_frames, frame + 100)
                else:
                    continue
                
                # „Ç§„Éô„É≥„ÉàÂêç„Çí‰ªò‰∏é
                event_name = f'critical_{i}'
                events.append((start, end, event_name))
                logger.info(f"     Event {event_name}: frames {start}-{end}")
            
            if events:
                logger.info(f"   Processing {len(events)} events for residue analysis")
                
                # ÊÆãÂü∫Ëß£ÊûêË®≠ÂÆö
                residue_config = ResidueAnalysisConfig()
                residue_config.sensitivity = 2.0
                residue_config.correlation_threshold = 0.15
                residue_config.use_confidence = True
                residue_config.n_bootstrap = 100
                residue_config.parallel_events = True
                
                # TwoStageAnalyzerGPUÂàùÊúüÂåñ
                analyzer = TwoStageAnalyzerGPU(residue_config)
                logger.info("   Two-stage analyzer initialized")
                
                # „Çø„É≥„Éë„ÇØË≥™„ÅÆ„Åø„ÅÆ„Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„ÅßËß£Êûê
                two_stage_result = analyzer.analyze_trajectory(
                    protein_trajectory,      # „Çø„É≥„Éë„ÇØË≥™„ÅÆ„Åø
                    lambda_result,          # „Éû„ÇØ„É≠ÁµêÊûú
                    events,                 # „Ç§„Éô„É≥„Éà„É™„Çπ„Éà
                    n_protein_residues      # „Çø„É≥„Éë„ÇØË≥™ÊÆãÂü∫Êï∞
                )
                
                logger.info("   ‚úÖ Two-stage analysis complete")
                
                # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁµ±Ë®à„ÇíË°®Á§∫
                if hasattr(two_stage_result, 'global_network_stats'):
                    stats = two_stage_result.global_network_stats
                    logger.info("\n   üåê Global Network Statistics:")
                    logger.info(f"     Total causal links: {stats.get('total_causal_links', 0)}")
                    logger.info(f"     Total sync links: {stats.get('total_sync_links', 0)}")
                    logger.info(f"     Total async bonds: {stats.get('total_async_bonds', 0)}")
                    logger.info(f"     Async/Causal ratio: {stats.get('async_to_causal_ratio', 0):.2%}")
                    logger.info(f"     Mean adaptive window: {stats.get('mean_adaptive_window', 0):.1f} frames")
                
                # ÈáçË¶ÅÊÆãÂü∫„ÅÆË°®Á§∫
                if hasattr(two_stage_result, 'global_residue_importance'):
                    top_residues = sorted(
                        two_stage_result.global_residue_importance.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )[:5]
                    
                    logger.info("\n   üéØ Top Important Residues:")
                    for res_id, score in top_residues:
                        if 'protein' in metadata and 'residue_mapping' in metadata['protein']:
                            res_name = metadata['protein']['residue_mapping'].get(
                                str(res_id), {}
                            ).get('name', f'RES{res_id+1}')
                        else:
                            res_name = f'RES{res_id+1}'
                        logger.info(f"     {res_name}: {score:.3f}")
                
                # ‰ªãÂÖ•„Éù„Ç§„É≥„Éà„ÅÆÊèêÊ°à
                if hasattr(two_stage_result, 'suggested_intervention_points'):
                    points = two_stage_result.suggested_intervention_points[:3]
                    logger.info(f"\n   üí° Suggested intervention points: {points}")
                    
            else:
                logger.warning("   No events suitable for Two-Stage analysis")
                
        except Exception as e:
            logger.error(f"Two-stage analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            # Á∂öË°åÔºàÈáèÂ≠êÊ§úË®º„ÅØÂèØËÉΩÔºâ
    
    # ========================================
    # Step 4: ÈáèÂ≠êÊ§úË®ºÔºàVersion 3.0ÂØæÂøúÁâàÔºâ
    # ========================================
    logger.info("\n‚öõÔ∏è Running Quantum Validation (v3.0)...")
    
    quantum_events = []
    
    try:
        # „É°„Çø„Éá„Éº„Çø„Åã„ÇâÁâ©ÁêÜ„Éë„É©„É°„Éº„ÇøÂèñÂæó
        temperature = metadata.get('temperature', 
                                  metadata.get('simulation', {}).get('temperature_K', 310.0))
        dt_ps = metadata.get('time_step_ps', 
                           metadata.get('simulation', {}).get('dt_ps', 2.0))
        
        # ÈáèÂ≠êÊ§úË®ºÂô®ÂàùÊúüÂåñÔºàVersion 3.0„Éë„É©„É°„Éº„ÇøÔºâ
        quantum_validator = QuantumValidationGPU(
            trajectory=trajectory[:, protein_indices, :],  # „Çø„É≥„Éë„ÇØË≥™„ÅÆ„Åø
            metadata=metadata,
            dt_ps=dt_ps,                      # „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó
            temperature_K=temperature,         # Ê∏©Â∫¶
            bootstrap_iterations=1000,         # BootstrapÂèçÂæ©Êï∞
            significance_level=0.01,           # ÊúâÊÑèÊ∞¥Ê∫ñ
            force_cpu=False                    # GPU‰ΩøÁî®
        )
        
        logger.info(f"   Quantum validator v3.0 initialized")
        logger.info(f"   Temperature: {temperature:.1f} K")
        logger.info(f"   Time step: {dt_ps:.1f} ps")
        logger.info(f"   Thermal decoherence: {quantum_validator.thermal_decoherence_ps:.3e} ps")
        
        # ÈáèÂ≠ê„Ç´„Çπ„Ç±„Éº„ÉâËß£ÊûêÔºàVersion 3.0Ôºâ
        # two_stage_result„Çí„Åù„ÅÆ„Åæ„ÅæÊ∏°„ÅôÔºàÂÜÖÈÉ®„ÅßÂá¶ÁêÜ„Åï„Çå„ÇãÔºâ
        quantum_events = quantum_validator.analyze_quantum_cascade(
            lambda_result,
            two_stage_result  # Version 3.0: Áõ¥Êé•Ê∏°„Åô
        )
        
        logger.info(f"   ‚úÖ Quantum validation complete")
        logger.info(f"   Quantum events detected: {len(quantum_events)}")
        
        # „Ç§„Éô„É≥„Éà„Çø„Ç§„ÉóÂà•ÈõÜË®àÔºàVersion 3.0„ÅÆÊñ∞Ê©üËÉΩÔºâ
        event_types = Counter(e.event_type.value for e in quantum_events)
        
        logger.info("\n   üìä Event Type Distribution:")
        for event_type, count in event_types.items():
            logger.info(f"     {event_type}: {count}")
        
        # ÈáèÂ≠ê„Ç§„Éô„É≥„Éà„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
        quantum_only = [e for e in quantum_events if e.quantum_metrics.is_quantum]
        logger.info(f"   Confirmed quantum events: {len(quantum_only)}/{len(quantum_events)}")
        
        # Âà§ÂÆöÂü∫Ê∫ñ„ÅÆÁµ±Ë®àÔºàVersion 3.0„ÅÆÊñ∞Ê©üËÉΩÔºâ
        criterion_stats = {}
        for event in quantum_events:
            for criterion in event.quantum_metrics.criteria_passed:
                name = criterion.criterion.value
                if name not in criterion_stats:
                    criterion_stats[name] = {'passed': 0, 'total': 0}
                criterion_stats[name]['total'] += 1
                if criterion.passed:
                    criterion_stats[name]['passed'] += 1
        
        if criterion_stats:
            logger.info("\n   üìà Validation Criteria Statistics:")
            for name, stats in sorted(criterion_stats.items()):
                if stats['total'] > 0:
                    rate = stats['passed'] / stats['total'] * 100
                    logger.info(f"     {name}: {stats['passed']}/{stats['total']} ({rate:.1f}%)")
        
        # Ëá®Áïå„Ç§„Éô„É≥„Éà„ÅÆÁâπÂÆöÔºàVersion 3.0Ôºâ
        critical_quantum = [e for e in quantum_events if e.is_critical]
        if critical_quantum:
            logger.info(f"\n   üí´ Critical quantum events: {len(critical_quantum)}")
            for i, event in enumerate(critical_quantum[:3]):
                qm = event.quantum_metrics
                logger.info(f"     {i+1}. Frame {event.frame_start}-{event.frame_end}")
                logger.info(f"        Type: {event.event_type.value}")
                logger.info(f"        Quantum confidence: {qm.quantum_confidence:.3f}")
                if qm.bell_violated:
                    logger.info(f"        CHSH: {qm.chsh_value:.3f} (p={qm.chsh_p_value:.4f})")
                logger.info(f"        Reasons: {', '.join(event.critical_reasons)}")
        
        # BellÈÅïÂèç„ÅÆË©≥Á¥∞Áµ±Ë®à
        bell_violations = [e for e in quantum_events 
                          if e.quantum_metrics.bell_violated]
        
        if bell_violations:
            violation_rate = 100 * len(bell_violations) / len(quantum_events)
            logger.info(f"\n   üîî Bell violations: {len(bell_violations)}/{len(quantum_events)} "
                       f"({violation_rate:.1f}%)")
            
            # CHSHÂÄ§„ÅÆÁµ±Ë®à
            chsh_values = [e.quantum_metrics.chsh_value for e in bell_violations]
            raw_values = [e.quantum_metrics.chsh_raw_value for e in bell_violations]
            
            logger.info(f"   CHSH statistics:")
            logger.info(f"     Corrected: mean={np.mean(chsh_values):.3f}, "
                       f"max={np.max(chsh_values):.3f}")
            logger.info(f"     Raw: mean={np.mean(raw_values):.3f}, "
                       f"max={np.max(raw_values):.3f}")
            logger.info(f"     Tsirelson bound: {2*np.sqrt(2):.3f}")
        
        # „Çµ„Éû„É™„ÉºË°®Á§∫ÔºàVersion 3.0„ÅÆÊã°ÂºµÁâàÔºâ
        quantum_validator.print_validation_summary(quantum_events)
        
        # ÈáèÂ≠ê„Ç§„Éô„É≥„Éà„ÅÆË©≥Á¥∞‰øùÂ≠òÔºàVersion 3.0ÂØæÂøúÔºâ
        save_quantum_events_v3(quantum_events, output_path, metadata)
        
        # ÊüªË™≠Áî®„É¨„Éù„Éº„ÉàÁîüÊàêÔºàVersion 3.0„ÅÆÊñ∞Ê©üËÉΩÔºâ
        report = generate_quantum_report(quantum_events)
        
        with open(output_path / 'quantum_validation_report.txt', 'w') as f:
            f.write(report)
        logger.info(f"   üìÑ Validation report saved")
        
    except Exception as e:
        logger.error(f"Quantum validation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
    
    # ========================================
    # Step 5: ÂèØË¶ñÂåñÔºàË©≥Á¥∞ÁâàÔºâ
    # ========================================
    if enable_visualization:
        logger.info("\nüìä Creating detailed visualizations...")
        
        try:
            visualizer = Lambda3VisualizerGPU()
            
            # Lambda¬≥ÁµêÊûú„ÅÆÂèØË¶ñÂåñ
            fig_lambda = visualizer.visualize_results(
                lambda_result,
                save_path=str(output_path / 'lambda_results.png')
            )
            logger.info("   Lambda¬≥ results visualized")
            
            # ÈáèÂ≠ê„Ç§„Éô„É≥„Éà„ÅÆÂèØË¶ñÂåñ
            if quantum_events:
                fig_quantum = visualize_quantum_results(
                    quantum_events,
                    save_path=str(output_path / 'quantum_events.png')
                )
                logger.info("   Quantum events visualized")
            
            # Two-StageÁµêÊûú„ÅÆÂèØË¶ñÂåñ
            if two_stage_result:
                fig_network = visualize_residue_network(
                    two_stage_result,
                    save_path=str(output_path / 'residue_network.png')
                )
                logger.info("   Residue network visualized")
            
            logger.info("   ‚úÖ All visualizations saved")
            
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
    
    # ========================================
    # Step 6: ÂåÖÊã¨ÁöÑ„É¨„Éù„Éº„ÉàÁîüÊàê
    # ========================================
    logger.info("\nüìù Generating comprehensive report...")
    
    report = generate_comprehensive_report(
        lambda_result,
        quantum_events,
        two_stage_result,
        metadata,
        protein_indices
    )
    
    report_path = output_path / 'analysis_report.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    logger.info(f"   Report saved to {report_path}")
    
    # ========================================
    # ÂÆå‰∫Ü
    # ========================================
    logger.info("\n" + "="*70)
    logger.info("‚úÖ PIPELINE COMPLETE!")
    logger.info(f"   Results directory: {output_path}")
    logger.info(f"   Key findings:")
    
    if lambda_result:
        logger.info(f"     - {len(lambda_result.critical_events)} critical events")
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        logger.info(f"     - {two_stage_result.global_network_stats.get('total_causal_links', 0)} causal links")
    if quantum_events:
        logger.info(f"     - {len(quantum_events)} quantum signatures")
        logger.info(f"     - {len(quantum_only)} confirmed quantum events")
        logger.info(f"     - {len(bell_violations)} Bell violations")
    
    logger.info("="*70)
    
    return {
        'lambda_result': lambda_result,
        'quantum_events': quantum_events,
        'two_stage_result': two_stage_result,
        'output_dir': output_path,
        'success': True
    }


# ============================================
# Ë£úÂä©Èñ¢Êï∞Áæ§
# ============================================

def save_quantum_events_v3(quantum_events: List, output_path: Path, metadata: Dict):
    """ÈáèÂ≠ê„Ç§„Éô„É≥„Éà„ÅÆË©≥Á¥∞‰øùÂ≠òÔºàVersion 3.0ÂØæÂøúÔºâ"""
    quantum_data = []
    
    for event in quantum_events:
        event_dict = {
            'frame_start': event.frame_start,
            'frame_end': event.frame_end,
            'event_type': event.event_type.value,
            'is_critical': event.is_critical,
            'critical_reasons': event.critical_reasons,
            'validation_window': list(event.validation_window)
        }
        
        # ÈáèÂ≠ê„É°„Éà„É™„ÇØ„ÇπÔºàVersion 3.0„ÅÆÂÖ®„Éï„Ç£„Éº„É´„ÉâÔºâ
        qm = event.quantum_metrics
        event_dict['quantum_metrics'] = {
            # Âü∫Êú¨ÂàÜÈ°û
            'event_type': qm.event_type.value,
            'duration_frames': qm.duration_frames,
            'duration_ps': qm.duration_ps,
            
            # ÈáèÂ≠êÂà§ÂÆö
            'is_quantum': qm.is_quantum,
            'quantum_confidence': float(qm.quantum_confidence),
            'quantum_score': float(qm.quantum_score),
            
            # CHSHÊ§úË®º
            'bell_violated': qm.bell_violated,
            'chsh_value': float(qm.chsh_value),
            'chsh_raw_value': float(qm.chsh_raw_value),
            'chsh_confidence': float(qm.chsh_confidence),
            'chsh_p_value': float(qm.chsh_p_value),
            
            # Áâ©ÁêÜÊåáÊ®ô
            'coherence_time_ps': float(qm.coherence_time_ps),
            'thermal_ratio': float(qm.thermal_ratio),
            'tunneling_probability': float(qm.tunneling_probability),
            'energy_barrier_kT': float(qm.energy_barrier_kT),
            
            # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊåáÊ®ô
            'n_async_bonds': qm.n_async_bonds,
            'max_causality': float(qm.max_causality),
            'min_sync_rate': float(qm.min_sync_rate),
            'mean_lag_frames': float(qm.mean_lag_frames),
            
            # Áµ±Ë®àÊÉÖÂ†±
            'n_samples_used': qm.n_samples_used,
            'data_quality': float(qm.data_quality),
            'bootstrap_iterations': qm.bootstrap_iterations
        }
        
        # Âà§ÂÆöÂü∫Ê∫ñ„ÅÆË©≥Á¥∞ÔºàVersion 3.0Ôºâ
        event_dict['criteria_passed'] = []
        for criterion in qm.criteria_passed:
            event_dict['criteria_passed'].append({
                'criterion': criterion.criterion.value,
                'reference': criterion.reference,
                'value': float(criterion.value),
                'threshold': float(criterion.threshold),
                'passed': criterion.passed,
                'p_value': float(criterion.p_value) if criterion.p_value else None,
                'description': criterion.description
            })
        
        # ÊÆãÂü∫ÊÉÖÂ†±ÔºàÂØæÂøú„Åô„ÇãÊÆãÂü∫ID„ÇíË®òÈå≤Ôºâ
        event_dict['residue_ids'] = event.residue_ids
        
        # async bondsÊÉÖÂ†±Ôºà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØËß£Êûê„Åã„ÇâÔºâ
        if event.async_bonds_used:
            event_dict['async_bonds'] = []
            for bond in event.async_bonds_used[:10]:  # ÊúÄÂ§ß10ÂÄã
                bond_dict = {
                    'residue_pair': bond.get('residue_pair', []),
                    'strength': float(bond.get('strength', 0)),
                    'lag': int(bond.get('lag', 0)),
                    'sync_rate': float(bond.get('sync_rate', 0)),
                    'type': bond.get('type', 'unknown')
                }
                event_dict['async_bonds'].append(bond_dict)
        
        # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁµ±Ë®à
        if event.network_stats:
            event_dict['network_stats'] = {
                'n_async_bonds': len(event.network_stats.get('async_bonds', [])),
                'n_causal_links': len(event.network_stats.get('causal_links', [])),
                'network_type': event.network_stats.get('network_type')
            }
        
        # Áµ±Ë®àÁöÑÊ§úÂÆöÁµêÊûúÔºà„ÅÇ„Çå„Å∞Ôºâ
        if event.statistical_tests:
            event_dict['statistical_tests'] = event.statistical_tests
        
        quantum_data.append(event_dict)
    
    # „É°„Çø„Éá„Éº„Çø„ÇÇÂê´„ÇÅ„Å¶‰øùÂ≠ò
    output_data = {
        'metadata': {
            'system_name': metadata.get('system_name', 'Unknown'),
            'temperature_K': metadata.get('temperature', 310.0),
            'dt_ps': metadata.get('time_step_ps', 2.0),
            'n_protein_residues': metadata.get('protein', {}).get('n_residues', 0),
            'analysis_version': '3.0.0'
        },
        'summary': {
            'total_events': len(quantum_events),
            'quantum_events': sum(1 for e in quantum_events if e.quantum_metrics.is_quantum),
            'bell_violations': sum(1 for e in quantum_events if e.quantum_metrics.bell_violated),
            'critical_events': sum(1 for e in quantum_events if e.is_critical),
            'event_type_distribution': dict(Counter(e.event_type.value for e in quantum_events))
        },
        'events': quantum_data
    }
    
    with open(output_path / 'quantum_events_v3.json', 'w') as f:
        json.dump(output_data, f, indent=2)
    
    logger.info(f"   üíæ Saved {len(quantum_data)} quantum events with full v3.0 metrics")


def visualize_quantum_results(quantum_events: List, 
                             save_path: Optional[str] = None) -> plt.Figure:
    """ÈáèÂ≠êÊ§úË®ºÁµêÊûú„ÅÆÂèØË¶ñÂåñÔºàVersion 3.0ÂØæÂøúÔºâ"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    if not quantum_events:
        fig.text(0.5, 0.5, 'No Quantum Events Detected', 
                ha='center', va='center', fontsize=20)
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        return fig
    
    # „Éá„Éº„ÇøÊäΩÂá∫
    frames = []
    chsh_values = []
    quantum_scores = []
    confidences = []
    event_type_list = []
    
    for e in quantum_events:
        if hasattr(e, 'frame_start'):
            frames.append(e.frame_start)
        if hasattr(e, 'quantum_metrics'):
            chsh_values.append(e.quantum_metrics.chsh_value)
            quantum_scores.append(e.quantum_metrics.quantum_score)
            confidences.append(e.quantum_metrics.chsh_confidence)
        if hasattr(e, 'event_type'):
            event_type_list.append(e.event_type.value)
    
    # 1. CHSHÂÄ§„ÅÆÊôÇÁ≥ªÂàó
    ax1 = axes[0, 0]
    if frames and chsh_values:
        ax1.scatter(frames[:len(chsh_values)], chsh_values[:len(frames)], 
                   c='blue', alpha=0.6, s=50)
        ax1.axhline(y=2.0, color='red', linestyle='--', label='Classical Bound')
        ax1.axhline(y=2*np.sqrt(2), color='green', linestyle='--', 
                   label=f'Tsirelson Bound ({2*np.sqrt(2):.3f})')
        ax1.set_xlabel('Frame')
        ax1.set_ylabel('CHSH Value')
        ax1.set_title('CHSH Inequality Timeline')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # 2. „Ç§„Éô„É≥„Éà„Çø„Ç§„ÉóÂàÜÂ∏ÉÔºàVersion 3.0Ôºâ
    ax2 = axes[0, 1]
    event_counts = Counter(event_type_list)
    if event_counts:
        ax2.pie(event_counts.values(), 
               labels=event_counts.keys(),
               autopct='%1.1f%%',
               startangle=90)
        ax2.set_title('Event Type Distribution')
    
    # 3. BellÈÅïÂèç„ÅÆÂàÜÂ∏É
    ax3 = axes[0, 2]
    n_violated = sum(1 for e in quantum_events 
                    if hasattr(e, 'quantum_metrics') 
                    and e.quantum_metrics.bell_violated)
    n_classical = len(quantum_events) - n_violated
    
    ax3.pie([n_violated, n_classical], 
           labels=[f'Violated ({n_violated})', f'Classical ({n_classical})'],
           colors=['red', 'blue'],
           autopct='%1.1f%%',
           startangle=90)
    ax3.set_title('Bell Violation Distribution')
    
    # 4. ÈáèÂ≠ê„Çπ„Ç≥„Ç¢ÂàÜÂ∏É
    ax4 = axes[1, 0]
    if quantum_scores:
        ax4.hist(quantum_scores, bins=20, alpha=0.7, color='purple', edgecolor='black')
        ax4.set_xlabel('Quantum Score')
        ax4.set_ylabel('Count')
        ax4.set_title('Quantum Score Distribution')
        ax4.grid(True, alpha=0.3)
    
    # 5. ‰ø°È†ºÂ∫¶vs CHSHÂÄ§
    ax5 = axes[1, 1]
    if confidences and chsh_values and quantum_scores:
        scatter = ax5.scatter(confidences[:len(chsh_values)], 
                            chsh_values[:len(confidences)], 
                            c=quantum_scores[:min(len(confidences), len(chsh_values))], 
                            cmap='viridis',
                            s=50, alpha=0.6)
        plt.colorbar(scatter, ax=ax5, label='Quantum Score')
        ax5.set_xlabel('Confidence')
        ax5.set_ylabel('CHSH Value')
        ax5.set_title('Confidence vs CHSH Value')
        ax5.grid(True, alpha=0.3)
    
    # 6. Âà§ÂÆöÂü∫Ê∫ñÈÄöÈÅéÁéáÔºàVersion 3.0Ôºâ
    ax6 = axes[1, 2]
    criterion_counts = {}
    for e in quantum_events:
        if hasattr(e, 'quantum_metrics'):
            for criterion in e.quantum_metrics.criteria_passed:
                name = criterion.criterion.value
                if name not in criterion_counts:
                    criterion_counts[name] = {'passed': 0, 'total': 0}
                criterion_counts[name]['total'] += 1
                if criterion.passed:
                    criterion_counts[name]['passed'] += 1
    
    if criterion_counts:
        names = list(criterion_counts.keys())
        pass_rates = [c['passed']/c['total']*100 if c['total'] > 0 else 0 
                     for c in criterion_counts.values()]
        ax6.bar(range(len(names)), pass_rates, alpha=0.7, color='steelblue')
        ax6.set_xticks(range(len(names)))
        ax6.set_xticklabels(names, rotation=45, ha='right')
        ax6.set_ylabel('Pass Rate (%)')
        ax6.set_title('Validation Criteria Pass Rates')
        ax6.grid(True, alpha=0.3)
    
    plt.suptitle('Quantum Validation Results v3.0', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def visualize_residue_network(two_stage_result: Any,
                             save_path: Optional[str] = None) -> plt.Figure:
    """ÊÆãÂü∫„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂèØË¶ñÂåñ"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. ÊÆãÂü∫ÈáçË¶ÅÂ∫¶„É©„É≥„Ç≠„É≥„Ç∞
    ax1 = axes[0, 0]
    if hasattr(two_stage_result, 'global_residue_importance'):
        top_residues = sorted(
            two_stage_result.global_residue_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        if top_residues:
            residue_ids = [f"R{r[0]+1}" for r in top_residues]
            scores = [r[1] for r in top_residues]
            
            ax1.barh(residue_ids, scores, alpha=0.7, color='steelblue')
            ax1.set_xlabel('Importance Score')
            ax1.set_title('Top 20 Important Residues')
            ax1.invert_yaxis()
    
    # 2. „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁµ±Ë®à
    ax2 = axes[0, 1]
    if hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        labels = ['Causal', 'Sync', 'Async']
        values = [
            stats.get('total_causal_links', 0),
            stats.get('total_sync_links', 0),
            stats.get('total_async_bonds', 0)
        ]
        
        if sum(values) > 0:
            ax2.pie(values, labels=labels, autopct='%1.1f%%',
                   startangle=90, colors=['red', 'green', 'blue'])
            ax2.set_title('Network Link Distribution')
    
    # 3. „Ç§„Éô„É≥„ÉàÂà•Áµ±Ë®à
    ax3 = axes[1, 0]
    if hasattr(two_stage_result, 'residue_analyses'):
        event_names = list(two_stage_result.residue_analyses.keys())
        n_residues = [len(a.residue_events) 
                      for a in two_stage_result.residue_analyses.values()]
        
        if event_names and n_residues:
            ax3.bar(range(len(event_names)), n_residues, alpha=0.7, color='orange')
            ax3.set_xticks(range(len(event_names)))
            ax3.set_xticklabels(event_names, rotation=45, ha='right')
            ax3.set_ylabel('Number of Residues Involved')
            ax3.set_title('Residues per Event')
    
    # 4. GPUÊÄßËÉΩ
    ax4 = axes[1, 1]
    if hasattr(two_stage_result, 'residue_analyses'):
        gpu_times = [a.gpu_time for a in two_stage_result.residue_analyses.values()]
        event_names = list(two_stage_result.residue_analyses.keys())
        
        if gpu_times and event_names:
            ax4.plot(range(len(event_names)), gpu_times, 'go-', 
                    linewidth=2, markersize=8)
            ax4.set_xticks(range(len(event_names)))
            ax4.set_xticklabels(event_names, rotation=45, ha='right')
            ax4.set_ylabel('GPU Time (seconds)')
            ax4.set_title('GPU Processing Time per Event')
            ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Residue-Level Analysis Summary', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig


def generate_comprehensive_report(
    lambda_result: Any,
    quantum_events: List,
    two_stage_result: Optional[Any],
    metadata: Dict,
    protein_indices: np.ndarray
) -> str:
    """ÂåÖÊã¨ÁöÑ„Å™Ëß£Êûê„É¨„Éù„Éº„Éà„ÅÆÁîüÊàêÔºàVersion 3.0ÂØæÂøúÔºâ"""
    
    # „Ç∑„Çπ„ÉÜ„É†Âêç„ÅÆÂèñÂæó
    system_name = metadata.get('system_name', 'Unknown System')
    
    report = f"""# Lambda¬≥ GPU Analysis Report v3.0

## System Information
- **System**: {system_name}
- **Temperature**: {metadata.get('temperature', metadata.get('simulation', {}).get('temperature_K', 310))} K
- **Time step**: {metadata.get('time_step_ps', metadata.get('simulation', {}).get('dt_ps', 2.0))} ps
- **Frames analyzed**: {lambda_result.n_frames}
- **Total atoms**: {lambda_result.n_atoms}
- **Protein atoms**: {len(protein_indices)}
"""
    
    # „Çø„É≥„Éë„ÇØË≥™ÊÉÖÂ†±
    if 'protein' in metadata:
        protein_info = metadata['protein']
        report += f"""- **Protein residues**: {protein_info.get('n_residues', 'N/A')}
- **Sequence length**: {len(protein_info.get('sequence', ''))}
"""
    
    report += f"""- **Computation time**: {lambda_result.computation_time:.2f} seconds
- **Analysis version**: 3.0.0 (Publication Ready)

## Lambda¬≥ Analysis Results
- **Critical events**: {len(lambda_result.critical_events)}
"""
    
    # ÊßãÈÄ†Â¢ÉÁïå„Å®„Éà„Éù„É≠„Ç∏„Ç´„É´Á†¥„Çå
    if hasattr(lambda_result, 'structural_boundaries'):
        report += f"- **Structural boundaries**: {len(lambda_result.structural_boundaries)}\n"
    
    if hasattr(lambda_result, 'topological_breaks'):
        report += f"- **Topological breaks**: {len(lambda_result.topological_breaks)}\n"
    
    # Two-StageËß£ÊûêÁµêÊûú
    if two_stage_result:
        report += f"""
## Two-Stage Residue-Level Analysis

### Overview
"""
        if hasattr(two_stage_result, 'residue_analyses'):
            report += f"- **Events analyzed**: {len(two_stage_result.residue_analyses)}\n"
        
        if hasattr(two_stage_result, 'global_network_stats'):
            stats = two_stage_result.global_network_stats
            report += f"""
### Network Statistics
- **Total causal links**: {stats.get('total_causal_links', 0)}
- **Total sync links**: {stats.get('total_sync_links', 0)}
- **Total async bonds**: {stats.get('total_async_bonds', 0)}
- **Async/Causal ratio**: {stats.get('async_to_causal_ratio', 0):.2%}
- **Mean adaptive window**: {stats.get('mean_adaptive_window', 0):.1f} frames
"""
        
        # ÈáçË¶ÅÊÆãÂü∫
        if hasattr(two_stage_result, 'global_residue_importance'):
            top_residues = sorted(
                two_stage_result.global_residue_importance.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]
            
            if top_residues:
                report += """
### Top Important Residues
| Rank | Residue | Importance Score |
|------|---------|-----------------|
"""
                for i, (res_id, score) in enumerate(top_residues, 1):
                    # ÊÆãÂü∫Âêç„ÅÆÂèñÂæó
                    if 'protein' in metadata and 'residue_mapping' in metadata['protein']:
                        res_name = metadata['protein']['residue_mapping'].get(
                            str(res_id), {}
                        ).get('name', f'RES{res_id+1}')
                    else:
                        res_name = f'RES{res_id+1}'
                    report += f"| {i} | {res_name} | {score:.3f} |\n"
        
        # ‰ªãÂÖ•„Éù„Ç§„É≥„Éà
        if hasattr(two_stage_result, 'suggested_intervention_points'):
            points = two_stage_result.suggested_intervention_points[:5]
            if points:
                report += f"\n### Suggested Intervention Points\n"
                report += f"Residues: {points}\n"
    
    # ÈáèÂ≠êÊ§úË®ºÁµêÊûúÔºàVersion 3.0Êã°ÂºµÔºâ
    report += f"""
## Quantum Validation Results (v3.0)
- **Total events analyzed**: {len(quantum_events)}
"""
    
    if quantum_events:
        # Âü∫Êú¨Áµ±Ë®à
        n_quantum = sum(1 for e in quantum_events if e.quantum_metrics.is_quantum)
        n_bell = sum(1 for e in quantum_events if e.quantum_metrics.bell_violated)
        n_critical = sum(1 for e in quantum_events if e.is_critical)
        
        report += f"""- **Confirmed quantum events**: {n_quantum} ({100*n_quantum/len(quantum_events):.1f}%)
- **Bell violations**: {n_bell} ({100*n_bell/len(quantum_events):.1f}%)
- **Critical quantum events**: {n_critical}

### Event Type Distribution
"""
        # „Ç§„Éô„É≥„Éà„Çø„Ç§„ÉóÂàÜÂ∏É
        event_types = Counter(e.event_type.value for e in quantum_events)
        for event_type, count in sorted(event_types.items()):
            report += f"- **{event_type}**: {count}\n"
        
        # Âà§ÂÆöÂü∫Ê∫ñÁµ±Ë®à
        criterion_stats = {}
        for event in quantum_events:
            for criterion in event.quantum_metrics.criteria_passed:
                name = criterion.criterion.value
                if name not in criterion_stats:
                    criterion_stats[name] = {'passed': 0, 'total': 0, 'refs': set()}
                criterion_stats[name]['total'] += 1
                if criterion.passed:
                    criterion_stats[name]['passed'] += 1
                criterion_stats[name]['refs'].add(criterion.reference)
        
        if criterion_stats:
            report += """
### Validation Criteria Statistics
| Criterion | Pass Rate | References |
|-----------|-----------|------------|
"""
            for name, stats in sorted(criterion_stats.items()):
                rate = stats['passed'] / stats['total'] * 100 if stats['total'] > 0 else 0
                refs = list(stats['refs'])[0] if stats['refs'] else 'N/A'
                report += f"| {name} | {rate:.1f}% ({stats['passed']}/{stats['total']}) | {refs} |\n"
        
        # CHSHÁµ±Ë®à
        chsh_values = [e.quantum_metrics.chsh_value 
                      for e in quantum_events 
                      if hasattr(e, 'quantum_metrics')]
        if chsh_values:
            report += f"""
### CHSH Statistics
- **Average CHSH value**: {np.mean(chsh_values):.3f}
- **Max CHSH value**: {np.max(chsh_values):.3f}
- **Min CHSH value**: {np.min(chsh_values):.3f}
- **Standard deviation**: {np.std(chsh_values):.3f}
- **Classical bound**: 2.000
- **Tsirelson bound**: {2*np.sqrt(2):.3f}

### Interpretation
"""
            if np.max(chsh_values) > 2.0:
                report += "‚öõÔ∏è **Quantum correlations detected**: CHSH values exceed classical bound\n"
            if np.max(chsh_values) > 2.4:
                report += "üåü **Strong quantum signatures**: Significant Bell inequality violations\n"
            if n_quantum > len(quantum_events) * 0.1:
                report += "üí´ **Quantum prevalence**: >10% of events show quantum characteristics\n"
    
    # ÁµêË´ñ
    report += f"""
## Conclusions

The Lambda¬≥ GPU v3.0 analysis of {system_name} successfully completed with the following key findings:

1. **Structural dynamics**: {len(lambda_result.critical_events)} critical events identified
"""
    
    if two_stage_result and hasattr(two_stage_result, 'global_network_stats'):
        stats = two_stage_result.global_network_stats
        report += f"""2. **Residue interactions**: {stats.get('total_causal_links', 0)} causal relationships detected
3. **Async coupling**: {stats.get('total_async_bonds', 0)} async strong bonds identified
"""
    
    if quantum_events:
        n_quantum = sum(1 for e in quantum_events if e.quantum_metrics.is_quantum)
        n_bell = sum(1 for e in quantum_events if e.quantum_metrics.bell_violated)
        if n_bell > 0:
            report += f"""4. **Quantum signatures**: {n_bell} Bell violations confirm non-classical correlations
5. **Quantum events**: {n_quantum} events passed quantum validation criteria
"""
    
    report += """
## Recommendations

Based on the analysis results:
"""
    
    # Êé®Â•®‰∫ãÈ†Ö
    if two_stage_result and hasattr(two_stage_result, 'suggested_intervention_points'):
        points = two_stage_result.suggested_intervention_points[:3]
        if points:
            report += f"""
1. **Target residues for intervention**: Focus on residues {points} for potential drug targeting or mutation studies
"""
    
    if quantum_events and n_quantum > 0:
        report += """
2. **Quantum effects**: Consider quantum mechanical effects in protein dynamics modeling
3. **Non-classical correlations**: Account for Bell violations in theoretical models
"""
    
    report += """
---
*Generated by Lambda¬≥ GPU Quantum Validation Pipeline v3.0*
*Publication Ready - Peer Review Compatible*
*NO TIME, NO PHYSICS, ONLY STRUCTURE!*
"""
    
    return report


# ============================================
# CLI Interface
# ============================================

def main():
    """„Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"""
    
    parser = argparse.ArgumentParser(
        description='Lambda¬≥ GPU Quantum Validation Pipeline v3.0 - Publication Ready',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic run with protein atoms
  %(prog)s trajectory.npy metadata.json protein.npy
  
  # With custom output directory
  %(prog)s trajectory.npy metadata.json protein.npy --output ./my_results
  
  # Skip visualizations for faster processing
  %(prog)s trajectory.npy metadata.json protein.npy --no-viz
  
  # Verbose mode for debugging
  %(prog)s trajectory.npy metadata.json protein.npy --verbose
        """
    )
    
    # ÂøÖÈ†àÂºïÊï∞
    parser.add_argument('trajectory', 
                       help='Path to trajectory file (.npy)')
    parser.add_argument('metadata', 
                       help='Path to metadata file (.json)')
    parser.add_argument('protein',
                       help='Path to protein indices file (.npy) - use protein.npy for detailed analysis')
    
    # „Ç™„Éó„Ç∑„Éß„É≥ÂºïÊï∞
    parser.add_argument('--output', '-o', 
                       default='./quantum_results',
                       help='Output directory (default: ./quantum_results)')
    parser.add_argument('--no-two-stage', 
                       action='store_true',
                       help='Skip two-stage residue analysis')
    parser.add_argument('--no-viz', 
                       action='store_true',
                       help='Skip visualization')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='Verbose output for debugging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # „Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
    try:
        results = run_quantum_validation_pipeline(
            trajectory_path=args.trajectory,
            metadata_path=args.metadata,
            protein_indices_path=args.protein,
            enable_two_stage=not args.no_two_stage,
            enable_visualization=not args.no_viz,
            output_dir=args.output,
            verbose=args.verbose
        )
        
        if results and results.get('success'):
            print(f"\n‚úÖ Success! Results saved to: {results['output_dir']}")
            return 0
        else:
            print("\n‚ùå Pipeline failed. Check logs for details.")
            return 1
            
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
