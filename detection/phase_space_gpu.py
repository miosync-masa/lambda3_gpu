"""
LambdaÂ³ GPUç‰ˆä½ç›¸ç©ºé–“è§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
é«˜æ¬¡å…ƒä½ç›¸ç©ºé–“ã§ã®ç•°å¸¸æ¤œå‡ºã¨ã‚¢ãƒˆãƒ©ã‚¯ã‚¿è§£æ
"""

import numpy as np
import cupy as cp
from typing import Dict, List, Tuple, Optional, Any
from numba import cuda
from cupyx.scipy.spatial import distance_matrix

from ..types import ArrayType, NDArray
from ..core.gpu_utils import GPUBackend

class PhaseSpaceAnalyzerGPU(GPUBackend):
    """ä½ç›¸ç©ºé–“è§£æã®GPUå®Ÿè£…"""
    
    def __init__(self, force_cpu=False):
        super().__init__(force_cpu)
        self.embedding_cache = {}
        
    def analyze_phase_space(self,
                          structures: Dict[str, np.ndarray],
                          embedding_params: Optional[Dict] = None) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªä½ç›¸ç©ºé–“è§£æ
        
        Parameters
        ----------
        structures : Dict[str, np.ndarray]
            Lambdaæ§‹é€ ä½“
        embedding_params : Dict, optional
            åŸ‹ã‚è¾¼ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns
        -------
        Dict[str, Any]
            ä½ç›¸ç©ºé–“è§£æçµæœ
        """
        print("\nğŸŒŒ Comprehensive Phase Space Analysis on GPU...")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if embedding_params is None:
            embedding_params = {
                'embedding_dim': 3,
                'delay': 50,
                'n_neighbors': 20,
                'recurrence_threshold': 0.1
            }
        
        # ä¸»è¦ãªæ™‚ç³»åˆ—ã‚’å–å¾—
        primary_series = self._get_primary_series(structures)
        
        results = {}
        
        with self.memory_manager.batch_context(len(primary_series) * 10):
            # 1. æœ€é©åŸ‹ã‚è¾¼ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®š
            optimal_params = self._estimate_embedding_parameters_gpu(
                primary_series, embedding_params
            )
            results['optimal_parameters'] = optimal_params
            
            # 2. ä½ç›¸ç©ºé–“å†æ§‹æˆ
            phase_space = self._reconstruct_phase_space_gpu(
                primary_series,
                optimal_params['embedding_dim'],
                optimal_params['delay']
            )
            results['phase_space'] = self.to_cpu(phase_space)
            
            # 3. ã‚¢ãƒˆãƒ©ã‚¯ã‚¿è§£æ
            attractor_features = self._analyze_attractor_gpu(phase_space)
            results['attractor_features'] = attractor_features
            
            # 4. ãƒªã‚«ãƒ¬ãƒ³ã‚¹ãƒ—ãƒ­ãƒƒãƒˆè§£æ
            recurrence_features = self._recurrence_analysis_gpu(
                phase_space, embedding_params['recurrence_threshold']
            )
            results['recurrence_features'] = recurrence_features
            
            # 5. ç•°å¸¸è»Œé“æ¤œå‡º
            anomaly_scores = self._detect_anomalous_trajectories_gpu(
                phase_space, embedding_params['n_neighbors']
            )
            results['anomaly_scores'] = self.to_cpu(anomaly_scores)
            
            # 6. ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ç‰¹æ€§
            dynamics_features = self._analyze_dynamics_gpu(phase_space)
            results['dynamics_features'] = dynamics_features
            
            # 7. çµ±åˆã‚¹ã‚³ã‚¢
            integrated_score = self._compute_integrated_score_gpu(
                anomaly_scores, attractor_features, recurrence_features
            )
            results['integrated_anomaly_score'] = self.to_cpu(integrated_score)
        
        self._print_summary(results)
        
        return results
    
    def detect_phase_transitions(self,
                               structures: Dict[str, np.ndarray],
                               window_size: int = 1000,
                               stride: int = 100) -> Dict[str, cp.ndarray]:
        """
        ä½ç›¸ç©ºé–“ã§ã®é·ç§»æ¤œå‡ºï¼ˆã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
        """
        print("\nğŸ”„ Detecting phase transitions in sliding windows...")
        
        primary_series = self._get_primary_series(structures)
        n_frames = len(primary_series)
        n_windows = (n_frames - window_size) // stride + 1
        
        transition_scores = cp.zeros(n_frames)
        phase_distances = []
        
        # å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ä½ç›¸ç©ºé–“ã‚’æ§‹ç¯‰ã—æ¯”è¼ƒ
        prev_features = None
        
        for i in range(n_windows):
            start = i * stride
            end = start + window_size
            
            if end > n_frames:
                break
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ä½ç›¸ç©ºé–“
            window_series = primary_series[start:end]
            phase_space = self._reconstruct_phase_space_gpu(
                window_series, embedding_dim=3, delay=20
            )
            
            # ä½ç›¸ç©ºé–“ã®ç‰¹å¾´æŠ½å‡º
            features = self._extract_phase_features_gpu(phase_space)
            
            # å‰ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ã®è·é›¢
            if prev_features is not None:
                distance = self._phase_space_distance_gpu(prev_features, features)
                phase_distances.append(float(distance))
                
                # ã‚¹ã‚³ã‚¢ã«åæ˜ 
                for j in range(start, end):
                    transition_scores[j] += distance / window_size
            
            prev_features = features
        
        # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã¨æ­£è¦åŒ–
        transition_scores = self._smooth_and_normalize_gpu(transition_scores)
        
        return {
            'transition_scores': transition_scores,
            'phase_distances': phase_distances,
            'window_size': window_size,
            'stride': stride
        }
    
    # === Private methods ===
    
    def _get_primary_series(self, structures: Dict) -> cp.ndarray:
        """ä¸»è¦ãªæ™‚ç³»åˆ—ã‚’é¸æŠ"""
        if 'rho_T' in structures:
            return self.to_gpu(structures['rho_T'])
        elif 'lambda_F_mag' in structures:
            return self.to_gpu(structures['lambda_F_mag'])
        else:
            # æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªç³»åˆ—
            for key, value in structures.items():
                if isinstance(value, np.ndarray) and len(value.shape) == 1:
                    return self.to_gpu(value)
        
        raise ValueError("No suitable time series found in structures")
    
    def _estimate_embedding_parameters_gpu(self,
                                         series: cp.ndarray,
                                         params: Dict) -> Dict:
        """æœ€é©ãªåŸ‹ã‚è¾¼ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š"""
        # ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹é…å»¶æ™‚é–“ã®æ¨å®š
        optimal_delay = self._estimate_delay_mutual_info_gpu(series)
        
        # False Nearest Neighborsæ³•ã«ã‚ˆã‚‹æ¬¡å…ƒæ¨å®š
        optimal_dim = self._estimate_dimension_fnn_gpu(
            series, optimal_delay, max_dim=10
        )
        
        return {
            'embedding_dim': int(optimal_dim),
            'delay': int(optimal_delay),
            'estimated_from': 'mutual_info_and_fnn'
        }
    
    def _reconstruct_phase_space_gpu(self,
                                   series: cp.ndarray,
                                   embedding_dim: int,
                                   delay: int) -> cp.ndarray:
        """æ™‚ç³»åˆ—ã‹ã‚‰ä½ç›¸ç©ºé–“ã‚’å†æ§‹æˆ"""
        n = len(series)
        embed_length = n - (embedding_dim - 1) * delay
        
        if embed_length <= 0:
            raise ValueError(f"Series too short for embedding: {n} < {(embedding_dim - 1) * delay}")
        
        # GPUä¸Šã§åŸ‹ã‚è¾¼ã¿
        phase_space = cp.zeros((embed_length, embedding_dim))
        
        for i in range(embedding_dim):
            phase_space[:, i] = series[i*delay:i*delay + embed_length]
        
        return phase_space
    
    def _analyze_attractor_gpu(self, phase_space: cp.ndarray) -> Dict:
        """ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ã®ç‰¹æ€§ã‚’è§£æ"""
        features = {}
        
        # 1. æ¬¡å…ƒæ¨å®šï¼ˆç›¸é–¢æ¬¡å…ƒï¼‰
        features['correlation_dimension'] = float(
            self._correlation_dimension_gpu(phase_space)
        )
        
        # 2. LyapunovæŒ‡æ•°ï¼ˆç°¡æ˜“æ¨å®šï¼‰
        features['lyapunov_exponent'] = float(
            self._estimate_lyapunov_gpu(phase_space)
        )
        
        # 3. ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ã®ä½“ç©
        features['attractor_volume'] = float(
            self._compute_attractor_volume_gpu(phase_space)
        )
        
        # 4. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ€§
        features['fractal_measure'] = float(
            self._compute_fractal_measure_gpu(phase_space)
        )
        
        return features
    
    def _recurrence_analysis_gpu(self,
                                phase_space: cp.ndarray,
                                threshold: float) -> Dict:
        """ãƒªã‚«ãƒ¬ãƒ³ã‚¹ãƒ—ãƒ­ãƒƒãƒˆè§£æ"""
        n = len(phase_space)
        
        # è·é›¢è¡Œåˆ—ã®è¨ˆç®—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚åˆ†å‰²ï¼‰
        max_size = 5000
        if n > max_size:
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            indices = cp.random.choice(n, max_size, replace=False)
            phase_space_sample = phase_space[indices]
            n = max_size
        else:
            phase_space_sample = phase_space
        
        # ãƒªã‚«ãƒ¬ãƒ³ã‚¹è¡Œåˆ—
        recurrence_matrix = self._compute_recurrence_matrix_gpu(
            phase_space_sample, threshold
        )
        
        # ãƒªã‚«ãƒ¬ãƒ³ã‚¹å®šé‡åŒ–è§£æï¼ˆRQAï¼‰
        features = {
            'recurrence_rate': float(cp.mean(recurrence_matrix)),
            'determinism': float(self._compute_determinism_gpu(recurrence_matrix)),
            'laminarity': float(self._compute_laminarity_gpu(recurrence_matrix)),
            'entropy': float(self._recurrence_entropy_gpu(recurrence_matrix))
        }
        
        return features
    
    def _detect_anomalous_trajectories_gpu(self,
                                         phase_space: cp.ndarray,
                                         n_neighbors: int) -> cp.ndarray:
        """ç•°å¸¸è»Œé“ã®æ¤œå‡º"""
        n = len(phase_space)
        anomaly_scores = cp.zeros(n)
        
        # k-NNç•°å¸¸æ¤œå‡º
        threads = 256
        blocks = (n + threads - 1) // threads
        
        self._knn_trajectory_anomaly_kernel[blocks, threads](
            phase_space, anomaly_scores, n_neighbors, n, phase_space.shape[1]
        )
        
        # è»Œé“ã®æ›²ç‡ç•°å¸¸
        curvature_anomaly = self._detect_curvature_anomaly_gpu(phase_space)
        
        # é€Ÿåº¦ç•°å¸¸
        velocity_anomaly = self._detect_velocity_anomaly_gpu(phase_space)
        
        # çµ±åˆ
        anomaly_scores = 0.5 * anomaly_scores + 0.3 * curvature_anomaly + 0.2 * velocity_anomaly
        
        return anomaly_scores
    
    def _analyze_dynamics_gpu(self, phase_space: cp.ndarray) -> Dict:
        """ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ç‰¹æ€§ã®è§£æ"""
        features = {}
        
        # 1. å‘¨æœŸæ€§ã®æ¤œå‡º
        features['periodicity'] = float(
            self._detect_periodicity_phase_space_gpu(phase_space)
        )
        
        # 2. ã‚«ã‚ªã‚¹æ€§ã®æŒ‡æ¨™
        features['chaos_measure'] = float(
            self._compute_chaos_measure_gpu(phase_space)
        )
        
        # 3. å®‰å®šæ€§
        features['stability'] = float(
            self._compute_stability_gpu(phase_space)
        )
        
        # 4. è¤‡é›‘æ€§
        features['complexity'] = float(
            self._compute_complexity_gpu(phase_space)
        )
        
        return features
    
    def _compute_integrated_score_gpu(self,
                                    anomaly_scores: cp.ndarray,
                                    attractor_features: Dict,
                                    recurrence_features: Dict) -> cp.ndarray:
        """çµ±åˆç•°å¸¸ã‚¹ã‚³ã‚¢"""
        # ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ç•°å¸¸åº¦
        attractor_anomaly = (
            abs(attractor_features['lyapunov_exponent']) * 0.3 +
            (1 - attractor_features['fractal_measure']) * 0.2 +
            attractor_features['correlation_dimension'] * 0.1
        )
        
        # ãƒªã‚«ãƒ¬ãƒ³ã‚¹ç•°å¸¸åº¦
        recurrence_anomaly = (
            (1 - recurrence_features['determinism']) * 0.4 +
            recurrence_features['entropy'] * 0.3 +
            (1 - recurrence_features['laminarity']) * 0.3
        )
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ç•°å¸¸åº¦ã‚’å„æ™‚ç‚¹ã«åæ˜ 
        global_anomaly = (attractor_anomaly + recurrence_anomaly) / 2
        
        # æ™‚ç³»åˆ—å…¨ä½“ã«é©ç”¨
        integrated = anomaly_scores + global_anomaly
        
        return integrated
    
    # === è£œåŠ©GPUé–¢æ•° ===
    
    def _estimate_delay_mutual_info_gpu(self, series: cp.ndarray) -> int:
        """ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹é…å»¶æ¨å®š"""
        max_delay = min(100, len(series) // 10)
        mi_values = cp.zeros(max_delay)
        
        for delay in range(1, max_delay):
            # ç°¡æ˜“çš„ãªç›¸äº’æƒ…å ±é‡è¨ˆç®—
            x = series[:-delay]
            y = series[delay:]
            
            # ãƒ“ãƒ‹ãƒ³ã‚°
            n_bins = 20
            hist_2d = cp.histogram2d(x, y, bins=n_bins)[0]
            
            # ç›¸äº’æƒ…å ±é‡
            pxy = hist_2d / cp.sum(hist_2d)
            px = cp.sum(pxy, axis=1)
            py = cp.sum(pxy, axis=0)
            
            # MIè¨ˆç®—
            mi = 0.0
            for i in range(n_bins):
                for j in range(n_bins):
                    if pxy[i, j] > 0 and px[i] > 0 and py[j] > 0:
                        mi += pxy[i, j] * cp.log(pxy[i, j] / (px[i] * py[j]))
            
            mi_values[delay] = mi
        
        # æœ€åˆã®æ¥µå°å€¤
        for i in range(2, len(mi_values)-1):
            if mi_values[i] < mi_values[i-1] and mi_values[i] < mi_values[i+1]:
                return i
        
        return 10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _estimate_dimension_fnn_gpu(self,
                                  series: cp.ndarray,
                                  delay: int,
                                  max_dim: int) -> int:
        """False Nearest Neighborsæ³•ã«ã‚ˆã‚‹æ¬¡å…ƒæ¨å®š"""
        fnn_fractions = []
        
        for dim in range(1, max_dim):
            # åŸ‹ã‚è¾¼ã¿
            try:
                phase_space = self._reconstruct_phase_space_gpu(series, dim, delay)
                
                # FNNè¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                n = min(1000, len(phase_space))  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                fnn_count = 0
                
                for i in range(n):
                    # æœ€è¿‘å‚ã‚’è¦‹ã¤ã‘ã‚‹
                    distances = cp.sqrt(cp.sum((phase_space - phase_space[i])**2, axis=1))
                    distances[i] = cp.inf
                    nn_idx = cp.argmin(distances)
                    
                    # æ¬¡å…ƒã‚’å¢—ã‚„ã—ãŸæ™‚ã®è·é›¢
                    if dim < len(series) - delay * dim:
                        next_dim_dist = abs(
                            series[i + dim * delay] - series[nn_idx + dim * delay]
                        )
                        
                        if next_dim_dist / distances[nn_idx] > 10:
                            fnn_count += 1
                
                fnn_fraction = fnn_count / n
                fnn_fractions.append(float(fnn_fraction))
                
                # FNNãŒååˆ†å°ã•ããªã£ãŸã‚‰
                if fnn_fraction < 0.01:
                    return dim
            
            except:
                break
        
        return 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _correlation_dimension_gpu(self, phase_space: cp.ndarray) -> float:
        """ç›¸é–¢æ¬¡å…ƒã®è¨ˆç®—"""
        n = min(1000, len(phase_space))
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(phase_space) > n:
            indices = cp.random.choice(len(phase_space), n, replace=False)
            phase_space = phase_space[indices]
        
        # è·é›¢è¨ˆç®—
        radii = cp.logspace(-2, 1, 20)
        correlation_sums = []
        
        for r in radii:
            count = 0
            for i in range(n):
                distances = cp.sqrt(cp.sum((phase_space - phase_space[i])**2, axis=1))
                count += cp.sum(distances < r) - 1  # è‡ªåˆ†è‡ªèº«ã‚’é™¤ã
            
            correlation_sum = count / (n * (n - 1))
            if correlation_sum > 0:
                correlation_sums.append((float(cp.log(r)), float(cp.log(correlation_sum))))
        
        # ç·šå½¢ãƒ•ã‚£ãƒƒãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if len(correlation_sums) > 2:
            x = cp.array([c[0] for c in correlation_sums])
            y = cp.array([c[1] for c in correlation_sums])
            
            # æœ€å°äºŒä¹—æ³•
            slope = cp.sum((x - cp.mean(x)) * (y - cp.mean(y))) / cp.sum((x - cp.mean(x))**2)
            
            return float(slope)
        
        return 2.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _estimate_lyapunov_gpu(self, phase_space: cp.ndarray) -> float:
        """LyapunovæŒ‡æ•°ã®ç°¡æ˜“æ¨å®š"""
        n = len(phase_space)
        lyap_sum = 0.0
        count = 0
        
        # è¿‘å‚ç‚¹ã®ç™ºæ•£ç‡ã‚’è¨ˆç®—
        for i in range(min(100, n-10)):
            # è¿‘å‚ç‚¹ã‚’æ¢ã™
            distances = cp.sqrt(cp.sum((phase_space - phase_space[i])**2, axis=1))
            distances[i] = cp.inf
            
            # å°ã•ãªåŠå¾„å†…ã®ç‚¹
            radius = cp.std(distances) * 0.1
            neighbors = cp.where(distances < radius)[0]
            
            if len(neighbors) > 0:
                for j in neighbors[:5]:  # æœ€å¤§5ã¤
                    # æ™‚é–“ç™ºå±•å¾Œã®è·é›¢
                    for dt in range(1, min(10, n-max(i,int(j)))):
                        d0 = float(distances[j])
                        d1 = float(cp.sqrt(cp.sum((phase_space[i+dt] - phase_space[j+dt])**2)))
                        
                        if d0 > 0 and d1 > 0:
                            lyap_sum += cp.log(d1 / d0) / dt
                            count += 1
        
        return float(lyap_sum / count) if count > 0 else 0.0
    
    def _compute_recurrence_matrix_gpu(self,
                                     phase_space: cp.ndarray,
                                     threshold: float) -> cp.ndarray:
        """ãƒªã‚«ãƒ¬ãƒ³ã‚¹è¡Œåˆ—ã®è¨ˆç®—"""
        n = len(phase_space)
        
        # è·é›¢è¡Œåˆ—
        distances = distance_matrix(phase_space, phase_space)
        
        # é–¾å€¤å‡¦ç†
        recurrence_matrix = (distances < threshold).astype(cp.float32)
        
        return recurrence_matrix
    
    def _smooth_and_normalize_gpu(self, scores: cp.ndarray) -> cp.ndarray:
        """ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã¨æ­£è¦åŒ–"""
        from cupyx.scipy.ndimage import gaussian_filter1d as gaussian_filter1d_gpu
        
        # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        smoothed = gaussian_filter1d_gpu(scores, sigma=10)
        
        # æ­£è¦åŒ–
        if cp.std(smoothed) > 1e-10:
            normalized = (smoothed - cp.mean(smoothed)) / cp.std(smoothed)
        else:
            normalized = smoothed
        
        return normalized
    
    def _print_summary(self, results: Dict):
        """è§£æçµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ“Š Phase Space Analysis Summary:")
        print(f"   Embedding dimension: {results['optimal_parameters']['embedding_dim']}")
        print(f"   Delay: {results['optimal_parameters']['delay']}")
        
        if 'attractor_features' in results:
            att = results['attractor_features']
            print(f"\n   Attractor Features:")
            print(f"   - Correlation dimension: {att['correlation_dimension']:.3f}")
            print(f"   - Lyapunov exponent: {att['lyapunov_exponent']:.3f}")
            print(f"   - Fractal measure: {att['fractal_measure']:.3f}")
        
        if 'recurrence_features' in results:
            rec = results['recurrence_features']
            print(f"\n   Recurrence Features:")
            print(f"   - Recurrence rate: {rec['recurrence_rate']:.3f}")
            print(f"   - Determinism: {rec['determinism']:.3f}")
            print(f"   - Entropy: {rec['entropy']:.3f}")
    
    # === CUDAã‚«ãƒ¼ãƒãƒ« ===
    
    @cuda.jit
    def _knn_trajectory_anomaly_kernel(phase_space, anomaly_scores, k, n, dim):
        """k-NNè»Œé“ç•°å¸¸æ¤œå‡ºã‚«ãƒ¼ãƒãƒ«"""
        idx = cuda.grid(1)
        
        if idx < n:
            # ç°¡æ˜“çš„ãªk-NNï¼ˆå®Ÿéš›ã¯ã‚‚ã£ã¨åŠ¹ç‡çš„ãªå®Ÿè£…ãŒå¿…è¦ï¼‰
            distances = cuda.local.array(100, dtype=cuda.float32)
            
            # è·é›¢è¨ˆç®—
            for j in range(min(100, n)):
                if j != idx:
                    dist = 0.0
                    for d in range(dim):
                        diff = phase_space[j, d] - phase_space[idx, d]
                        dist += diff * diff
                    distances[j] = cuda.sqrt(dist)
                else:
                    distances[j] = 1e10
            
            # kè¿‘å‚ã®å¹³å‡
            knn_sum = 0.0
            for _ in range(k):
                min_idx = 0
                min_dist = distances[0]
                for j in range(1, min(100, n)):
                    if distances[j] < min_dist:
                        min_dist = distances[j]
                        min_idx = j
                knn_sum += min_dist
                distances[min_idx] = 1e10
            
            anomaly_scores[idx] = knn_sum / k
    
    # ä»–ã®è£œåŠ©ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆç°¡ç•¥åŒ–ï¼‰
    def _compute_attractor_volume_gpu(self, phase_space: cp.ndarray) -> float:
        """ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ä½“ç©ã®æ¨å®š"""
        return float(cp.prod(cp.max(phase_space, axis=0) - cp.min(phase_space, axis=0)))
    
    def compute_fractal_measure_gpu(self, phase_space: cp.ndarray) -> float:
        """
        ç›¸é–¢æ¬¡å…ƒã«ã‚ˆã‚‹ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¸¬åº¦ï¼ˆGrassberger-Procacciaæ³•ï¼‰
        """
        n_points = min(1000, len(phase_space))  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if n_points < 100:
            return 1.5  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        indices = cp.random.choice(len(phase_space), n_points, replace=False)
        sample = phase_space[indices]
        
        # è·é›¢è¡Œåˆ—ï¼ˆä¸Šä¸‰è§’ã®ã¿ï¼‰
        distances = []
        for i in range(n_points - 1):
            dists = cp.linalg.norm(sample[i+1:] - sample[i], axis=1)
            distances.append(dists)
        
        all_distances = cp.concatenate(distances)
        all_distances = all_distances[all_distances > 0]
        
        if len(all_distances) == 0:
            return 1.5
        
        # ç›¸é–¢ç©åˆ†C(r)ã‚’è¨ˆç®—
        r_values = cp.logspace(
            cp.log10(cp.min(all_distances)),
            cp.log10(cp.max(all_distances)),
            20
        )
        
        correlation_sum = cp.zeros(len(r_values))
        for i, r in enumerate(r_values):
            correlation_sum[i] = cp.sum(all_distances < r)
        
        # log-logå‹¾é…ã‹ã‚‰æ¬¡å…ƒæ¨å®š
        valid = correlation_sum > 0
        if cp.sum(valid) > 5:
            log_r = cp.log(r_values[valid])
            log_c = cp.log(correlation_sum[valid] / (n_points * (n_points - 1) / 2))
            
            # ä¸­é–“é ˜åŸŸã§å‹¾é…è¨ˆç®—
            mid_idx = len(log_r) // 2
            slope = cp.polyfit(
                log_r[mid_idx-2:mid_idx+3],
                log_c[mid_idx-2:mid_idx+3],
                1
            )[0]
            
            return float(cp.clip(slope, 0.5, 3.0))
        
        return 1.5  # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰...
    
    def _compute_determinism_gpu(self, rec_matrix: cp.ndarray) -> float:
        """æ±ºå®šè«–æ€§ã®è¨ˆç®—"""
        # å¯¾è§’ç·šæ§‹é€ ã®å‰²åˆ
        return float(cp.mean(cp.diag(rec_matrix, k=1)))
    
    def _compute_laminarity_gpu(self, rec_matrix: cp.ndarray) -> float:
        """å±¤æµæ€§ã®è¨ˆç®—"""
        # å‚ç›´/æ°´å¹³æ§‹é€ ã®å‰²åˆ
        return float(cp.mean(rec_matrix))
    
    def _recurrence_entropy_gpu(self, rec_matrix: cp.ndarray) -> float:
        """ãƒªã‚«ãƒ¬ãƒ³ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼"""
        # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®ç°¡æ˜“è¨ˆç®—
        p = cp.mean(rec_matrix)
        if p > 0 and p < 1:
            return float(-p * cp.log(p) - (1-p) * cp.log(1-p))
        return 0.0
    
    def _extract_phase_features_gpu(self, phase_space: cp.ndarray) -> Dict:
        """ä½ç›¸ç©ºé–“ã®ç‰¹å¾´æŠ½å‡º"""
        return {
            'center': cp.mean(phase_space, axis=0),
            'spread': cp.std(phase_space, axis=0),
            'volume': self._compute_attractor_volume_gpu(phase_space)
        }
    
    def _phase_space_distance_gpu(self, features1: Dict, features2: Dict) -> float:
        """ä½ç›¸ç©ºé–“ç‰¹å¾´é–“ã®è·é›¢"""
        dist = cp.linalg.norm(features1['center'] - features2['center'])
        dist += cp.linalg.norm(features1['spread'] - features2['spread'])
        dist += abs(features1['volume'] - features2['volume']) / (features1['volume'] + 1e-10)
        return float(dist)
    
    def _detect_curvature_anomaly_gpu(self, phase_space: cp.ndarray) -> cp.ndarray:
        """æ›²ç‡ç•°å¸¸ã®æ¤œå‡º"""
        n = len(phase_space)
        curvature = cp.zeros(n)
        
        for i in range(1, n-1):
            # 3ç‚¹ã§ã®æ›²ç‡è¿‘ä¼¼
            v1 = phase_space[i] - phase_space[i-1]
            v2 = phase_space[i+1] - phase_space[i]
            
            # è§’åº¦å¤‰åŒ–
            cos_angle = cp.dot(v1, v2) / (cp.linalg.norm(v1) * cp.linalg.norm(v2) + 1e-10)
            curvature[i] = 1 - cos_angle
        
        return curvature
    
    def _detect_velocity_anomaly_gpu(self, phase_space: cp.ndarray) -> cp.ndarray:
        """é€Ÿåº¦ç•°å¸¸ã®æ¤œå‡º"""
        velocity = cp.sqrt(cp.sum(cp.diff(phase_space, axis=0)**2, axis=1))
        velocity = cp.pad(velocity, (1, 0), mode='edge')
        
        # æ­£è¦åŒ–
        mean_vel = cp.mean(velocity)
        std_vel = cp.std(velocity)
        
        if std_vel > 1e-10:
            return cp.abs(velocity - mean_vel) / std_vel
        else:
            return cp.zeros_like(velocity)
    
    def _detect_periodicity_phase_space_gpu(self, phase_space: cp.ndarray) -> float:
        """ä½ç›¸ç©ºé–“ã§ã®å‘¨æœŸæ€§æ¤œå‡º"""
        # ç°¡æ˜“ç‰ˆï¼šè‡ªå·±ç›¸é–¢
        n = len(phase_space)
        max_lag = min(n // 2, 1000)
        
        autocorr = []
        for lag in range(1, max_lag, 10):
            if lag < n:
                corr = cp.mean(cp.sum(phase_space[:-lag] * phase_space[lag:], axis=1))
                autocorr.append(float(corr))
        
        # å‘¨æœŸæ€§ã®å¼·ã•
        if autocorr:
            return float(cp.max(cp.array(autocorr)))
        return 0.0
    
    def _compute_chaos_measure_gpu(self, phase_space: cp.ndarray) -> float:
        """ã‚«ã‚ªã‚¹æ€§ã®æ¸¬å®š"""
        # LyapunovæŒ‡æ•°ã®çµ¶å¯¾å€¤ã‚’ä½¿ç”¨
        lyap = abs(self._estimate_lyapunov_gpu(phase_space))
        return float(1 / (1 + cp.exp(-lyap)))  # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›
    
    def _compute_stability_gpu(self, phase_space: cp.ndarray) -> float:
        """å®‰å®šæ€§ã®æ¸¬å®š"""
        # è»Œé“ã®åˆ†æ•£
        variance = cp.var(phase_space, axis=0)
        return float(1 / (1 + cp.mean(variance)))
    
    def _compute_complexity_gpu(self, phase_space: cp.ndarray) -> float:
        """è¤‡é›‘æ€§ã®æ¸¬å®š"""
        # ç›¸é–¢æ¬¡å…ƒã‚’æ­£è¦åŒ–
        corr_dim = self._correlation_dimension_gpu(phase_space)
        return float(corr_dim / phase_space.shape[1])
