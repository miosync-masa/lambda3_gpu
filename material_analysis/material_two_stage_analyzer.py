#!/usr/bin/env python3
"""
Material Two-Stage LambdaÂ³ Analyzer GPU
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ææ–™è§£æç”¨2æ®µéšLambdaÂ³è§£æã®GPUå®Ÿè£…
ãƒã‚¯ãƒ­ãƒ¬ãƒ™ãƒ«â†’ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ã®éšå±¤çš„è§£æã§è»¢ä½ãƒ»äº€è£‚ã‚’è¿½è·¡ï¼ğŸ’

MDç‰ˆã‚’ãƒ™ãƒ¼ã‚¹ã«ææ–™ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è§£æã«ç‰¹åŒ–

by ç’°ã¡ã‚ƒã‚“ - Material Edition v1.0
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import numpy as np
import time
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
import logging
import warnings

logger = logging.getLogger('lambda3_gpu.material_analysis')
warnings.filterwarnings('ignore')

# CuPyã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import cupy as cp
    HAS_CUPY = True
except ImportError:
    cp = None
    HAS_CUPY = False

from concurrent.futures import ThreadPoolExecutor

# LambdaÂ³ GPU imports
from lambda3_gpu.core.gpu_utils import GPUBackend
from lambda3_gpu.material.cluster_structures_gpu import ClusterStructuresGPU, ClusterStructureResult
from lambda3_gpu.material.cluster_network_gpu import ClusterNetworkGPU
from lambda3_gpu.material.cluster_causality_analysis_gpu import MaterialCausalityAnalyzerGPU
from lambda3_gpu.material.cluster_confidence_analysis_gpu import MaterialConfidenceAnalyzerGPU
from lambda3_gpu.material_analysis.material_lambda3_detector import MaterialLambda3Result
from lambda3_gpu.types import ArrayType, NDArray

# ===============================
# Configuration
# ===============================

@dataclass
class ClusterAnalysisConfig:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«è§£æã®è¨­å®š"""
    # è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆææ–™ç”¨ã«èª¿æ•´ï¼‰
    sensitivity: float = 0.3  # ææ–™ã¯ä½æ„Ÿåº¦
    correlation_threshold: float = 0.2  # ç›¸é–¢é–¾å€¤
    sync_threshold: float = 0.3  # åŒæœŸé–¾å€¤
    
    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆææ–™ã¯çŸ­ã„æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    min_window: int = 10  # æœ€å°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    max_window: int = 100  # æœ€å¤§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    base_window: int = 30  # åŸºæœ¬ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    base_lag_window: int = 50  # ãƒ©ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ¶ç´„
    max_causal_links: int = 200  # å› æœãƒªãƒ³ã‚¯æ•°ä¸Šé™
    min_causality_strength: float = 0.3  # æœ€å°å› æœå¼·åº¦
    
    # ææ–™ç‰¹æœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    detect_dislocations: bool = True  # è»¢ä½æ¤œå‡º
    detect_cracks: bool = True  # äº€è£‚æ¤œå‡º
    detect_phase_transitions: bool = True  # ç›¸å¤‰æ…‹æ¤œå‡º
    
    # Bootstrap ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    use_confidence: bool = True
    n_bootstrap: int = 30  # ææ–™ã¯å°‘ãªã‚
    confidence_level: float = 0.95
    
    # GPUè¨­å®š
    gpu_batch_clusters: int = 30  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒƒãƒã‚µã‚¤ã‚º
    parallel_events: bool = True
    adaptive_window: bool = True
    
    # ã‚¤ãƒ™ãƒ³ãƒˆå›ºæœ‰è¨­å®šï¼ˆææ–™ã‚¤ãƒ™ãƒ³ãƒˆï¼‰
    event_sensitivities: Dict[str, float] = field(default_factory=lambda: {
        'elastic_deformation': 0.2,
        'plastic_deformation': 0.4,
        'dislocation_nucleation': 0.5,
        'crack_initiation': 0.6,
        'crack_propagation': 0.5,
        'phase_transition': 0.7,
        'fatigue_damage': 0.4
    })
    
    event_windows: Dict[str, int] = field(default_factory=lambda: {
        'elastic_deformation': 20,
        'plastic_deformation': 50,
        'dislocation_nucleation': 30,
        'crack_initiation': 40,
        'crack_propagation': 30,
        'phase_transition': 100,
        'fatigue_damage': 80
    })

@dataclass
class ClusterEvent:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆ"""
    cluster_id: int
    cluster_name: str
    start_frame: int
    end_frame: int
    peak_strain: float  # æœ€å¤§æ­ªã¿
    peak_damage: float  # æœ€å¤§æå‚·åº¦
    propagation_delay: int
    role: str  # 'initiator', 'mediator', 'responder'
    adaptive_window: int = 50
    
    # ææ–™ç‰¹æœ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    dislocation_density: Optional[float] = None
    coordination_defect: Optional[float] = None
    von_mises_stress: Optional[float] = None

@dataclass
class ClusterLevelAnalysis:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«è§£æçµæœ"""
    event_name: str
    macro_start: int
    macro_end: int
    cluster_events: List[ClusterEvent] = field(default_factory=list)
    causality_chain: List[Tuple[int, int, float]] = field(default_factory=list)
    initiator_clusters: List[int] = field(default_factory=list)
    propagation_paths: List[List[int]] = field(default_factory=list)
    
    # ææ–™ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    strain_network: List[Dict] = field(default_factory=list)
    dislocation_network: List[Dict] = field(default_factory=list)
    damage_network: List[Dict] = field(default_factory=list)
    
    network_stats: Dict = field(default_factory=dict)
    confidence_results: List[Any] = field(default_factory=list)
    
    # ææ–™ç‰¹æ€§
    failure_probability: float = 0.0
    reliability_index: float = 5.0
    critical_stress_intensity: Optional[float] = None
    
    gpu_time: float = 0.0

@dataclass
class MaterialTwoStageResult:
    """ææ–™2æ®µéšè§£æã®çµ±åˆçµæœ"""
    macro_result: MaterialLambda3Result
    cluster_analyses: Dict[str, ClusterLevelAnalysis]
    global_cluster_importance: Dict[int, float]
    critical_clusters: List[int]  # ç ´å£Šå±é™ºã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
    suggested_reinforcement_points: List[int]  # è£œå¼·æ¨å¥¨ç®‡æ‰€
    global_network_stats: Dict
    material_state: Dict  # ææ–™çŠ¶æ…‹ï¼ˆå¥å…¨/æå‚·/ç ´å£Šï¼‰
    total_gpu_time: float = 0.0

# ===============================
# Material Two-Stage Analyzer
# ===============================

class MaterialTwoStageAnalyzerGPU(GPUBackend):
    """ææ–™ç”¨GPUç‰ˆ2æ®µéšè§£æå™¨"""
    
    def __init__(self, config: ClusterAnalysisConfig = None, material_type: str = 'SUJ2'):
        super().__init__()
        self.config = config or ClusterAnalysisConfig()
        self.material_type = material_type
        
        # ææ–™ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£è¨­å®š
        self._set_material_properties()
        
        # GPUç‰ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.cluster_structures = ClusterStructuresGPU(**self.material_props)
        self.cluster_network = ClusterNetworkGPU(**self.material_props)
        self.causality_analyzer = MaterialCausalityAnalyzerGPU(**self.material_props)
        self.confidence_analyzer = MaterialConfidenceAnalyzerGPU(**self.material_props)
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£å…±æœ‰
        for component in [self.cluster_structures, self.cluster_network,
                         self.causality_analyzer, self.confidence_analyzer]:
            if hasattr(component, 'memory_manager'):
                component.memory_manager = self.memory_manager
            if hasattr(component, 'device'):
                component.device = self.device
    
    def _set_material_properties(self):
        """ææ–™ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£è¨­å®š"""
        if self.material_type == 'SUJ2':
            self.material_props = {
                'elastic_modulus': 210.0,  # GPa
                'yield_strength': 1.5,
                'ultimate_strength': 2.0,
                'fatigue_strength': 0.7,
                'fracture_toughness': 30.0,
                'poisson_ratio': 0.3
            }
        elif self.material_type == 'AL7075':
            self.material_props = {
                'elastic_modulus': 71.7,
                'yield_strength': 0.503,
                'ultimate_strength': 0.572,
                'fatigue_strength': 0.159,
                'fracture_toughness': 23.0,
                'poisson_ratio': 0.33
            }
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆé‹¼é‰„ï¼‰
            self.material_props = {
                'elastic_modulus': 210.0,
                'yield_strength': 1.0,
                'ultimate_strength': 1.5,
                'fatigue_strength': 0.5,
                'fracture_toughness': 20.0,
                'poisson_ratio': 0.3
            }
    
    def analyze_trajectory(self,
                          trajectory: np.ndarray,
                          macro_result: MaterialLambda3Result,
                          detected_events: List[Tuple[int, int, str]],
                          cluster_atoms: Dict[int, List[int]],
                          atom_types: np.ndarray) -> MaterialTwoStageResult:
        """
        ææ–™2æ®µéšLambdaÂ³è§£æã®å®Ÿè¡Œ
        
        Parameters
        ----------
        trajectory : np.ndarray
            åŸå­ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒª (n_frames, n_atoms, 3)
        macro_result : MaterialLambda3Result
            ãƒã‚¯ãƒ­ãƒ¬ãƒ™ãƒ«è§£æçµæœ
        detected_events : List[Tuple[int, int, str]]
            æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ [(start, end, event_type), ...]
        cluster_atoms : Dict[int, List[int]]
            ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å®šç¾©
        atom_types : np.ndarray
            åŸå­ã‚¿ã‚¤ãƒ—é…åˆ—
            
        Returns
        -------
        MaterialTwoStageResult
            çµ±åˆè§£æçµæœ
        """
        start_time = time.time()
        
        n_clusters = len(cluster_atoms)
        
        print("\n" + "="*60)
        print("=== Material Two-Stage LambdaÂ³ Analysis (GPU) ===")
        print("="*60)
        print(f"Material: {self.material_type}")
        print(f"Events to analyze: {len(detected_events)}")
        print(f"Number of clusters: {n_clusters}")
        print(f"GPU Device: {self.device}")
        
        # å…¥åŠ›æ¤œè¨¼
        if len(trajectory.shape) != 3:
            raise ValueError(f"Expected 3D trajectory, got shape {trajectory.shape}")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åè¨­å®š
        cluster_names = {i: f"CLUSTER_{i}" for i in range(n_clusters)}
        
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã‚’è§£æ
        cluster_analyses = {}
        all_important_clusters = {}
        
        # ä¸¦åˆ—å‡¦ç†
        if self.config.parallel_events and len(detected_events) > 1:
            cluster_analyses = self._analyze_events_parallel(
                trajectory, detected_events, cluster_atoms,
                cluster_names, atom_types
            )
        else:
            cluster_analyses = self._analyze_events_sequential(
                trajectory, detected_events, cluster_atoms,
                cluster_names, atom_types
            )
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¦åº¦è¨ˆç®—
        for event_name, analysis in cluster_analyses.items():
            for event in analysis.cluster_events:
                cluster_id = event.cluster_id
                if cluster_id not in all_important_clusters:
                    all_important_clusters[cluster_id] = 0.0
                
                # ææ–™é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆæ­ªã¿ã¨æå‚·ã‚’è€ƒæ…®ï¼‰
                importance = (
                    event.peak_strain * self.material_props['elastic_modulus'] +
                    event.peak_damage * 10.0
                )
                all_important_clusters[cluster_id] += importance
        
        # è‡¨ç•Œã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å®š
        critical_clusters = self._identify_critical_clusters(
            all_important_clusters, cluster_analyses
        )
        
        # è£œå¼·æ¨å¥¨ç®‡æ‰€
        reinforcement_points = self._identify_reinforcement_points(
            all_important_clusters, cluster_analyses
        )
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆ
        global_stats = self._compute_global_stats(cluster_analyses)
        
        # ææ–™çŠ¶æ…‹è©•ä¾¡
        material_state = self._evaluate_material_state(
            cluster_analyses, critical_clusters
        )
        
        # çµæœã‚µãƒãƒªãƒ¼
        self._print_summary(
            all_important_clusters, critical_clusters,
            reinforcement_points, material_state
        )
        
        total_time = time.time() - start_time
        
        return MaterialTwoStageResult(
            macro_result=macro_result,
            cluster_analyses=cluster_analyses,
            global_cluster_importance=all_important_clusters,
            critical_clusters=critical_clusters,
            suggested_reinforcement_points=reinforcement_points,
            global_network_stats=global_stats,
            material_state=material_state,
            total_gpu_time=total_time
        )
    
    def _analyze_events_parallel(self,
                                trajectory: np.ndarray,
                                detected_events: List[Tuple[int, int, str]],
                                cluster_atoms: Dict[int, List[int]],
                                cluster_names: Dict[int, str],
                                atom_types: np.ndarray) -> Dict[str, ClusterLevelAnalysis]:
        """ã‚¤ãƒ™ãƒ³ãƒˆã®ä¸¦åˆ—è§£æ"""
        print("\nâš¡ Processing material events in parallel on GPU...")
        
        cluster_analyses = {}
        
        with ThreadPoolExecutor(max_workers=min(4, len(detected_events))) as executor:
            futures = []
            
            for start, end, event_name in detected_events:
                future = executor.submit(
                    self._analyze_single_event_gpu,
                    trajectory, event_name, start, end,
                    cluster_atoms, cluster_names, atom_types
                )
                futures.append((event_name, future))
            
            for event_name, future in futures:
                try:
                    analysis = future.result()
                    cluster_analyses[event_name] = analysis
                    print(f"  âœ“ {event_name} complete (GPU time: {analysis.gpu_time:.2f}s)")
                except Exception as e:
                    logger.error(f"  âœ— {event_name} failed: {str(e)}")
        
        return cluster_analyses
    
    def _analyze_events_sequential(self,
                                  trajectory: np.ndarray,
                                  detected_events: List[Tuple[int, int, str]],
                                  cluster_atoms: Dict[int, List[int]],
                                  cluster_names: Dict[int, str],
                                  atom_types: np.ndarray) -> Dict[str, ClusterLevelAnalysis]:
        """ã‚¤ãƒ™ãƒ³ãƒˆã®é€æ¬¡è§£æ"""
        print("\nâš™ï¸ Processing material events sequentially on GPU...")
        
        cluster_analyses = {}
        
        for start, end, event_name in detected_events:
            print(f"\n  â†’ Analyzing {event_name}...")
            analysis = self._analyze_single_event_gpu(
                trajectory, event_name, start, end,
                cluster_atoms, cluster_names, atom_types
            )
            cluster_analyses[event_name] = analysis
            print(f"    GPU time: {analysis.gpu_time:.2f}s")
        
        return cluster_analyses
    
    def _analyze_single_event_gpu(self,
                                 trajectory: np.ndarray,
                                 event_name: str,
                                 start_frame: int,
                                 end_frame: int,
                                 cluster_atoms: Dict[int, List[int]],
                                 cluster_names: Dict[int, str],
                                 atom_types: np.ndarray) -> ClusterLevelAnalysis:
        """å˜ä¸€ã‚¤ãƒ™ãƒ³ãƒˆã®GPUè§£æ"""
        event_start_time = time.time()
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²æ¤œè¨¼
        if end_frame <= start_frame:
            end_frame = min(start_frame + 1, trajectory.shape[0])
        
        event_frames = end_frame - start_frame
        event_trajectory = trajectory[start_frame:end_frame]
        
        with self.memory_manager.batch_context(event_frames * len(cluster_atoms) * 3 * 4):
            
            # 1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹é€ è¨ˆç®—
            structures = self.cluster_structures.compute_cluster_structures(
                event_trajectory, 0, event_frames - 1,
                cluster_atoms, atom_types,
                window_size=self.config.base_window
            )
            
            # 2. ç•°å¸¸æ¤œå‡º
            anomaly_scores = self._detect_cluster_anomalies_gpu(
                structures, event_name
            )
            
            # 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ
            network_results = self.cluster_network.analyze_network(
                anomaly_scores,
                structures.cluster_coupling,
                structures.cluster_centers,
                structures.coordination_numbers,
                structures.local_strain
            )
            
            # 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆæ§‹ç¯‰
            cluster_events = self._build_cluster_events_gpu(
                structures, anomaly_scores, cluster_names,
                start_frame, network_results
            )
            
            # 5. å› æœè§£æ
            causality_chains = []
            initiators = []
            propagation_paths = []
            
            if self.config.detect_dislocations and network_results.dislocation_network:
                # è»¢ä½ä¼æ’­è§£æ
                causality_result = self.causality_analyzer.detect_causal_pairs(
                    anomaly_scores,
                    structures.coordination_numbers,
                    structures.local_strain,
                    threshold=self.config.min_causality_strength
                )
                
                causality_chains = [
                    (r.pair[0], r.pair[1], r.causality_strength)
                    for r in causality_result[:self.config.max_causal_links]
                ]
                
                initiators = self._find_dislocation_sources(
                    cluster_events, causality_chains
                )
                
                propagation_paths = self._trace_dislocation_paths(
                    initiators, causality_chains
                )
            
            elif self.config.detect_cracks and network_results.damage_network:
                # äº€è£‚ä¼æ’­è§£æ
                initiators = self._find_crack_initiation_sites(
                    cluster_events, network_results.damage_network
                )
                
                propagation_paths = self._trace_crack_paths(
                    initiators, network_results.damage_network
                )
            
            # 6. ä¿¡é ¼æ€§è§£æ
            confidence_results = []
            if self.config.use_confidence and causality_chains:
                cluster_data = {
                    i: {
                        'strain': structures.local_strain[:, i],
                        'coordination': structures.coordination_numbers[:, i],
                        'anomaly': scores
                    }
                    for i, scores in anomaly_scores.items()
                }
                
                confidence_results = self.confidence_analyzer.analyze_material_reliability(
                    causality_chains[:10],
                    cluster_data,
                    analysis_type='strain',
                    n_bootstrap=self.config.n_bootstrap
                )
            
            # 7. ç ´å£Šç¢ºç‡è¨ˆç®—
            failure_prob = self._compute_failure_probability(
                structures.local_strain
            )
            
            # 8. ä¿¡é ¼æ€§æŒ‡æ¨™
            reliability_idx = self._compute_reliability_index(
                structures.local_strain
            )
        
        gpu_time = time.time() - event_start_time
        
        # çµæœæ§‹ç¯‰
        network_stats = network_results.network_stats.copy()
        network_stats.update({
            'n_strain_links': len(network_results.strain_network),
            'n_dislocation_links': len(network_results.dislocation_network),
            'n_damage_links': len(network_results.damage_network),
            'mean_coordination': float(np.mean(structures.coordination_numbers))
        })
        
        return ClusterLevelAnalysis(
            event_name=event_name,
            macro_start=start_frame,
            macro_end=end_frame,
            cluster_events=cluster_events,
            causality_chain=causality_chains,
            initiator_clusters=initiators,
            propagation_paths=propagation_paths[:5],
            strain_network=network_results.strain_network,
            dislocation_network=network_results.dislocation_network,
            damage_network=network_results.damage_network,
            network_stats=network_stats,
            confidence_results=confidence_results,
            failure_probability=failure_prob,
            reliability_index=reliability_idx,
            gpu_time=gpu_time
        )
    
    def _detect_cluster_anomalies_gpu(self,
                                     structures: ClusterStructureResult,
                                     event_type: str) -> Dict[int, np.ndarray]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç•°å¸¸æ¤œå‡º"""
        n_frames, n_clusters = structures.cluster_rho_t.shape
        
        sensitivity = self.config.event_sensitivities.get(
            event_type, self.config.sensitivity
        )
        
        cluster_anomaly_scores = {}
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—
        for cluster_id in range(n_clusters):
            # æ­ªã¿ç•°å¸¸
            strain_values = structures.local_strain[:, cluster_id]
            von_mises = np.sqrt(np.sum(strain_values**2, axis=-1))
            strain_anomaly = self._compute_anomaly_score(von_mises)
            
            # é…ä½æ•°ç•°å¸¸
            coord_values = structures.coordination_numbers[:, cluster_id]
            ideal_coord = 12.0 if hasattr(structures, 'crystal_structure') else 8.0
            coord_anomaly = np.abs(coord_values - ideal_coord) / ideal_coord
            
            # çµ±åˆã‚¹ã‚³ã‚¢
            combined = (strain_anomaly + coord_anomaly) / 2
            
            if np.max(combined) > sensitivity:
                cluster_anomaly_scores[cluster_id] = combined
        
        # æœ€ä½é™ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ç¢ºä¿
        if len(cluster_anomaly_scores) < min(5, n_clusters):
            for cluster_id in range(min(5, n_clusters)):
                if cluster_id not in cluster_anomaly_scores:
                    cluster_anomaly_scores[cluster_id] = np.ones(n_frames) * 0.5
        
        return cluster_anomaly_scores
    
    def _build_cluster_events_gpu(self,
                                 structures: ClusterStructureResult,
                                 anomaly_scores: Dict[int, np.ndarray],
                                 cluster_names: Dict[int, str],
                                 start_frame: int,
                                 network_results) -> List[ClusterEvent]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆæ§‹ç¯‰"""
        events = []
        
        for cluster_id, scores in anomaly_scores.items():
            # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
            peak_idx = np.argmax(scores)
            peak_score = scores[peak_idx]
            
            # æ­ªã¿ã¨æå‚·ã®å–å¾—
            strain_values = structures.local_strain[:, cluster_id]
            von_mises_strain = np.sqrt(np.sum(strain_values**2, axis=-1))
            peak_strain = float(np.max(von_mises_strain))
            
            # æå‚·åº¦ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
            peak_damage = peak_strain * self.material_props['elastic_modulus'] / \
                         self.material_props['ultimate_strength']
            
            # è»¢ä½å¯†åº¦æ¨å®š
            coord_defect = np.abs(structures.coordination_numbers[:, cluster_id] - 12.0)
            dislocation_density = 1e14 * np.mean(coord_defect)**2  # /cm^2
            
            # å½¹å‰²æ±ºå®š
            role = self._determine_cluster_role(
                cluster_id, network_results
            )
            
            event = ClusterEvent(
                cluster_id=cluster_id,
                cluster_name=cluster_names.get(cluster_id, f"C{cluster_id}"),
                start_frame=start_frame + peak_idx,
                end_frame=start_frame + min(peak_idx + 10, len(scores) - 1),
                peak_strain=peak_strain,
                peak_damage=float(peak_damage),
                propagation_delay=peak_idx,
                role=role,
                adaptive_window=self.config.base_window,
                dislocation_density=float(dislocation_density),
                coordination_defect=float(np.mean(coord_defect)),
                von_mises_stress=peak_strain * self.material_props['elastic_modulus']
            )
            
            events.append(event)
        
        return events
    
    def _determine_cluster_role(self, cluster_id: int, network_results) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å½¹å‰²æ±ºå®š"""
        # ãƒãƒ–ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¤å®š
        if hasattr(network_results, 'network_stats'):
            stats = network_results.network_stats
            if 'hub_clusters' in stats:
                hub_ids = [h[0] for h in stats['hub_clusters']]
                if cluster_id in hub_ids:
                    return "initiator"
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®å½¹å‰²
        if hasattr(network_results, 'critical_clusters'):
            if cluster_id in network_results.critical_clusters:
                return "critical"
        
        return "participant"
    
    def _find_dislocation_sources(self,
                                 cluster_events: List[ClusterEvent],
                                 causality_chains: List) -> List[int]:
        """è»¢ä½ç™ºç”Ÿæºã®ç‰¹å®š"""
        sources = []
        
        # é«˜è»¢ä½å¯†åº¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        for event in cluster_events:
            if event.dislocation_density and event.dislocation_density > 1e12:
                sources.append(event.cluster_id)
        
        # å› æœãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚½ãƒ¼ã‚¹
        if causality_chains:
            out_degree = {}
            for from_c, to_c, _ in causality_chains:
                out_degree[from_c] = out_degree.get(from_c, 0) + 1
            
            top_sources = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:3]
            for cluster_id, _ in top_sources:
                if cluster_id not in sources:
                    sources.append(cluster_id)
        
        return sources[:5]
    
    def _trace_dislocation_paths(self,
                                sources: List[int],
                                causality_chains: List) -> List[List[int]]:
        """è»¢ä½ä¼æ’­çµŒè·¯ã®è¿½è·¡"""
        paths = []
        
        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        graph = {}
        for from_c, to_c, weight in causality_chains:
            if from_c not in graph:
                graph[from_c] = []
            graph[from_c].append((to_c, weight))
        
        # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰æ¢ç´¢
        for source in sources:
            path = self._dfs_path(source, graph, max_depth=5)
            if len(path) > 1:
                paths.append(path)
        
        return paths
    
    def _find_crack_initiation_sites(self,
                                    cluster_events: List[ClusterEvent],
                                    damage_network: List) -> List[int]:
        """äº€è£‚é–‹å§‹ç‚¹ã®ç‰¹å®š"""
        sites = []
        
        # é«˜æå‚·ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        for event in cluster_events:
            if event.peak_damage > 0.5:  # 50%æå‚·
                sites.append(event.cluster_id)
        
        return sites[:5]
    
    def _trace_crack_paths(self,
                         initiation_sites: List[int],
                         damage_network: List) -> List[List[int]]:
        """äº€è£‚ä¼æ’­çµŒè·¯ã®è¿½è·¡"""
        paths = []
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰çµŒè·¯æ§‹ç¯‰
        for site in initiation_sites:
            path = [site]
            current = site
            
            for link in damage_network:
                if hasattr(link, 'from_cluster') and link.from_cluster == current:
                    path.append(link.to_cluster)
                    current = link.to_cluster
                    
                    if len(path) >= 5:
                        break
            
            if len(path) > 1:
                paths.append(path)
        
        return paths
    
    def _dfs_path(self, start: int, graph: Dict, max_depth: int = 5) -> List[int]:
        """æ·±ã•å„ªå…ˆæ¢ç´¢ã§çµŒè·¯å–å¾—"""
        path = [start]
        current = start
        visited = {start}
        
        for _ in range(max_depth):
            if current not in graph:
                break
            
            # æœ€å¤§é‡ã¿ã®éš£æ¥ãƒãƒ¼ãƒ‰é¸æŠ
            neighbors = [(n, w) for n, w in graph[current] if n not in visited]
            if not neighbors:
                break
            
            next_node = max(neighbors, key=lambda x: x[1])[0]
            path.append(next_node)
            visited.add(next_node)
            current = next_node
        
        return path
    
    def _compute_anomaly_score(self, values: np.ndarray) -> np.ndarray:
        """ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        mean = np.mean(values)
        std = np.std(values) + 1e-10
        return np.abs(values - mean) / std
    
    def _compute_failure_probability(self, strain_tensor: np.ndarray) -> float:
        """ç ´å£Šç¢ºç‡è¨ˆç®—"""
        if strain_tensor is None or strain_tensor.size == 0:
            return 0.0
        
        # von Miseså¿œåŠ›
        von_mises_strain = np.mean(np.abs(strain_tensor))
        von_mises_stress = von_mises_strain * self.material_props['elastic_modulus']
        
        # ç ´å£Šç¢ºç‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if von_mises_stress > self.material_props['yield_strength']:
            ratio = von_mises_stress / self.material_props['ultimate_strength']
            return min(1.0, ratio)
        
        return 0.0
    
    def _compute_reliability_index(self, strain_tensor: np.ndarray) -> float:
        """ä¿¡é ¼æ€§æŒ‡æ¨™è¨ˆç®—"""
        if strain_tensor is None or strain_tensor.size == 0:
            return 5.0
        
        mean_strain = np.mean(np.abs(strain_tensor))
        std_strain = np.std(np.abs(strain_tensor))
        critical_strain = self.material_props['yield_strength'] / self.material_props['elastic_modulus']
        
        if std_strain > 0:
            beta = (critical_strain - mean_strain) / std_strain
            return float(beta)
        
        return 5.0
    
    def _identify_critical_clusters(self,
                                   importance_scores: Dict[int, float],
                                   cluster_analyses: Dict) -> List[int]:
        """è‡¨ç•Œã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å®š"""
        critical = []
        
        # é«˜é‡è¦åº¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        if importance_scores:
            sorted_clusters = sorted(
                importance_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            for cluster_id, score in sorted_clusters[:10]:
                # ç ´å£Šç¢ºç‡ãƒã‚§ãƒƒã‚¯
                for analysis in cluster_analyses.values():
                    for event in analysis.cluster_events:
                        if event.cluster_id == cluster_id:
                            if event.peak_damage > 0.7:  # 70%æå‚·
                                critical.append(cluster_id)
                                break
        
        return list(set(critical))
    
    def _identify_reinforcement_points(self,
                                      importance_scores: Dict[int, float],
                                      cluster_analyses: Dict) -> List[int]:
        """è£œå¼·æ¨å¥¨ç®‡æ‰€ã®ç‰¹å®š"""
        reinforcement = []
        
        # ä¸­ç¨‹åº¦ã®é‡è¦åº¦ã§æå‚·åˆæœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        if importance_scores:
            for cluster_id, score in importance_scores.items():
                for analysis in cluster_analyses.values():
                    for event in analysis.cluster_events:
                        if event.cluster_id == cluster_id:
                            if 0.3 < event.peak_damage < 0.7:  # 30-70%æå‚·
                                reinforcement.append(cluster_id)
                                break
        
        return list(set(reinforcement))[:10]
    
    def _evaluate_material_state(self,
                                cluster_analyses: Dict,
                                critical_clusters: List) -> Dict:
        """ææ–™çŠ¶æ…‹è©•ä¾¡"""
        max_damage = 0.0
        mean_strain = 0.0
        n_events = 0
        
        for analysis in cluster_analyses.values():
            for event in analysis.cluster_events:
                max_damage = max(max_damage, event.peak_damage)
                mean_strain += event.peak_strain
                n_events += 1
        
        if n_events > 0:
            mean_strain /= n_events
        
        # çŠ¶æ…‹åˆ¤å®š
        if max_damage > 0.8:
            state = "critical_damage"
            health = 0.2
        elif max_damage > 0.5:
            state = "moderate_damage"
            health = 0.5
        elif max_damage > 0.2:
            state = "minor_damage"
            health = 0.8
        else:
            state = "healthy"
            health = 1.0
        
        return {
            'state': state,
            'health_index': health,
            'max_damage': float(max_damage),
            'mean_strain': float(mean_strain),
            'n_critical_clusters': len(critical_clusters)
        }
    
    def _compute_global_stats(self, cluster_analyses: Dict) -> Dict:
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆè¨ˆç®—"""
        total_strain_links = sum(
            len(a.strain_network) for a in cluster_analyses.values()
        )
        total_dislocation_links = sum(
            len(a.dislocation_network) for a in cluster_analyses.values()
        )
        total_damage_links = sum(
            len(a.damage_network) for a in cluster_analyses.values()
        )
        
        max_failure_prob = max(
            (a.failure_probability for a in cluster_analyses.values()),
            default=0.0
        )
        
        min_reliability = min(
            (a.reliability_index for a in cluster_analyses.values()),
            default=5.0
        )
        
        total_gpu_time = sum(a.gpu_time for a in cluster_analyses.values())
        
        return {
            'total_strain_links': total_strain_links,
            'total_dislocation_links': total_dislocation_links,
            'total_damage_links': total_damage_links,
            'max_failure_probability': max_failure_prob,
            'min_reliability_index': min_reliability,
            'total_gpu_time': total_gpu_time,
            'events_analyzed': len(cluster_analyses)
        }
    
    def _print_summary(self,
                      importance_scores: Dict,
                      critical_clusters: List,
                      reinforcement_points: List,
                      material_state: Dict):
        """è§£æã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ’ Material Analysis Complete!")
        print(f"   Material state: {material_state['state']}")
        print(f"   Health index: {material_state['health_index']:.1%}")
        print(f"   Key clusters identified: {len(importance_scores)}")
        print(f"   Critical clusters: {len(critical_clusters)}")
        print(f"   Reinforcement points: {len(reinforcement_points)}")
        print(f"   Max damage: {material_state['max_damage']:.1%}")
        print(f"   Mean strain: {material_state['mean_strain']:.3f}")

# ===============================
# Utility Functions
# ===============================

def perform_material_two_stage_analysis_gpu(
    trajectory: np.ndarray,
    macro_result: MaterialLambda3Result,
    detected_events: List[Tuple[int, int, str]],
    cluster_atoms: Dict[int, List[int]],
    atom_types: np.ndarray,
    material_type: str = 'SUJ2',
    config: ClusterAnalysisConfig = None
) -> MaterialTwoStageResult:
    """
    ææ–™2æ®µéšè§£æã®ä¾¿åˆ©ãªãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
    
    Parameters
    ----------
    trajectory : np.ndarray
        åŸå­ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒª
    macro_result : MaterialLambda3Result
        ãƒã‚¯ãƒ­è§£æçµæœ
    detected_events : List[Tuple[int, int, str]]
        æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆ
    cluster_atoms : Dict[int, List[int]]
        ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å®šç¾©
    atom_types : np.ndarray
        åŸå­ã‚¿ã‚¤ãƒ—
    material_type : str
        ææ–™ã‚¿ã‚¤ãƒ— ('SUJ2', 'AL7075', etc.)
    config : ClusterAnalysisConfig
        è¨­å®š
    """
    analyzer = MaterialTwoStageAnalyzerGPU(config, material_type)
    return analyzer.analyze_trajectory(
        trajectory, macro_result, detected_events,
        cluster_atoms, atom_types
    )
