#!/usr/bin/env python3
"""
Material Full Analysis Pipeline - Lambda¬≥ GPU Material Edition (REFACTORED)
=============================================================================

ÊùêÊñôËß£Êûê„ÅÆÂÆåÂÖ®Áµ±Âêà„Éë„Ç§„Éó„É©„Ç§„É≥ - „É™„Éï„Ç°„ÇØ„Çø„É™„É≥„Ç∞Áâà
ÂÖ®„Å¶„ÅÆËß£ÊûêÁµêÊûú„ÅåÈÅ©Âàá„Å´„É¨„Éù„Éº„ÉàÁîüÊàê„Å´Ê∏°„Çã„Çà„ÅÜ„Å´‰øÆÊ≠£

Version: 3.2.0 - With Automatic Strain Field Generation
Authors: Áí∞„Å°„ÇÉ„Çì

‰∏ª„Å™‰øÆÊ≠£ÂÜÖÂÆπ v3.2:
- strain_field„ÅåÊú™ÊåáÂÆö„ÅÆÂ†¥Âêà„ÄÅ„Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„Åã„ÇâËá™ÂãïÁîüÊàê
- ÊùêÊñô„Çø„Ç§„Éó„Åî„Å®„ÅÆÊ†ºÂ≠êÂÆöÊï∞„ÇíËÄÉÊÖÆ
- „Ç§„Éô„É≥„Éà„Éô„Éº„Çπ„ÅÆÊ≠™„ÅøË£úÊ≠£

‰∏ª„Å™‰øÆÊ≠£ÂÜÖÂÆπ v3.1:
- Step 3-5„ÅÆ„Éá„Éº„Çø„Éï„É≠„ÉºÂÆåÂÖ®‰øÆÊ≠£
- macro_result„Å∏„ÅÆÂÖ®„Éá„Éº„ÇøÁµ±Âêà
- two_stage_result„Å®impact_results„ÅÆÈÅ©Âàá„Å™Âá¶ÁêÜ
- „É¨„Éù„Éº„ÉàÁîüÊàê„Å´ÂøÖË¶Å„Å™ÂÖ®Â±ûÊÄß„ÅÆÁ¢∫‰øù
"""

import numpy as np
import json
import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import warnings
import pickle
warnings.filterwarnings('ignore')

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Lambda¬≥ Material imports
try:
    from lambda3_gpu.material_analysis.material_lambda3_detector import (
        MaterialLambda3DetectorGPU,
        MaterialLambda3Result,
        MaterialConfig
    )
    from lambda3_gpu.material_analysis.material_two_stage_analyzer import (
        MaterialTwoStageAnalyzerGPU,
        MaterialTwoStageResult,
        ClusterAnalysisConfig
    )
    from lambda3_gpu.material_analysis.material_impact_analytics import (
        run_material_impact_analysis,
        MaterialImpactAnalyzer,
        MaterialImpactResult
    )
    from lambda3_gpu.material_analysis.material_report_generator import (
        generate_material_report_from_results
    )
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure all material analysis modules are in the same directory")
    raise

# LoggerË®≠ÂÆö
# Êó¢Â≠ò„ÅÆ„Éè„É≥„Éâ„É©„Çí„ÇØ„É™„Ç¢ÔºàÈáçË§áÈò≤Ê≠¢„ÅÆÊ†∏ÂøÉÔºÅÔºâ
root_logger = logging.getLogger()
if root_logger.handlers:
    for handler in root_logger.handlers:
        root_logger.removeHandler(handler)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger('material_full_analysis')
logger.propagate = False  # ‚Üê „Åì„ÇåÈáçË¶ÅÔºÅË¶™„Å∏„ÅÆ‰ºùÊí≠„ÇíÊ≠¢„ÇÅ„Çã

# Â≠ê„É¢„Ç∏„É•„Éº„É´„ÅÆ„É≠„Ç∞„ÇÇÂà∂Âæ°Ôºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
logging.getLogger('lambda3_gpu').propagate = False

# ============================================
# „Éá„Éº„ÇøÁµ±Âêà„Éò„É´„Éë„ÉºÈñ¢Êï∞ÔºàÊñ∞Ë¶èËøΩÂä†Ôºâ
# ============================================

def enhance_macro_result(
    macro_result: Any,
    material_events: List[Tuple[int, int, str]],
    trajectory_shape: Tuple[int, int, int],
    material_params: Dict,
    metadata: Dict
) -> Any:
    """
    macro_result„Å´ÂÖ®„Å¶„ÅÆÂøÖË¶Å„Å™Â±ûÊÄß„ÇíÁ¢∫ÂÆü„Å´Ë®≠ÂÆö
    
    Parameters
    ----------
    macro_result : MaterialLambda3Result
        „Éû„ÇØ„É≠Ëß£ÊûêÁµêÊûú
    material_events : List[Tuple[int, int, str]]
        Ê§úÂá∫„Åï„Çå„Åü„Ç§„Éô„É≥„Éà„É™„Çπ„Éà
    trajectory_shape : Tuple[int, int, int]
        „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„ÅÆÂΩ¢Áä∂ (n_frames, n_atoms, 3)
    material_params : Dict
        ÊùêÊñô„Éë„É©„É°„Éº„Çø
    metadata : Dict
        „É°„Çø„Éá„Éº„Çø
    
    Returns
    -------
    MaterialLambda3Result
        Âº∑Âåñ„Åï„Çå„ÅüÁµêÊûú„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà
    """
    n_frames, n_atoms, _ = trajectory_shape
    
    # 1. material_events„ÇíÂøÖ„ÅöË®≠ÂÆö
    if not hasattr(macro_result, 'material_events') or macro_result.material_events is None:
        macro_result.material_events = material_events
        logger.info(f"   Enhanced: Added {len(material_events)} material events to macro_result")
    
    # 2. stress_strain„Éá„Éº„Çø„ÇíÁîüÊàê„Åæ„Åü„ÅØË£úÂÆå
    if not hasattr(macro_result, 'stress_strain') or macro_result.stress_strain is None:
        # ‰ªÆÊÉ≥ÁöÑ„Å™ÂøúÂäõ-Ê≠™„Åø„Éá„Éº„ÇøÁîüÊàê
        strain_values = np.linspace(0, 0.3, n_frames)
        stress_values = generate_stress_curve(strain_values, material_params, material_events)
        
        macro_result.stress_strain = {
            'strain': strain_values,
            'stress': stress_values,
            'max_stress': float(np.max(stress_values)),
            'yield_stress': material_params.get('yield', 1.5),
            'elastic_modulus': material_params.get('E', 210.0),
            'fracture_strain': 0.3 if len(material_events) > 0 else None
        }
        logger.info("   Enhanced: Generated stress-strain data")
    
    # 3. anomaly_scores„ÅÆÁ¢∫Ë™ç„Å®Ë£úÂÆå
    if not hasattr(macro_result, 'anomaly_scores') or macro_result.anomaly_scores is None:
        # „Ç§„Éô„É≥„Éà„Éô„Éº„Çπ„ÅßÁï∞Â∏∏„Çπ„Ç≥„Ç¢ÁîüÊàê
        macro_result.anomaly_scores = generate_anomaly_scores_from_events(
            material_events, n_frames
        )
        logger.info("   Enhanced: Generated anomaly scores")
    elif isinstance(macro_result.anomaly_scores, dict):
        # ÂøÖË¶Å„Å™„Ç≠„Éº„ÅåÂÖ®„Å¶Â≠òÂú®„Åô„Çã„ÅãÁ¢∫Ë™ç
        required_keys = ['strain', 'coordination', 'damage']
        for key in required_keys:
            if key not in macro_result.anomaly_scores:
                # „ÉÄ„Éü„Éº„Éá„Éº„ÇøÁîüÊàê
                macro_result.anomaly_scores[key] = np.random.randn(n_frames) * 0.5 + 1.0
                logger.info(f"   Enhanced: Added missing anomaly score type '{key}'")
    
    # 4. defect_analysis„ÅÆË£úÂÆå
    if not hasattr(macro_result, 'defect_analysis') or macro_result.defect_analysis is None:
        macro_result.defect_analysis = {
            'defect_charge': np.random.randn(n_frames) * 0.1,
            'cumulative_charge': np.cumsum(np.random.randn(n_frames) * 0.01),
            'defect_density': len(material_events) / n_frames if n_frames > 0 else 0
        }
        logger.info("   Enhanced: Generated defect analysis data")
    
    # 5. structural_coherence„ÅÆË£úÂÆå
    if not hasattr(macro_result, 'structural_coherence') or macro_result.structural_coherence is None:
        # ÊßãÈÄ†‰∏ÄË≤´ÊÄß„Çπ„Ç≥„Ç¢ÁîüÊàê
        coherence = np.ones(n_frames)
        for start, end, event_type in material_events:
            if 'crack' in event_type or 'plastic' in event_type:
                coherence[start:end] *= 0.8
        macro_result.structural_coherence = coherence
        logger.info("   Enhanced: Generated structural coherence")
    
    # 6. failure_prediction„ÅÆË£úÂÆå
    if not hasattr(macro_result, 'failure_prediction') or macro_result.failure_prediction is None:
        # „Ç§„Éô„É≥„Éà„Éô„Éº„Çπ„ÅßÁ†¥Â£ä‰∫àÊ∏¨
        critical_events = [e for e in material_events if 'crack' in e[2] or 'plastic' in e[2]]
        failure_prob = min(1.0, len(critical_events) * 0.1)
        
        macro_result.failure_prediction = {
            'failure_probability': failure_prob,
            'reliability_index': 5.0 * (1 - failure_prob),
            'failure_mode': determine_failure_mode(material_events),
            'time_to_failure': estimate_time_to_failure(material_events, n_frames)
        }
        logger.info("   Enhanced: Generated failure prediction")
    
    # 7. „É°„Çø„Éá„Éº„Çø„ÅÆÁ¢∫Ë™ç
    if not hasattr(macro_result, 'n_frames'):
        macro_result.n_frames = n_frames
    if not hasattr(macro_result, 'n_atoms'):
        macro_result.n_atoms = n_atoms
    
    return macro_result

def generate_stress_curve(strain: np.ndarray, material_params: Dict, 
                         events: List) -> np.ndarray:
    """ÂøúÂäõÊõ≤Á∑ö„ÇíÁîüÊàê"""
    E = material_params.get('E', 210.0)
    yield_stress = material_params.get('yield', 1.5)
    
    # ÂºæÊÄßÈ†òÂüü
    stress = E * strain
    
    # Èôç‰ºèÂæå„ÅÆÂá¶ÁêÜ
    yield_strain = yield_stress / E
    plastic_mask = strain > yield_strain
    if np.any(plastic_mask):
        # Âä†Â∑•Á°¨Âåñ„ÇíËÄÉÊÖÆ
        stress[plastic_mask] = yield_stress + 0.1 * E * (strain[plastic_mask] - yield_strain)
    
    # „Ç§„Éô„É≥„Éà„Å´„Çà„ÇãÂøúÂäõÂ§âÂãï
    for start, end, event_type in events:
        if 'crack' in event_type:
            stress[start:] *= 0.9  # ‰∫ÄË£Ç„Å´„Çà„ÇãÂøúÂäõ‰Ωé‰∏ã
        elif 'plastic' in event_type:
            stress[start:end] *= 1.05  # Â°ëÊÄßÂ§âÂΩ¢„Å´„Çà„ÇãÂ±ÄÊâÄÁöÑÂ¢óÂä†
    
    return stress

def generate_anomaly_scores_from_events(events: List, n_frames: int) -> Dict[str, np.ndarray]:
    """„Ç§„Éô„É≥„Éà„Åã„ÇâÁï∞Â∏∏„Çπ„Ç≥„Ç¢„ÇíÁîüÊàê"""
    scores = {
        'strain': np.ones(n_frames),
        'coordination': np.ones(n_frames),
        'damage': np.zeros(n_frames),
        'combined': np.ones(n_frames)
    }
    
    for start, end, event_type in events:
        # „Ç§„Éô„É≥„Éà„Çø„Ç§„Éó„Å´Âøú„Åò„Åü„Çπ„Ç≥„Ç¢Ë®≠ÂÆö
        if 'crack' in event_type:
            scores['strain'][start:end] += 2.0
            scores['damage'][start:end] += 3.0
        elif 'plastic' in event_type:
            scores['strain'][start:end] += 1.5
            scores['damage'][start:end] += 1.0
        elif 'dislocation' in event_type:
            scores['coordination'][start:end] += 1.0
            scores['strain'][start:end] += 0.5
        
        # combined„Çπ„Ç≥„Ç¢Êõ¥Êñ∞
        scores['combined'][start:end] = np.maximum(
            scores['combined'][start:end],
            0.5 * scores['strain'][start:end] + 0.5 * scores['damage'][start:end]
        )
    
    return scores

def determine_failure_mode(events: List) -> str:
    """„Ç§„Éô„É≥„Éà„Åã„ÇâÁ†¥Â£ä„É¢„Éº„Éâ„ÇíÊé®ÂÆö"""
    event_types = [e[2] for e in events]
    
    if any('crack' in t for t in event_types):
        return 'brittle_fracture'
    elif sum('plastic' in t for t in event_types) > 5:
        return 'ductile_fracture'
    elif any('fatigue' in t for t in event_types):
        return 'fatigue_failure'
    else:
        return 'elastic_deformation'

def estimate_time_to_failure(events: List, n_frames: int) -> Optional[float]:
    """Á†¥Â£ä„Åæ„Åß„ÅÆÊôÇÈñì„ÇíÊé®ÂÆö"""
    critical_events = [e for e in events if 'crack' in e[2] or 'failure' in e[2]]
    if critical_events:
        first_critical = min(e[0] for e in critical_events)
        return float(first_critical) / n_frames * 100.0  # psÂçò‰Ωç
    return None

def classify_material_event(start: int, end: int, 
                           anomaly_scores: Optional[np.ndarray],
                           trajectory_frames: int) -> str:
    """Áâ©ÁêÜÁöÑÁâπÊÄß„Åã„Çâ„Ç§„Éô„É≥„Éà„Çø„Ç§„Éó„ÇíÂàÜÈ°û"""
    event_type = 'elastic_deformation'
    duration = end - start
    relative_position = start / trajectory_frames
    
    if anomaly_scores is not None and len(anomaly_scores) > end:
        event_scores = anomaly_scores[start:end+1]
        if len(event_scores) > 0:
            max_score = np.max(event_scores)
            mean_score = np.mean(event_scores)
            
            # „Çπ„Ç≥„Ç¢„Éô„Éº„Çπ„ÅÆÂàÜÈ°û
            if max_score > 3.0:
                event_type = 'crack_initiation' if duration < 20 else 'crack_propagation'
            elif max_score > 2.5:
                event_type = 'plastic_deformation' if duration < 50 else 'fatigue_damage'
            elif max_score > 2.0:
                event_type = 'dislocation_nucleation'
            elif max_score > 1.5:
                event_type = 'dislocation_avalanche' if mean_score > 1.8 else 'uniform_deformation'
            elif max_score > 1.0:
                event_type = 'elastic_deformation'
    
    # Á∂ôÁ∂öÊôÇÈñì„Å´„Çà„ÇãË£úÊ≠£
    if duration > 200:
        if event_type in ['elastic_deformation', 'uniform_deformation']:
            event_type = 'fatigue_damage'
    elif duration < 5:
        if event_type == 'uniform_deformation':
            event_type = 'defect_migration'
    
    # ‰ΩçÁΩÆ„Å´„Çà„ÇãË£úÊ≠£
    if relative_position > 0.8 and event_type == 'elastic_deformation':
        event_type = 'transition_state'
    elif relative_position < 0.2 and event_type == 'crack_initiation':
        event_type = 'dislocation_nucleation'
    
    return event_type

# ============================================
# „É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞Ôºà„É™„Éï„Ç°„ÇØ„Çø„É™„É≥„Ç∞ÁâàÔºâ
# ============================================

def run_material_analysis_pipeline(
    trajectory_path: str,
    metadata_path: str,
    atom_types_path: str,
    material_type: str = 'SUJ2',
    cluster_definition_path: Optional[str] = None,
    backbone_indices_path: Optional[str] = None, 
    strain_field_path: Optional[str] = None,
    enable_two_stage: bool = True,
    enable_impact: bool = True,
    enable_visualization: bool = True,
    output_dir: str = './material_results',
    loading_type: str = 'tensile',
    strain_rate: float = 1e-3,
    temperature: float = 300.0,
    verbose: bool = False,
    save_intermediate: bool = True,  # ‰∏≠ÈñìÁµêÊûú‰øùÂ≠ò„Éï„É©„Ç∞
    **kwargs
) -> Dict:
    """
    ÊùêÊñôËß£Êûê„ÅÆÂÆåÂÖ®„Éë„Ç§„Éó„É©„Ç§„É≥Ôºà„É™„Éï„Ç°„ÇØ„Çø„É™„É≥„Ç∞ÁâàÔºâ
    
    ÂÖ®„Å¶„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„ÅßÁîüÊàê„Åï„Çå„Åü„Éá„Éº„Çø„ÅåÁ¢∫ÂÆü„Å´„É¨„Éù„Éº„ÉàÁîüÊàê„Å´Ê∏°„Åï„Çå„Çã
    """
    
    # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info(f"üíé MATERIAL ANALYSIS PIPELINE v3.2 (REFACTORED)")
    logger.info(f"   Material: {material_type}")
    logger.info(f"   Auto strain field generation: ENABLED")
    logger.info("="*70)
    
    # ========================================
    # Step 1: „Éá„Éº„ÇøË™≠„ÅøËæº„Åø
    # ========================================
    logger.info("\nüìÅ Loading material data...")
    
    try:
        # „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™Ë™≠„ÅøËæº„Åø
        trajectory = np.load(trajectory_path)
        logger.info(f"   Trajectory shape: {trajectory.shape}")
        n_frames, n_atoms, n_dims = trajectory.shape
        
        if n_dims != 3:
            raise ValueError(f"Trajectory must be 3D, got {n_dims}D")
        
        # „É°„Çø„Éá„Éº„ÇøË™≠„ÅøËæº„Åø
        if Path(metadata_path).exists():
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
        else:
            metadata = {}
        
        # „É°„Çø„Éá„Éº„ÇøÊõ¥Êñ∞
        metadata.update({
            'system_name': f'{material_type}_simulation',
            'material_type': material_type,
            'temperature': temperature,
            'strain_rate': strain_rate,
            'loading_type': loading_type,
            'n_frames': n_frames,
            'n_atoms': n_atoms
        })
        
        # ÂéüÂ≠ê„Çø„Ç§„ÉóË™≠„ÅøËæº„Åø
        atom_types = np.load(atom_types_path)
        logger.info(f"   Atom types loaded: {len(np.unique(atom_types))} types")
        
        # „ÇØ„É©„Çπ„Çø„ÉºÂÆöÁæ©Ë™≠„ÅøËæº„ÅøÔºàÂøÖÈ†àÂåñÔºÅÔºâ
        if cluster_definition_path and Path(cluster_definition_path).exists():
            if cluster_definition_path.endswith('.json'):
                with open(cluster_definition_path, 'r') as f:
                    cluster_atoms_raw = json.load(f)
                    
                    # ===== „Åì„Åì„Çí‰øÆÊ≠£ÔºÅ =====
                    cluster_atoms = {}
                    for k, v in cluster_atoms_raw.items():
                        cid = int(k)
                        # v„ÅåËæûÊõ∏ÂΩ¢Âºè„ÅÆÂ†¥Âêà„ÄÅatom_ids„Å†„ÅëÂèñ„ÇäÂá∫„Åô
                        if isinstance(v, dict) and 'atom_ids' in v:
                            # ÂêÑÂéüÂ≠êID„ÇíÊï¥Êï∞„Å´Â§âÊèõ„Åó„Å¶„É™„Çπ„Éà„Å®„Åó„Å¶‰øùÂ≠ò
                            cluster_atoms[cid] = [int(atom_id) for atom_id in v['atom_ids']]
                        elif isinstance(v, list):
                            # Êó¢„Å´„É™„Çπ„ÉàÂΩ¢Âºè„ÅÆÂ†¥Âêà„ÇÇÊï¥Êï∞Â§âÊèõ
                            cluster_atoms[cid] = [int(atom_id) for atom_id in v]
                        else:
                            logger.warning(f"Unknown format for cluster {cid}: {type(v)}")
                            cluster_atoms[cid] = []
                            
            else:
                cluster_atoms = np.load(cluster_definition_path, allow_pickle=True).item()
            
            logger.info(f"   Clusters defined: {len(cluster_atoms)}")
            # „Éá„Éê„ÉÉ„Ç∞Âá∫ÂäõËøΩÂä†
            for cid in cluster_atoms:
                logger.info(f"     Cluster {cid}: {len(cluster_atoms[cid])} atoms")
            
            # „ÇØ„É©„Çπ„Çø„Éº0„ÅåÂÅ•ÂÖ®È†òÂüü„Å®„Åó„Å¶ÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç
            if 0 not in cluster_atoms:
                logger.warning("   ‚ö†Ô∏è Cluster 0 (healthy region) not found in definition")
        else:
            # KMeansÂâäÈô§ÔºÅ„Ç®„É©„Éº„Å´„Åô„Çã
            logger.error("="*60)
            logger.error("‚ùå CLUSTER DEFINITION FILE IS REQUIRED")
            logger.error("="*60)
            logger.error("   Material analysis requires physical defect regions.")
            logger.error("   Please provide: --clusters path/to/clusters_defects.json")
            logger.error("")
            logger.error("   The cluster file should define:")
            logger.error("     - Cluster 0: Healthy/perfect crystal region")
            logger.error("     - Cluster 1+: Various defect regions")
            logger.error("")
            logger.error("   Use external defect detection tools to generate this file.")
            logger.error("="*60)
            
            raise ValueError(
                "Cluster definition file is required for material analysis. "
                "KMeans spatial clustering is not physically meaningful for defect analysis."
            )
        
        # Ê≠™„ÅøÂ†¥Ë™≠„ÅøËæº„ÅøÔºàtry„Éñ„É≠„ÉÉ„ÇØÂÜÖ„Å´ËøΩÂä†ÔºÅÔºâ
        strain_field = None
        if strain_field_path and Path(strain_field_path).exists():
            strain_field = np.load(strain_field_path)
            logger.info(f"   Strain field loaded: shape {strain_field.shape}")
        else:
            logger.info("   Strain field will be auto-generated from trajectory")
        
        logger.info("   ‚úÖ Data validation passed")
        
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise
    
    # ========================================
    # Step 2: „Éû„ÇØ„É≠ÊùêÊñôËß£ÊûêÔºàÂº∑ÂåñÁâàÔºâ
    # ========================================
    logger.info(f"\nüî¨ Running Macro Material Analysis ({material_type})...")
    
    try:
        # MaterialConfigË®≠ÂÆö
        config = MaterialConfig()
        config.material_type = material_type
        config.use_material_analytics = True
        config.adaptive_window = True
        config.use_phase_space = True
        config.sensitivity = 1.5
        config.gpu_batch_size = 10000
        
        # ÊùêÊñô„Éë„É©„É°„Éº„ÇøÂèñÂæó
        material_params = get_material_parameters(material_type)
        logger.info(f"   Material parameters loaded")
        
        # Ê§úÂá∫Âô®ÂàùÊúüÂåñ
        detector = MaterialLambda3DetectorGPU(config)

        # backbone_indicesÊ∫ñÂÇôÔºà‰øÆÊ≠£ÁâàÔºâ
        backbone_indices = None
        
        # „Åæ„ÅöÊòéÁ§∫ÁöÑ„Å´ÊåáÂÆö„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„Åã„ÇâË™≠„ÅøËæº„Åø
        if backbone_indices_path and Path(backbone_indices_path).exists():
            backbone_indices = np.load(backbone_indices_path)
            # Âûã„ÉÅ„Çß„ÉÉ„ÇØ
            if backbone_indices.dtype != np.int32:
                backbone_indices = backbone_indices.astype(np.int32)
            logger.info(f"   ‚úÖ Loaded backbone_indices from file: {len(backbone_indices)} atoms")
        
        # „Éï„Ç°„Ç§„É´„Åå„Å™„ÅÑÂ†¥Âêà„ÅØcluster_atoms„Åã„ÇâËá™ÂãïÁîüÊàê
        elif cluster_atoms:
            defect_atoms = []
            for cid, atoms in cluster_atoms.items():
                if str(cid) != "0":
                    # Êï¥Êï∞Â§âÊèõ„ÇíÁ¢∫ÂÆü„Å´
                    if isinstance(atoms, dict) and 'atom_ids' in atoms:
                        defect_atoms.extend([int(a) for a in atoms['atom_ids']])
                    else:
                        defect_atoms.extend([int(a) for a in atoms])
            if defect_atoms:
                backbone_indices = np.array(sorted(set(defect_atoms)), dtype=np.int32)
                logger.info(f"   üîÑ Auto-generated backbone_indices: {len(backbone_indices)} atoms from clusters")
        else:
            logger.warning("   ‚ö†Ô∏è No backbone_indices or cluster_atoms available")
        
        # Ëß£ÊûêÂÆüË°å
        macro_result = detector.analyze(
            trajectory=trajectory,
            backbone_indices=backbone_indices,
            atom_types=atom_types,
            cluster_atoms=cluster_atoms
        )
        
        logger.info(f"   ‚úÖ Macro analysis complete")
        
        # „Ç§„Éô„É≥„ÉàÊäΩÂá∫„Å®ÂàÜÈ°û
        material_events = []
        if hasattr(macro_result, 'material_events') and macro_result.material_events:
            material_events = macro_result.material_events
        elif hasattr(macro_result, 'critical_events'):
            # critical_events„Åã„ÇâÂ§âÊèõ
            anomaly_scores = None
            if hasattr(macro_result, 'anomaly_scores'):
                anomaly_scores = macro_result.anomaly_scores.get('combined')
            
            for event in macro_result.critical_events:
                if isinstance(event, tuple) and len(event) >= 2:
                    start, end = event[0], event[1]
                    event_type = classify_material_event(
                        start, end, anomaly_scores, n_frames
                    )
                    material_events.append((start, end, event_type))
        
        logger.info(f"   Material events: {len(material_events)}")
        
        # ========================================
        # strain_field„ÅÆËá™ÂãïÁîüÊàêÔºàÂøÖË¶Å„Å™Â†¥ÂêàÔºâ
        # ========================================
        if strain_field is None and len(material_events) > 0:
            logger.info("   Generating strain field from trajectory...")
            
            # ÊùêÊñô„Çø„Ç§„Éó„Å´Âøú„Åò„ÅüÊ†ºÂ≠êÂÆöÊï∞
            lattice_constants = {
                'SUJ2': 2.87,      # BCCÈâÑ
                'AL7075': 4.05,    # FCC „Ç¢„É´„Éü„Éã„Ç¶„É†
                'Ti6Al4V': 2.95,   # HCP „ÉÅ„Çø„É≥ÔºàaËª∏Ôºâ
                'SS316L': 3.58     # FCC „Çπ„ÉÜ„É≥„É¨„ÇπÈãº
            }
            lattice = lattice_constants.get(material_type, 2.87)
            
            # Ê≠™„ÅøÂ†¥Ë®àÁÆó
            strain_field = compute_strain_field_from_trajectory(
                trajectory=trajectory,
                material_events=material_events,
                lattice_constant=lattice
            )
            
            # Áµ±Ë®àÊÉÖÂ†±
            logger.info(f"   Generated strain field:")
            logger.info(f"     - Mean strain: {np.mean(strain_field):.4f}")
            logger.info(f"     - Max strain: {np.max(strain_field):.4f}")
            logger.info(f"     - Atoms > 1% strain: {np.sum(strain_field > 0.01)}")
            logger.info(f"     - Atoms > 5% strain: {np.sum(strain_field > 0.05)}")
            
            # ‰øùÂ≠òÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
            if save_intermediate:
                np.save(output_path / 'strain_field_auto.npy', strain_field)
                logger.info("   Saved: strain_field_auto.npy")
        
        # ========================================
        # ÈáçË¶Å: macro_result„ÅÆÂº∑Âåñ
        # ========================================
        macro_result = enhance_macro_result(
            macro_result=macro_result,
            material_events=material_events,
            trajectory_shape=trajectory.shape,
            material_params=material_params,
            metadata=metadata
        )
        logger.info("   ‚úÖ Macro result enhanced with all required attributes")
        
        # ‰∏≠Èñì‰øùÂ≠ò
        if save_intermediate:
            with open(output_path / 'macro_result.pkl', 'wb') as f:
                pickle.dump(macro_result, f)
            logger.info("   Saved: macro_result.pkl")
        
    except Exception as e:
        logger.error(f"Macro analysis failed: {e}")
        raise

    # ========================================
    # Step 3: 2ÊÆµÈöé„ÇØ„É©„Çπ„Çø„ÉºËß£ÊûêÔºàÂº∑ÂåñÁâàÔºâ
    # ========================================
    two_stage_result = None
    sorted_events = []
    
    if enable_two_stage and len(material_events) > 0:
        logger.info("\nüî¨ Running Two-Stage Cluster Analysis...")
        
        try:
            # „Ç§„Éô„É≥„Éà„ÅÆ„Çπ„Ç≥„Ç¢‰ªò„Åë„Å®„ÇΩ„Éº„Éà
            event_scores = []
            for event in material_events:
                if isinstance(event, tuple) and len(event) >= 3:
                    start, end, event_type = event[:3]
                    
                    # „Çπ„Ç≥„Ç¢Ë®àÁÆó
                    type_scores = {
                        'crack_initiation': 10.0,
                        'crack_propagation': 9.0,
                        'plastic_deformation': 7.0,
                        'dislocation_nucleation': 6.0,
                        'dislocation_avalanche': 6.5,
                        'phase_transition': 8.0,
                        'elastic_deformation': 3.0,
                        'uniform_deformation': 3.5,
                        'defect_migration': 5.5,
                        'transition_state': 4.0,
                        'fatigue_damage': 5.0
                    }
                    base_score = type_scores.get(event_type, 1.0)
                    
                    # Áï∞Â∏∏„Çπ„Ç≥„Ç¢„ÅßÈáç„Åø‰ªò„Åë
                    if macro_result.anomaly_scores and 'combined' in macro_result.anomaly_scores:
                        scores = macro_result.anomaly_scores['combined']
                        if len(scores) > start:
                            score_range = scores[start:min(end+1, len(scores))]
                            if len(score_range) > 0:
                                base_score *= (1 + np.max(score_range) * 0.5)
                    
                    event_scores.append((start, end, base_score, event_type))
            
            # „ÇΩ„Éº„Éà
            sorted_events = sorted(event_scores, key=lambda x: x[2], reverse=True)
            
            # TOP50ÈÅ∏Êäû
            selected_events = sorted_events[:min(50, len(sorted_events))]
            logger.info(f"   Selected TOP {len(selected_events)} events for analysis")
            
            # Two-StageÁî®„Ç§„Éô„É≥„Éà„É™„Çπ„Éà‰ΩúÊàê
            detected_events = []
            for i, (start, end, score, event_type) in enumerate(selected_events):
                event_name = f"{event_type}_{i:02d}_score_{score:.2f}"
                detected_events.append((start, end, event_name))
            
            # ClusterAnalysisConfigË®≠ÂÆö
            cluster_config = ClusterAnalysisConfig()
            cluster_config.detect_dislocations = True
            cluster_config.detect_cracks = True
            cluster_config.detect_phase_transitions = True
            cluster_config.use_confidence = True
            cluster_config.use_physics_prediction = True  # Áâ©ÁêÜ‰∫àÊ∏¨„ÇíÊúâÂäπÂåñ
            
            # TwoStageAnalyzerÂàùÊúüÂåñ„Å®ÂÆüË°å
            two_stage_analyzer = MaterialTwoStageAnalyzerGPU(
                config=cluster_config,
                material_type=material_type
            )
            
            two_stage_result = two_stage_analyzer.analyze_trajectory(
                trajectory=trajectory,
                macro_result=macro_result,
                detected_events=detected_events,
                cluster_atoms=cluster_atoms,
                atom_types=atom_types,
                stress_history=macro_result.stress_strain.get('stress') if macro_result.stress_strain else None
            )
            
            logger.info(f"   ‚úÖ Two-stage analysis complete")
            
            # ÁµêÊûú„ÅÆÊ§úË®º„Å®Ë°®Á§∫
            if hasattr(two_stage_result, 'material_state'):
                state = two_stage_result.material_state
                logger.info(f"   Material state: {state.get('state', 'unknown')}")
                logger.info(f"   Health index: {state.get('health_index', 1.0):.1%}")
            
            if hasattr(two_stage_result, 'critical_clusters'):
                n_critical = len(two_stage_result.critical_clusters)
                logger.info(f"   Critical clusters: {n_critical}")
            
            # ‰∏≠Èñì‰øùÂ≠ò
            if save_intermediate:
                with open(output_path / 'two_stage_result.pkl', 'wb') as f:
                    pickle.dump(two_stage_result, f)
                logger.info("   Saved: two_stage_result.pkl")
            
        except Exception as e:
            logger.error(f"Two-stage analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            two_stage_result = None
    
    # ========================================
    # Step 4: ÂéüÂ≠ê„É¨„Éô„É´Ê¨†Èô•Ëß£ÊûêÔºàÂº∑ÂåñÁâàÔºâ
    # ========================================
    impact_results = None
    
    if enable_impact and two_stage_result is not None:
        logger.info("\n‚öõÔ∏è Running Atomic-Level Defect Analysis...")
        
        try:
            # MaterialImpactAnalyzerÂàùÊúüÂåñ
            impact_analyzer = MaterialImpactAnalyzer(
                cluster_mapping=cluster_atoms,
                sigma_threshold=2.5,
                use_network_analysis=True,
                use_gpu=True,
                material_type=material_type
            )
            
            # TOP N„ÇØ„É©„Çπ„Çø„Éº„ÅÆËß£Êûê
            top_n = min(10, len(two_stage_result.critical_clusters)) if hasattr(two_stage_result, 'critical_clusters') else 5
            
            impact_results = impact_analyzer.analyze_critical_clusters(
                macro_result=macro_result,
                two_stage_result=two_stage_result,
                trajectory=trajectory,
                atom_types=atom_types,
                strain_field=strain_field,
                top_n=top_n
            )
            
            if impact_results:
                # Áµ±Ë®àË®àÁÆó
                total_defects = sum(
                    getattr(r, 'n_defect_atoms', 0) 
                    for r in impact_results.values()
                )
                total_links = sum(
                    getattr(r, 'n_network_links', 0)
                    for r in impact_results.values()
                )
                
                logger.info(f"   ‚úÖ Defect analysis complete")
                logger.info(f"   Events analyzed: {len(impact_results)}")
                logger.info(f"   Total defect atoms: {total_defects}")
                logger.info(f"   Network links: {total_links}")
                
                # Ê¨†Èô•„Çø„Ç§„ÉóÂàÜÂ∏É
                defect_types = Counter()
                for result in impact_results.values():
                    if hasattr(result, 'dominant_defect') and result.dominant_defect:
                        defect_types[result.dominant_defect] += 1
                
                if defect_types:
                    logger.info("   Defect types:")
                    for dtype, count in defect_types.most_common():
                        logger.info(f"     - {dtype}: {count}")
                
                # ÊúÄÂ§ßÂøúÂäõÈõÜ‰∏≠
                max_stress = max(
                    (getattr(r, 'max_stress_concentration', 0) 
                     for r in impact_results.values()),
                    default=0
                )
                if max_stress > 0:
                    logger.info(f"   Max stress concentration: {max_stress:.2f} GPa")
                
                # ‰∏≠Èñì‰øùÂ≠ò
                if save_intermediate:
                    with open(output_path / 'impact_results.pkl', 'wb') as f:
                        pickle.dump(impact_results, f)
                    logger.info("   Saved: impact_results.pkl")
            else:
                logger.warning("   No impact results generated")
            
        except Exception as e:
            logger.error(f"Impact analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            impact_results = None
    
    # ========================================
    # Step 5: Áµ±Âêà„É¨„Éù„Éº„ÉàÁîüÊàêÔºàÂÆåÂÖ®ÁâàÔºâ
    # ========================================
    logger.info("\nüìù Generating comprehensive material report...")
    
    try:
        # sorted_events„Çí3Ë¶ÅÁ¥†„Çø„Éó„É´„Å´Â§âÊèõÔºà„É¨„Éù„Éº„ÉàÁî®Ôºâ
        sorted_events_for_report = []
        for item in sorted_events:
            if len(item) >= 3:
                start, end, score = item[0], item[1], item[2]
                sorted_events_for_report.append((start, end, score))
        
        # „Éá„Éê„ÉÉ„Ç∞: Ê∏°„Åô„Éá„Éº„Çø„ÅÆÁ¢∫Ë™ç
        logger.info("   Report generation inputs:")
        logger.info(f"     - macro_result: {macro_result is not None}")
        if macro_result:
            logger.info(f"       - material_events: {len(macro_result.material_events) if macro_result.material_events else 0}")
            logger.info(f"       - stress_strain: {macro_result.stress_strain is not None}")
            logger.info(f"       - anomaly_scores: {macro_result.anomaly_scores is not None}")
            logger.info(f"       - failure_prediction: {macro_result.failure_prediction is not None}")
        logger.info(f"     - two_stage_result: {two_stage_result is not None}")
        logger.info(f"     - impact_results: {len(impact_results) if impact_results else 0}")
        logger.info(f"     - sorted_events: {len(sorted_events_for_report)}")
        
        # „É¨„Éù„Éº„ÉàÁîüÊàê
        report = generate_material_report_from_results(
            macro_result=macro_result,
            two_stage_result=two_stage_result,
            impact_results=impact_results,
            sorted_events=sorted_events_for_report,
            metadata=metadata,
            material_type=material_type,
            output_dir=str(output_path),
            verbose=verbose,
            debug=True  # „Éá„Éê„ÉÉ„Ç∞„É¢„Éº„ÉâON
        )
        
        logger.info(f"   ‚úÖ Report generated successfully")
        logger.info(f"   Report length: {len(report):,} characters")
        
        # „É¨„Éù„Éº„Éà‰øùÂ≠òÁ¢∫Ë™ç
        report_path = output_path / 'material_report.md'
        if report_path.exists():
            logger.info(f"   Report saved to: {report_path}")
        
        # Áµ±ÂêàÁµêÊûú„ÅÆJSON‰øùÂ≠ò
        summary_data = {
            'material_type': material_type,
            'metadata': metadata,
            'n_frames': n_frames,
            'n_atoms': n_atoms,
            'n_events': len(material_events),
            'event_types': dict(Counter(e[2] for e in material_events if len(e) >= 3)),
            'analysis_complete': True,
            'report_generated': True,
            'report_length': len(report)
        }
        
        # Ëß£ÊûêÁµêÊûú„ÅÆËøΩÂä†
        if macro_result:
            if macro_result.failure_prediction:
                summary_data['failure_prediction'] = {
                    'probability': macro_result.failure_prediction.get('failure_probability', 0),
                    'mode': macro_result.failure_prediction.get('failure_mode', 'unknown')
                }
            if macro_result.stress_strain:
                summary_data['max_stress'] = macro_result.stress_strain.get('max_stress', 0)
        
        if two_stage_result:
            if hasattr(two_stage_result, 'material_state'):
                summary_data['material_state'] = two_stage_result.material_state.get('state', 'unknown')
            if hasattr(two_stage_result, 'critical_clusters'):
                summary_data['n_critical_clusters'] = len(two_stage_result.critical_clusters)
        
        if impact_results:
            summary_data['n_defect_analyses'] = len(impact_results)
        
        with open(output_path / 'analysis_summary.json', 'w') as f:
            json.dump(summary_data, f, indent=2)
        logger.info("   Saved: analysis_summary.json")
        
    except Exception as e:
        logger.error(f"Report generation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        report = None
    
    # ========================================
    # Step 6: ÂèØË¶ñÂåñÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
    # ========================================
    if enable_visualization:
        logger.info("\nüìä Creating visualizations...")
        
        try:
            # ÂêÑÁ®ÆÂèØË¶ñÂåñÈñ¢Êï∞„ÅÆÂëº„Å≥Âá∫„Åó
            # (Êó¢Â≠ò„ÅÆÂèØË¶ñÂåñ„Ç≥„Éº„Éâ„ÅØ„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®)
            
            # „Ç§„Éô„É≥„Éà„Çø„Ç§„É†„É©„Ç§„É≥
            if material_events:
                fig = visualize_event_timeline(
                    material_events,
                    save_path=str(output_path / 'event_timeline.png')
                )
                logger.info("   Event timeline visualized")
            
            # ÂøúÂäõ-Ê≠™„ÅøÊõ≤Á∑ö
            if macro_result and macro_result.stress_strain:
                fig = visualize_stress_strain(
                    macro_result.stress_strain,
                    material_type,
                    save_path=str(output_path / 'stress_strain.png')
                )
                logger.info("   Stress-strain curve visualized")
            
            # „ÇØ„É©„Çπ„Çø„Éº„ÉÄ„É°„Éº„Ç∏„Éû„ÉÉ„Éó
            if two_stage_result:
                fig = visualize_cluster_damage(
                    two_stage_result,
                    save_path=str(output_path / 'cluster_damage.png')
                )
                logger.info("   Cluster damage map visualized")
            
            # Ê¨†Èô•„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ
            if impact_results:
                fig = visualize_defect_network(
                    impact_results,
                    save_path=str(output_path / 'defect_network.png')
                )
                logger.info("   Defect network visualized")
            
            logger.info("   ‚úÖ All visualizations completed")
            
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
    
    # ========================================
    # ÂÆå‰∫Ü
    # ========================================
    logger.info("\n" + "="*70)
    logger.info("‚úÖ MATERIAL ANALYSIS PIPELINE COMPLETE!")
    logger.info(f"   Material: {material_type}")
    logger.info(f"   Output directory: {output_path}")
    logger.info("   Summary:")
    
    if macro_result:
        logger.info(f"     ‚úì {len(material_events)} material events detected")
        if macro_result.failure_prediction:
            fp = macro_result.failure_prediction.get('failure_probability', 0)
            logger.info(f"     ‚úì Failure probability: {fp:.1%}")
    
    if two_stage_result:
        if hasattr(two_stage_result, 'material_state'):
            state = two_stage_result.material_state.get('state', 'unknown')
            logger.info(f"     ‚úì Material state: {state}")
        if hasattr(two_stage_result, 'critical_clusters'):
            n_crit = len(two_stage_result.critical_clusters)
            logger.info(f"     ‚úì {n_crit} critical clusters identified")
    
    if impact_results:
        logger.info(f"     ‚úì {len(impact_results)} atomic defect analyses completed")
    
    if report:
        logger.info(f"     ‚úì Report generated ({len(report):,} characters)")
    
    logger.info("="*70)
    
    return {
        'macro_result': macro_result,
        'two_stage_result': two_stage_result,
        'impact_results': impact_results,
        'sorted_events': sorted_events_for_report,
        'material_events': material_events,
        'report': report,
        'output_dir': output_path,
        'material_type': material_type,
        'metadata': metadata,
        'success': True
    }

# ============================================
# „Éò„É´„Éë„ÉºÈñ¢Êï∞ÔºàÊó¢Â≠ò„ÅÆ„ÇÇ„ÅÆ„ÇíÁ∂≠ÊåÅÔºâ
# ============================================

def get_material_parameters(material_type: str) -> Dict:
    """ÊùêÊñô„Éë„É©„É°„Éº„ÇøÂèñÂæó"""
    materials = {
        'SUJ2': {
            'E': 210.0,      # GPa
            'nu': 0.3,       # Poisson's ratio
            'yield': 1.5,    # GPa
            'ultimate': 2.0, # GPa
            'K_IC': 30.0,    # MPa‚àöm
            'density': 7.85  # g/cm¬≥
        },
        'AL7075': {
            'E': 71.7,
            'nu': 0.33,
            'yield': 0.503,
            'ultimate': 0.572,
            'K_IC': 23.0,
            'density': 2.81
        },
        'Ti6Al4V': {
            'E': 113.8,
            'nu': 0.342,
            'yield': 0.88,
            'ultimate': 0.95,
            'K_IC': 75.0,
            'density': 4.43
        },
        'SS316L': {
            'E': 193.0,
            'nu': 0.3,
            'yield': 0.205,
            'ultimate': 0.515,
            'K_IC': 112.0,
            'density': 8.0
        }
    }
    
    return materials.get(material_type, materials['SUJ2'])

def compute_strain_field_from_trajectory(
    trajectory: np.ndarray,
    material_events: List[Tuple[int, int, str]],
    lattice_constant: float = 2.87  # BCCÈâÑ„ÅÆ„Éá„Éï„Ç©„É´„Éà
) -> np.ndarray:
    """
    „Éà„É©„Ç∏„Çß„ÇØ„Éà„É™„Åã„ÇâÊ≠™„ÅøÂ†¥„ÇíÁ∞°ÊòìË®àÁÆó
    
    Parameters
    ----------
    trajectory : np.ndarray
        ÂéüÂ≠ê„Éà„É©„Ç∏„Çß„ÇØ„Éà„É™ (n_frames, n_atoms, 3)
    material_events : List[Tuple[int, int, str]]
        ÊùêÊñô„Ç§„Éô„É≥„Éà„É™„Çπ„ÉàÔºàÊ≠™„ÅøÊé®ÂÆö„ÅÆÂèÇËÄÉÔºâ
    lattice_constant : float
        Ê†ºÂ≠êÂÆöÊï∞Ôºà√ÖÔºâ- Ê≠£Ë¶èÂåñÁî®
    
    Returns
    -------
    np.ndarray
        ÂêÑÂéüÂ≠ê„ÅÆÂπ≥ÂùáÊ≠™„ÅøÂ†¥ (n_atoms,)
    """
    n_frames, n_atoms = trajectory.shape[:2]
    strain_field = np.zeros(n_atoms)
    
    # „Éï„É¨„Éº„É†Èñì„ÅÆÁ¥ØÁ©çÊ≠™„ÅøË®àÁÆó
    for i in range(1, n_frames):
        # ÂêÑÂéüÂ≠ê„ÅÆ„Éï„É¨„Éº„É†ÈñìÂ§â‰Ωç
        displacement = trajectory[i] - trajectory[i-1]
        displacement_norm = np.linalg.norm(displacement, axis=1)
        
        # Ê†ºÂ≠êÂÆöÊï∞„ÅßÊ≠£Ë¶èÂåñ„Åó„Å¶Ê≠™„Åø„Å´Â§âÊèõ
        frame_strain = displacement_norm / lattice_constant
        
        # Á¥ØÁ©ç
        strain_field += frame_strain
    
    # Âπ≥ÂùáÂåñ
    strain_field = strain_field / (n_frames - 1) if n_frames > 1 else strain_field
    
    # „Ç§„Éô„É≥„Éà„Éô„Éº„Çπ„ÅÆË£úÊ≠£Ôºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ
    if material_events:
        # Â°ëÊÄß„ÉªËª¢‰Ωç„Ç§„Éô„É≥„Éà„Åå„ÅÇ„ÇãÈ†òÂüü„ÇíÂº∑Ë™ø
        event_mask = np.zeros(n_frames, dtype=bool)
        for start, end, event_type in material_events:
            if any(key in event_type for key in ['plastic', 'dislocation', 'crack']):
                event_mask[start:min(end+1, n_frames)] = True
        
        # „Ç§„Éô„É≥„ÉàÁô∫Áîü„Éï„É¨„Éº„É†„Åß„ÅÆËøΩÂä†Ê≠™„Åø
        if np.any(event_mask):
            event_frames = np.where(event_mask)[0]
            for frame in event_frames:
                if frame > 0 and frame < n_frames:
                    event_displacement = np.linalg.norm(
                        trajectory[frame] - trajectory[frame-1], axis=1
                    )
                    # „Ç§„Éô„É≥„ÉàÈ†òÂüü„ÅØÊ≠™„Åø„ÇíÂ¢óÂπÖ
                    strain_field += event_displacement / lattice_constant * 0.5
    
    # „ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÔºàÁâ©ÁêÜÁöÑ„Å´Â¶•ÂΩì„Å™ÁØÑÂõ≤Ôºâ
    strain_field = np.clip(strain_field, 0, 0.5)  # ÊúÄÂ§ß50%Ê≠™„Åø
    
    return strain_field

# ÂèØË¶ñÂåñÈñ¢Êï∞ÔºàÊó¢Â≠ò„ÅÆ„ÇÇ„ÅÆ„Çí„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®Ôºâ
def visualize_stress_strain(curve_data: Dict, material_type: str,
                          save_path: Optional[str] = None) -> plt.Figure:
    """ÂøúÂäõ-Ê≠™„ÅøÊõ≤Á∑ö„ÅÆÂèØË¶ñÂåñ"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    if 'strain' in curve_data and 'stress' in curve_data:
        strain = curve_data['strain']
        stress = curve_data['stress']
        
        ax.plot(strain, stress, 'b-', linewidth=2, label=material_type)
        
        if 'yield_point' in curve_data:
            yield_idx = curve_data['yield_point']
            ax.plot(strain[yield_idx], stress[yield_idx], 'ro',
                   markersize=10, label='Yield Point')
        
        if 'fracture_point' in curve_data:
            frac_idx = curve_data['fracture_point']
            ax.plot(strain[frac_idx], stress[frac_idx], 'rx',
                   markersize=12, label='Fracture')
        
        ax.set_xlabel('Strain', fontsize=12)
        ax.set_ylabel('Stress (GPa)', fontsize=12)
        ax.set_title(f'Stress-Strain Curve - {material_type}', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

def visualize_event_timeline(events: List, save_path: Optional[str] = None) -> plt.Figure:
    """„Ç§„Éô„É≥„Éà„Çø„Ç§„É†„É©„Ç§„É≥„ÅÆÂèØË¶ñÂåñ"""
    fig, ax = plt.subplots(figsize=(14, 6))
    
    colors = {
        'elastic_deformation': 'green',
        'uniform_deformation': 'lightgreen',
        'plastic_deformation': 'yellow',
        'dislocation_nucleation': 'orange',
        'dislocation_avalanche': 'darkorange',
        'defect_migration': 'coral',
        'crack_initiation': 'red',
        'crack_propagation': 'darkred',
        'phase_transition': 'purple',
        'transition_state': 'mediumpurple',
        'fatigue_damage': 'brown'
    }
    
    y_positions = {}
    y_counter = 0
    
    for event in events:
        if isinstance(event, tuple) and len(event) >= 3:
            start, end, event_type = event[:3]
            
            if event_type not in y_positions:
                y_positions[event_type] = y_counter
                y_counter += 1
            
            y = y_positions[event_type]
            color = colors.get(event_type, 'gray')
            
            ax.barh(y, end - start, left=start, height=0.8,
                   color=color, alpha=0.7, edgecolor='black')
    
    ax.set_yticks(list(y_positions.values()))
    ax.set_yticklabels(list(y_positions.keys()))
    ax.set_xlabel('Frame', fontsize=12)
    ax.set_title('Material Event Timeline', fontsize=14)
    ax.grid(True, alpha=0.3, axis='x')
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

def visualize_cluster_damage(two_stage_result: Any,
                            save_path: Optional[str] = None) -> plt.Figure:
    """„ÇØ„É©„Çπ„Çø„Éº„ÉÄ„É°„Éº„Ç∏„Éû„ÉÉ„Éó„ÅÆÂèØË¶ñÂåñ"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    ax1 = axes[0]
    if hasattr(two_stage_result, 'critical_clusters'):
        critical = two_stage_result.critical_clusters[:20]
        if critical:
            ax1.bar(range(len(critical)), [1]*len(critical), color='red', alpha=0.7)
            ax1.set_xticks(range(len(critical)))
            ax1.set_xticklabels([f"C{c}" for c in critical], rotation=45)
            ax1.set_ylabel('Criticality', fontsize=12)
            ax1.set_title('Critical Clusters', fontsize=12)
    
    ax2 = axes[1]
    if hasattr(two_stage_result, 'global_cluster_importance'):
        top_clusters = sorted(
            two_stage_result.global_cluster_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        if top_clusters:
            cluster_ids = [f"C{c[0]}" for c in top_clusters]
            scores = [c[1] for c in top_clusters]
            
            ax2.barh(cluster_ids, scores, color='steelblue', alpha=0.7)
            ax2.set_xlabel('Importance Score', fontsize=12)
            ax2.set_title('Cluster Importance Ranking', fontsize=12)
            ax2.invert_yaxis()
    
    plt.suptitle('Cluster Damage Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

def visualize_defect_network(impact_results: Dict,
                            save_path: Optional[str] = None) -> plt.Figure:
    """Ê¨†Èô•„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂèØË¶ñÂåñ"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    ax1 = axes[0, 0]
    defect_types = Counter()
    for result in impact_results.values():
        if hasattr(result, 'dominant_defect') and result.dominant_defect:
            defect_types[result.dominant_defect] += 1
    
    if defect_types:
        ax1.pie(defect_types.values(), labels=defect_types.keys(),
               autopct='%1.1f%%', startangle=90)
        ax1.set_title('Defect Type Distribution')
    
    ax2 = axes[0, 1]
    stress_concs = []
    for r in impact_results.values():
        if hasattr(r, 'max_stress_concentration'):
            stress_concs.append(r.max_stress_concentration)
    
    if stress_concs:
        ax2.hist(stress_concs, bins=20, color='red', alpha=0.7, edgecolor='black')
        ax2.set_xlabel('Stress Concentration (GPa)')
        ax2.set_ylabel('Count')
        ax2.set_title('Stress Concentration Distribution')
        ax2.grid(True, alpha=0.3)
    
    ax3 = axes[1, 0]
    patterns = []
    for result in impact_results.values():
        if hasattr(result, 'defect_network') and result.defect_network:
            if hasattr(result.defect_network, 'network_pattern'):
                patterns.append(result.defect_network.network_pattern)
    
    if patterns:
        pattern_counts = Counter(patterns)
        ax3.bar(pattern_counts.keys(), pattern_counts.values(),
               color='green', alpha=0.7)
        ax3.set_xlabel('Network Pattern')
        ax3.set_ylabel('Count')
        ax3.set_title('Defect Network Patterns')
    
    ax4 = axes[1, 1]
    plastic_zones = []
    for r in impact_results.values():
        if hasattr(r, 'plastic_zone_size'):
            plastic_zones.append(r.plastic_zone_size)
    
    if plastic_zones:
        ax4.hist(plastic_zones, bins=20, color='orange', alpha=0.7, edgecolor='black')
        ax4.set_xlabel('Plastic Zone Size (√Ö)')
        ax4.set_ylabel('Count')
        ax4.set_title('Plastic Zone Distribution')
        ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Atomic Defect Network Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

# CLI InterfaceÔºàÊó¢Â≠ò„ÅÆ„ÇÇ„ÅÆ„ÇíÁ∂≠ÊåÅÔºâ
def main():
    """„Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"""
    parser = argparse.ArgumentParser(
        description='Material Full Analysis Pipeline - Lambda¬≥ GPU (REFACTORED v3.1)'
    )
    
    parser.add_argument('trajectory', help='Path to trajectory file (.npy)')
    parser.add_argument('atom_types', help='Path to atom types file (.npy)')
    parser.add_argument('--material', '-m', default='SUJ2',
                       choices=['SUJ2', 'AL7075', 'Ti6Al4V', 'SS316L'],
                       help='Material type')
    parser.add_argument('--metadata', help='Path to metadata file (.json)')
    parser.add_argument('--clusters', required=True,  # ‚Üê ÂøÖÈ†à„Å´ÔºÅ
                   help='Path to cluster definition file (REQUIRED). '
                        'Must define Cluster 0 as healthy region and others as defects.')
    parser.add_argument('--backbone', '-b',
                       help='Path to backbone_indices file (.npy) - pre-computed defect atoms')
    parser.add_argument('--strain', help='Path to strain field data (.npy)')
    parser.add_argument('--loading', '-l', default='tensile',
                       choices=['tensile', 'compression', 'shear', 'fatigue'])
    parser.add_argument('--strain-rate', type=float, default=1e-3)
    parser.add_argument('--temperature', '-T', type=float, default=300.0)
    parser.add_argument('--output', '-o', default='./material_results')
    parser.add_argument('--no-two-stage', action='store_true')
    parser.add_argument('--no-impact', action='store_true')
    parser.add_argument('--no-viz', action='store_true')
    parser.add_argument('--verbose', '-v', action='store_true')
    parser.add_argument('--save-intermediate', action='store_true',
                       help='Save intermediate results as pickle files')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    metadata_path = args.metadata if args.metadata else 'metadata_auto.json'
    
    try:
        results = run_material_analysis_pipeline(
            trajectory_path=args.trajectory,
            metadata_path=metadata_path,
            atom_types_path=args.atom_types,
            material_type=args.material,
            cluster_definition_path=args.clusters,
            backbone_indices_path=args.backbone, 
            strain_field_path=args.strain,
            enable_two_stage=not args.no_two_stage,
            enable_impact=not args.no_impact,
            enable_visualization=not args.no_viz,
            output_dir=args.output,
            loading_type=args.loading,
            strain_rate=args.strain_rate,
            temperature=args.temperature,
            verbose=args.verbose,
            save_intermediate=args.save_intermediate
        )
        
        if results and results.get('success'):
            print(f"\n‚úÖ Success! Results saved to: {results['output_dir']}")
            return 0
        else:
            print("\n‚ùå Pipeline failed. Check logs for details.")
            return 1
            
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == '__main__':
    exit(main())
