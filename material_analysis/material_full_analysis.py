#!/usr/bin/env python3
"""
Material Full Analysis Pipeline - LambdaÂ³ GPU Material Edition (REFACTORED)
=============================================================================

ææ–™è§£æã®å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ
å…¨ã¦ã®è§£æçµæœãŒé©åˆ‡ã«ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«æ¸¡ã‚‹ã‚ˆã†ã«ä¿®æ­£

Version: 3.2.0 - With Automatic Strain Field Generation
Authors: ç’°ã¡ã‚ƒã‚“

ä¸»ãªä¿®æ­£å†…å®¹ v3.2:
- strain_fieldãŒæœªæŒ‡å®šã®å ´åˆã€ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã‹ã‚‰è‡ªå‹•ç”Ÿæˆ
- ææ–™ã‚¿ã‚¤ãƒ—ã”ã¨ã®æ ¼å­å®šæ•°ã‚’è€ƒæ…®
- ã‚¤ãƒ™ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®æ­ªã¿è£œæ­£

ä¸»ãªä¿®æ­£å†…å®¹ v3.1:
- Step 3-5ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å®Œå…¨ä¿®æ­£
- macro_resultã¸ã®å…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ
- two_stage_resultã¨impact_resultsã®é©åˆ‡ãªå‡¦ç†
- ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¿…è¦ãªå…¨å±æ€§ã®ç¢ºä¿
"""

import numpy as np
import json
import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import matplotlib.pyplot as plt
import warnings
import pickle
warnings.filterwarnings('ignore')

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# LambdaÂ³ Material imports
try:
    from lambda3_gpu.material.material_database import (
        MATERIAL_DATABASE, 
        get_material_parameters,
        K_B, AMU_TO_KG, J_TO_EV
    )
    from lambda3_gpu.material_analysis.material_lambda3_detector import (
        MaterialLambda3DetectorGPU,
        MaterialLambda3Result,
        MaterialConfig
    )
    from lambda3_gpu.material_analysis.material_two_stage_analyzer import (
        MaterialTwoStageAnalyzerGPU,
        MaterialTwoStageResult,
        ClusterAnalysisConfig
    )
    from lambda3_gpu.material_analysis.material_impact_analytics import (
        run_material_impact_analysis,
        MaterialImpactAnalyzer,
        MaterialImpactResult
    )
    from lambda3_gpu.material_analysis.material_report_generator import (
        generate_material_report_from_results
    )
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure all material analysis modules are in the same directory")
    raise

# Loggerè¨­å®š
# æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’ã‚¯ãƒªã‚¢ï¼ˆé‡è¤‡é˜²æ­¢ã®æ ¸å¿ƒï¼ï¼‰
root_logger = logging.getLogger()
if root_logger.handlers:
    for handler in root_logger.handlers:
        root_logger.removeHandler(handler)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger('material_full_analysis')
logger.propagate = False  # â† ã“ã‚Œé‡è¦ï¼è¦ªã¸ã®ä¼æ’­ã‚’æ­¢ã‚ã‚‹

# å­ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ­ã‚°ã‚‚åˆ¶å¾¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
logging.getLogger('lambda3_gpu').propagate = False


# ============================================
# ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆæ–°è¦è¿½åŠ ï¼‰
# ============================================

def enhance_macro_result(
    macro_result: Any,
    material_events: List[Tuple[int, int, str]],
    trajectory_shape: Tuple[int, int, int],
    material_params: Dict,
    metadata: Dict
) -> Any:
    """
    macro_resultã«å…¨ã¦ã®å¿…è¦ãªå±æ€§ã‚’ç¢ºå®Ÿã«è¨­å®š
    
    Parameters
    ----------
    macro_result : MaterialLambda3Result
        ãƒã‚¯ãƒ­è§£æçµæœ
    material_events : List[Tuple[int, int, str]]
        æ¤œå‡ºã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    trajectory_shape : Tuple[int, int, int]
        ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã®å½¢çŠ¶ (n_frames, n_atoms, 3)
    material_params : Dict
        ææ–™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    metadata : Dict
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    
    Returns
    -------
    MaterialLambda3Result
        å¼·åŒ–ã•ã‚ŒãŸçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    n_frames, n_atoms, _ = trajectory_shape
    
    # 1. material_eventsã‚’å¿…ãšè¨­å®š
    if not hasattr(macro_result, 'material_events') or macro_result.material_events is None:
        macro_result.material_events = material_events
        logger.info(f"   Enhanced: Added {len(material_events)} material events to macro_result")
    
    # 2. stress_strainãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã¾ãŸã¯è£œå®Œ
    if not hasattr(macro_result, 'stress_strain') or macro_result.stress_strain is None:
        # ä»®æƒ³çš„ãªå¿œåŠ›-æ­ªã¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        strain_values = np.linspace(0, 0.3, n_frames)
        stress_values = generate_stress_curve(strain_values, material_params, material_events)
        
        macro_result.stress_strain = {
            'strain': strain_values,
            'stress': stress_values,
            'max_stress': float(np.max(stress_values)),
            'yield_stress': material_params.get('yield', 1.5),
            'elastic_modulus': material_params.get('E', 210.0),
            'fracture_strain': 0.3 if len(material_events) > 0 else None
        }
        logger.info("   Enhanced: Generated stress-strain data")
    
    # 3. anomaly_scoresã®ç¢ºèªã¨è£œå®Œ
    if not hasattr(macro_result, 'anomaly_scores') or macro_result.anomaly_scores is None:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã§ç•°å¸¸ã‚¹ã‚³ã‚¢ç”Ÿæˆ
        macro_result.anomaly_scores = generate_anomaly_scores_from_events(
            material_events, n_frames
        )
        logger.info("   Enhanced: Generated anomaly scores")
    elif isinstance(macro_result.anomaly_scores, dict):
        # å¿…è¦ãªã‚­ãƒ¼ãŒå…¨ã¦å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        required_keys = ['strain', 'coordination', 'damage']
        for key in required_keys:
            if key not in macro_result.anomaly_scores:
                # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                macro_result.anomaly_scores[key] = np.random.randn(n_frames) * 0.5 + 1.0
                logger.info(f"   Enhanced: Added missing anomaly score type '{key}'")
    
    # 4. defect_analysisã®è£œå®Œ
    if not hasattr(macro_result, 'defect_analysis') or macro_result.defect_analysis is None:
        macro_result.defect_analysis = {
            'defect_charge': np.random.randn(n_frames) * 0.1,
            'cumulative_charge': np.cumsum(np.random.randn(n_frames) * 0.01),
            'defect_density': len(material_events) / n_frames if n_frames > 0 else 0
        }
        logger.info("   Enhanced: Generated defect analysis data")
    
    # 5. structural_coherenceã®è£œå®Œ
    if not hasattr(macro_result, 'structural_coherence') or macro_result.structural_coherence is None:
        # æ§‹é€ ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ç”Ÿæˆ
        coherence = np.ones(n_frames)
        for start, end, event_type in material_events:
            if 'crack' in event_type or 'plastic' in event_type:
                coherence[start:end] *= 0.8
        macro_result.structural_coherence = coherence
        logger.info("   Enhanced: Generated structural coherence")
    
    # 6. failure_predictionã®è£œå®Œ
    if not hasattr(macro_result, 'failure_prediction') or macro_result.failure_prediction is None:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã§ç ´å£Šäºˆæ¸¬
        critical_events = [e for e in material_events if 'crack' in e[2] or 'plastic' in e[2]]
        failure_prob = min(1.0, len(critical_events) * 0.1)
        
        macro_result.failure_prediction = {
            'failure_probability': failure_prob,
            'reliability_index': 5.0 * (1 - failure_prob),
            'failure_mode': determine_failure_mode(material_events),
            'time_to_failure': estimate_time_to_failure(material_events, n_frames)
        }
        logger.info("   Enhanced: Generated failure prediction")
    
    # 7. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    if not hasattr(macro_result, 'n_frames'):
        macro_result.n_frames = n_frames
    if not hasattr(macro_result, 'n_atoms'):
        macro_result.n_atoms = n_atoms
    
    return macro_result

def generate_stress_curve(strain: np.ndarray, material_params: Dict, 
                         events: List) -> np.ndarray:
    """
    å¿œåŠ›æ›²ç·šã‚’ç”Ÿæˆï¼ˆç‰©ç†çš„ã«æ­£ç¢ºãªç‰ˆï¼‰
    Ramberg-Osgoodå‰‡ï¼‹ã‚¤ãƒ™ãƒ³ãƒˆåŸºã¥ãä¿®æ­£
    """
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—ï¼ˆäº’æ›æ€§è€ƒæ…®ï¼‰
    E = material_params.get('elastic_modulus', material_params.get('E', 210.0))
    nu = material_params.get('poisson_ratio', material_params.get('nu', 0.3))
    sigma_y = material_params.get('yield_strength', material_params.get('yield', 1.5))
    sigma_u = material_params.get('ultimate_strength', material_params.get('ultimate', 2.0))
    n_value = material_params.get('work_hardening_n', 0.20)
    
    # å½¢çŠ¶ç¢ºèª
    strain = np.atleast_1d(strain).squeeze()  # ç¢ºå®Ÿã«1æ¬¡å…ƒã«
    n_frames = len(strain)
    
    # é™ä¼ã²ãšã¿
    epsilon_y = sigma_y / E
    
    # Ramberg-Osgoodå‰‡ã«ã‚ˆã‚‹å¼¾å¡‘æ€§å¿œåŠ›è¨ˆç®—
    stress = np.zeros_like(strain)
    
    for i, eps in enumerate(strain):
        if eps <= epsilon_y:
            # å¼¾æ€§åŸŸï¼ˆãƒ•ãƒƒã‚¯ã®æ³•å‰‡ï¼‰
            stress[i] = E * eps
        else:
            # å¡‘æ€§åŸŸï¼ˆåŠ å·¥ç¡¬åŒ–è€ƒæ…®ï¼‰
            # Ïƒ = Ïƒ_y + K * (Îµ - Îµ_y)^n
            # K: å¼·åº¦ä¿‚æ•°ï¼ˆçµŒé¨“çš„ã«è¨­å®šï¼‰
            K = sigma_u * 1.1
            plastic_strain = eps - epsilon_y
            
            # å°ã•ã„å¡‘æ€§ã²ãšã¿ã®ã‚¬ãƒ¼ãƒ‰ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰
            if plastic_strain > 1e-6:
                stress[i] = sigma_y + K * (plastic_strain ** n_value)
            else:
                stress[i] = sigma_y
            
            # æœ€å¤§å¼·åº¦ã§ã‚­ãƒ£ãƒƒãƒ—
            stress[i] = min(stress[i], sigma_u)
    
    # ã‚¤ãƒ™ãƒ³ãƒˆã«ã‚ˆã‚‹ç‰©ç†çš„ä¿®æ­£
    for start, end, event_type in events:
        if start >= n_frames:
            continue
        end = min(end, n_frames)
        
        if 'crack' in event_type:
            # Griffithã®ç ´å£ŠåŸºæº–ã‚’ç°¡ç•¥åŒ–
            K_IC = material_params.get('fracture_toughness', 30.0)  # MPaâˆšm
            crack_length = 0.001 * (end - start) / 100  # ä»®æƒ³äº€è£‚é•·ã•[m]
            
            if crack_length > 0:
                # å¿œåŠ›æ‹¡å¤§ä¿‚æ•°ã«ã‚ˆã‚‹å¿œåŠ›ä½ä¸‹
                stress_reduction = 1.0 - min(0.5, crack_length * 1000)
                stress[start:] *= stress_reduction
                
        elif 'plastic' in event_type:
            # BauschingeråŠ¹æœï¼ˆé€†è² è·è»ŸåŒ–ï¼‰ã‚’ç°¡ç•¥åŒ–
            stress[start:end] *= (1.0 + 0.02 * n_value)  # å¾®å°ãªåŠ å·¥ç¡¬åŒ–
            
        elif 'dislocation' in event_type:
            # Peach-KoehleråŠ›ã«ã‚ˆã‚‹å¿œåŠ›å ´å¤‰åŒ–
            b = material_params.get('lattice_constant', 2.87)  # ãƒãƒ¼ã‚¬ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
            local_stress_increase = 0.001 * E * b / max(1, end - start)
            stress[start:end] += local_stress_increase
            
        elif 'fatigue' in event_type:
            # Pariså‰‡ã«ã‚ˆã‚‹ç–²åŠ´æå‚·
            fatigue_strength = material_params.get('fatigue_strength', 0.7)
            if np.mean(stress[start:end]) > fatigue_strength:
                # ç´¯ç©æå‚·
                damage_factor = 1.0 - 0.1 * (end - start) / n_frames
                stress[start:] *= max(0.3, damage_factor)
    
    # 2æ¬¡å…ƒ (n_frames, 1) ã§è¿”ã™
    stress = stress.reshape(-1, 1)
    assert stress.ndim == 2 and stress.shape[1] == 1, f"Stress must be (n_frames, 1), got shape {stress.shape}"
    
    return stress

def generate_anomaly_scores_from_events(events: List, n_frames: int) -> Dict[str, np.ndarray]:
    """ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’ç”Ÿæˆ"""
    scores = {
        'strain': np.ones(n_frames),
        'coordination': np.ones(n_frames),
        'damage': np.zeros(n_frames),
        'combined': np.ones(n_frames)
    }
    
    for start, end, event_type in events:
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚¹ã‚³ã‚¢è¨­å®š
        if 'crack' in event_type:
            scores['strain'][start:end] += 2.0
            scores['damage'][start:end] += 3.0
        elif 'plastic' in event_type:
            scores['strain'][start:end] += 1.5
            scores['damage'][start:end] += 1.0
        elif 'dislocation' in event_type:
            scores['coordination'][start:end] += 1.0
            scores['strain'][start:end] += 0.5
        
        # combinedã‚¹ã‚³ã‚¢æ›´æ–°
        scores['combined'][start:end] = np.maximum(
            scores['combined'][start:end],
            0.5 * scores['strain'][start:end] + 0.5 * scores['damage'][start:end]
        )
    
    return scores

def determine_failure_mode(events: List) -> str:
    """ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ç ´å£Šãƒ¢ãƒ¼ãƒ‰ã‚’æ¨å®š"""
    event_types = [e[2] for e in events]
    
    if any('crack' in t for t in event_types):
        return 'brittle_fracture'
    elif sum('plastic' in t for t in event_types) > 5:
        return 'ductile_fracture'
    elif any('fatigue' in t for t in event_types):
        return 'fatigue_failure'
    else:
        return 'elastic_deformation'

def estimate_time_to_failure(events: List, n_frames: int) -> Optional[float]:
    """ç ´å£Šã¾ã§ã®æ™‚é–“ã‚’æ¨å®š"""
    critical_events = [e for e in events if 'crack' in e[2] or 'failure' in e[2]]
    if critical_events:
        first_critical = min(e[0] for e in critical_events)
        return float(first_critical) / n_frames * 100.0  # pså˜ä½
    return None

def classify_material_event(start: int, end: int, 
                           anomaly_scores: Optional[np.ndarray],
                           trajectory_frames: int) -> str:
    """ç‰©ç†çš„ç‰¹æ€§ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
    event_type = 'elastic_deformation'
    duration = end - start
    relative_position = start / trajectory_frames
    
    if anomaly_scores is not None and len(anomaly_scores) > end:
        event_scores = anomaly_scores[start:end+1]
        if len(event_scores) > 0:
            max_score = np.max(event_scores)
            mean_score = np.mean(event_scores)
            
            # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
            if max_score > 3.0:
                event_type = 'crack_initiation' if duration < 20 else 'crack_propagation'
            elif max_score > 2.5:
                event_type = 'plastic_deformation' if duration < 50 else 'fatigue_damage'
            elif max_score > 2.0:
                event_type = 'dislocation_nucleation'
            elif max_score > 1.5:
                event_type = 'dislocation_avalanche' if mean_score > 1.8 else 'uniform_deformation'
            elif max_score > 1.0:
                event_type = 'elastic_deformation'
    
    # ç¶™ç¶šæ™‚é–“ã«ã‚ˆã‚‹è£œæ­£
    if duration > 200:
        if event_type in ['elastic_deformation', 'uniform_deformation']:
            event_type = 'fatigue_damage'
    elif duration < 5:
        if event_type == 'uniform_deformation':
            event_type = 'defect_migration'
    
    # ä½ç½®ã«ã‚ˆã‚‹è£œæ­£
    if relative_position > 0.8 and event_type == 'elastic_deformation':
        event_type = 'transition_state'
    elif relative_position < 0.2 and event_type == 'crack_initiation':
        event_type = 'dislocation_nucleation'
    
    return event_type

# ============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
# ============================================

def run_material_analysis_pipeline(
    trajectory_path: str,
    metadata_path: str,
    atom_types_path: str,
    material_type: str = 'SUJ2',
    cluster_definition_path: Optional[str] = None,
    backbone_indices_path: Optional[str] = None, 
    strain_field_path: Optional[str] = None,
    enable_two_stage: bool = True,
    enable_impact: bool = True,
    enable_visualization: bool = True,
    output_dir: str = './material_results',
    loading_type: str = 'tensile',
    strain_rate: float = 1e-3,
    temperature: float = 300.0,
    verbose: bool = False,
    save_intermediate: bool = True,  # ä¸­é–“çµæœä¿å­˜ãƒ•ãƒ©ã‚°
    subdivide_defects: bool = False,
    subdivision_size: int = 100,
    sort_by_position: bool = False,
    **kwargs
) -> Dict:
    """
    ææ–™è§£æã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
    
    å…¨ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒç¢ºå®Ÿã«ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«æ¸¡ã•ã‚Œã‚‹
    """
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info(f"ğŸ’ MATERIAL ANALYSIS PIPELINE v3.2 (REFACTORED)")
    logger.info(f"   Material: {material_type}")
    logger.info(f"   Auto strain field generation: ENABLED")
    logger.info("="*70)
    
    # ========================================
    # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    # ========================================
    logger.info("\nğŸ“ Loading material data...")
    
    try:
        # ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªèª­ã¿è¾¼ã¿
        trajectory = np.load(trajectory_path)
        logger.info(f"   Trajectory shape: {trajectory.shape}")
        n_frames, n_atoms, n_dims = trajectory.shape
        
        if n_dims != 3:
            raise ValueError(f"Trajectory must be 3D, got {n_dims}D")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if Path(metadata_path).exists():
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
        else:
            metadata = {}
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        metadata.update({
            'system_name': f'{material_type}_simulation',
            'material_type': material_type,
            'temperature': temperature,
            'strain_rate': strain_rate,
            'loading_type': loading_type,
            'n_frames': n_frames,
            'n_atoms': n_atoms
        })
        
        # åŸå­ã‚¿ã‚¤ãƒ—èª­ã¿è¾¼ã¿
        atom_types = np.load(atom_types_path)
        logger.info(f"   Atom types loaded: {len(np.unique(atom_types))} types")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å®šç¾©èª­ã¿è¾¼ã¿ï¼ˆå¿…é ˆåŒ–ï¼ï¼‰
        if cluster_definition_path and Path(cluster_definition_path).exists():
            if cluster_definition_path.endswith('.json'):
                with open(cluster_definition_path, 'r') as f:
                    cluster_atoms_raw = json.load(f)
                    
                    # ===== ã“ã“ã‚’ä¿®æ­£ï¼ =====
                    cluster_atoms = {}
                    for k, v in cluster_atoms_raw.items():
                        cid = int(k)
                        # vãŒè¾æ›¸å½¢å¼ã®å ´åˆã€atom_idsã ã‘å–ã‚Šå‡ºã™
                        if isinstance(v, dict) and 'atom_ids' in v:
                            # å„åŸå­IDã‚’æ•´æ•°ã«å¤‰æ›ã—ã¦ãƒªã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
                            cluster_atoms[cid] = [int(atom_id) for atom_id in v['atom_ids']]
                        elif isinstance(v, list):
                            # æ—¢ã«ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã‚‚æ•´æ•°å¤‰æ›
                            cluster_atoms[cid] = [int(atom_id) for atom_id in v]
                        else:
                            logger.warning(f"Unknown format for cluster {cid}: {type(v)}")
                            cluster_atoms[cid] = []
                            
            else:
                cluster_atoms = np.load(cluster_definition_path, allow_pickle=True).item()
            
            logger.info(f"   Clusters defined: {len(cluster_atoms)}")
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›è¿½åŠ 
            for cid in cluster_atoms:
                logger.info(f"     Cluster {cid}: {len(cluster_atoms[cid])} atoms")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼0ãŒå¥å…¨é ˜åŸŸã¨ã—ã¦å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            if 0 not in cluster_atoms:
                logger.warning("   âš ï¸ Cluster 0 (healthy region) not found in definition")
        else:
            # KMeanså‰Šé™¤ï¼ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹
            logger.error("="*60)
            logger.error("âŒ CLUSTER DEFINITION FILE IS REQUIRED")
            logger.error("="*60)
            logger.error("   Material analysis requires physical defect regions.")
            logger.error("   Please provide: --clusters path/to/clusters_defects.json")
            logger.error("")
            logger.error("   The cluster file should define:")
            logger.error("     - Cluster 0: Healthy/perfect crystal region")
            logger.error("     - Cluster 1+: Various defect regions")
            logger.error("")
            logger.error("   Use external defect detection tools to generate this file.")
            logger.error("="*60)
            
            raise ValueError(
                "Cluster definition file is required for material analysis. "
                "KMeans spatial clustering is not physically meaningful for defect analysis."
            )
        
        # æ­ªã¿å ´èª­ã¿è¾¼ã¿ï¼ˆtryãƒ–ãƒ­ãƒƒã‚¯å†…ã«è¿½åŠ ï¼ï¼‰
        strain_field = None
        if strain_field_path and Path(strain_field_path).exists():
            strain_field = np.load(strain_field_path)
            logger.info(f"   Strain field loaded: shape {strain_field.shape}")
        else:
            logger.info("   Strain field will be auto-generated from trajectory")
        
        logger.info("   âœ… Data validation passed")
        
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise

    # ========================================
    # ğŸ†• æ¬ æã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è‡ªå‹•ç´°åˆ†åŒ–
    # ========================================
    if subdivide_defects:  # æ–°ã—ã„ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        logger.info("\nğŸ”¬ Subdividing defect clusters for network analysis...")
        
        original_cluster_count = len(cluster_atoms)
        new_cluster_atoms = {}
        subdivision_size = 100  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚‚å¯èƒ½
        
        for cid, atoms in cluster_atoms.items():
            if cid == 0:  # å¥å…¨é ˜åŸŸã¯ãã®ã¾ã¾
                new_cluster_atoms[0] = atoms
                logger.info(f"   Cluster 0 (healthy): {len(atoms)} atoms [kept as-is]")
                
            else:  # æ¬ æé ˜åŸŸã‚’ç´°åˆ†åŒ–
                n_atoms = len(atoms)
                n_subdivisions = (n_atoms + subdivision_size - 1) // subdivision_size
                
                if n_subdivisions == 1:
                    # 100å€‹ä»¥ä¸‹ãªã‚‰åˆ†å‰²ã—ãªã„
                    new_cluster_atoms[cid] = atoms
                    logger.info(f"   Cluster {cid}: {n_atoms} atoms [too small, kept as-is]")
                else:
                    # ç´°åˆ†åŒ–å®Ÿè¡Œ
                    atoms_array = np.array(atoms)
                    
                    # ç©ºé–“çš„ã«è¿‘ã„åŸå­ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹ãŸã‚ã€åº§æ¨™ã§ã‚½ãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if sort_by_position:
                        first_frame_pos = trajectory[0, atoms]  # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®åº§æ¨™
                        # Zè»¸ã§ã‚½ãƒ¼ãƒˆï¼ˆã¾ãŸã¯è·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰
                        sort_indices = np.argsort(first_frame_pos[:, 2])
                        atoms_array = atoms_array[sort_indices]
                    
                    for sub_id in range(n_subdivisions):
                        start_idx = sub_id * subdivision_size
                        end_idx = min(start_idx + subdivision_size, n_atoms)
                        
                        # éšå±¤çš„ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IDï¼ˆä¾‹: "1-001", "1-002"ï¼‰
                        new_cid = f"{cid}-{sub_id+1:03d}"
                        new_cluster_atoms[new_cid] = atoms_array[start_idx:end_idx].tolist()
                    
                    logger.info(f"   Cluster {cid}: {n_atoms} atoms â†’ {n_subdivisions} subclusters")
        
        cluster_atoms = new_cluster_atoms
        logger.info(f"\n   ğŸ“Š Cluster subdivision complete:")
        logger.info(f"      Original clusters: {original_cluster_count}")
        logger.info(f"      New clusters: {len(cluster_atoms)}")
        logger.info(f"      Network nodes: {len(cluster_atoms) - 1} (excluding healthy region)")
        
    # ========================================
    # Step 2: ãƒã‚¯ãƒ­ææ–™è§£æï¼ˆå¼·åŒ–ç‰ˆï¼‰
    # ========================================
    logger.info(f"\nğŸ”¬ Running Macro Material Analysis ({material_type})...")
    
    try:
        # MaterialConfigè¨­å®š
        config = MaterialConfig()
        config.material_type = material_type
        config.use_material_analytics = True
        config.adaptive_window = True
        config.use_phase_space = True
        config.sensitivity = 1.5
        config.gpu_batch_size = 10000
        
        # ææ–™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        material_params = get_material_parameters(material_type)
        logger.info(f"   Material parameters loaded")
        
        # æ¤œå‡ºå™¨åˆæœŸåŒ–
        detector = MaterialLambda3DetectorGPU(config)

        # backbone_indicesæº–å‚™ï¼ˆä¿®æ­£ç‰ˆï¼‰
        backbone_indices = None
        
        # ã¾ãšæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        if backbone_indices_path and Path(backbone_indices_path).exists():
            backbone_indices = np.load(backbone_indices_path)
            # å‹ãƒã‚§ãƒƒã‚¯
            if backbone_indices.dtype != np.int32:
                backbone_indices = backbone_indices.astype(np.int32)
            logger.info(f"   âœ… Loaded backbone_indices from file: {len(backbone_indices)} atoms")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯cluster_atomsã‹ã‚‰è‡ªå‹•ç”Ÿæˆ
        elif cluster_atoms:
            defect_atoms = []
            for cid, atoms in cluster_atoms.items():
                if str(cid) != "0":
                    # æ•´æ•°å¤‰æ›ã‚’ç¢ºå®Ÿã«
                    if isinstance(atoms, dict) and 'atom_ids' in atoms:
                        defect_atoms.extend([int(a) for a in atoms['atom_ids']])
                    else:
                        defect_atoms.extend([int(a) for a in atoms])
            if defect_atoms:
                backbone_indices = np.array(sorted(set(defect_atoms)), dtype=np.int32)
                logger.info(f"   ğŸ”„ Auto-generated backbone_indices: {len(backbone_indices)} atoms from clusters")
        else:
            logger.warning("   âš ï¸ No backbone_indices or cluster_atoms available")
        
        # è§£æå®Ÿè¡Œ
        macro_result = detector.analyze(
            trajectory=trajectory,
            backbone_indices=backbone_indices,
            atom_types=atom_types,
            cluster_atoms=cluster_atoms
        )
        
        logger.info(f"   âœ… Macro analysis complete")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºã¨åˆ†é¡
        material_events = []
        if hasattr(macro_result, 'material_events') and macro_result.material_events:
            material_events = macro_result.material_events
        elif hasattr(macro_result, 'critical_events'):
            # critical_eventsã‹ã‚‰å¤‰æ›
            anomaly_scores = None
            if hasattr(macro_result, 'anomaly_scores'):
                anomaly_scores = macro_result.anomaly_scores.get('combined')
            
            for event in macro_result.critical_events:
                if isinstance(event, tuple) and len(event) >= 2:
                    start, end = event[0], event[1]
                    event_type = classify_material_event(
                        start, end, anomaly_scores, n_frames
                    )
                    material_events.append((start, end, event_type))
        
        logger.info(f"   Material events: {len(material_events)}")
        
        # ========================================
        # strain_fieldã®è‡ªå‹•ç”Ÿæˆï¼ˆå¿…è¦ãªå ´åˆï¼‰
        # ========================================
        if strain_field is None and len(material_events) > 0:
            logger.info("   Generating strain field from trajectory...")
            
            # ææ–™ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ ¼å­å®šæ•°
            lattice_constants = {
                'SUJ2': 2.87,      # BCCé‰„
                'AL7075': 4.05,    # FCC ã‚¢ãƒ«ãƒŸãƒ‹ã‚¦ãƒ 
                'Ti6Al4V': 2.95,   # HCP ãƒã‚¿ãƒ³ï¼ˆaè»¸ï¼‰
                'SS316L': 3.58     # FCC ã‚¹ãƒ†ãƒ³ãƒ¬ã‚¹é‹¼
            }
            lattice = lattice_constants.get(material_type, 2.87)
            
            # æ­ªã¿å ´è¨ˆç®—
            strain_field = compute_strain_field_from_trajectory(
                trajectory=trajectory,
                material_events=material_events,
                lattice_constant=lattice
            )
            
            # çµ±è¨ˆæƒ…å ±
            logger.info(f"   Generated strain field:")
            logger.info(f"     - Mean strain: {np.mean(strain_field):.4f}")
            logger.info(f"     - Max strain: {np.max(strain_field):.4f}")
            logger.info(f"     - Atoms > 1% strain: {np.sum(strain_field > 0.01)}")
            logger.info(f"     - Atoms > 5% strain: {np.sum(strain_field > 0.05)}")
            
            # ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if save_intermediate:
                np.save(output_path / 'strain_field_auto.npy', strain_field)
                logger.info("   Saved: strain_field_auto.npy")
        
        # ========================================
        # é‡è¦: macro_resultã®å¼·åŒ–
        # ========================================
        macro_result = enhance_macro_result(
            macro_result=macro_result,
            material_events=material_events,
            trajectory_shape=trajectory.shape,
            material_params=material_params,
            metadata=metadata
        )
        logger.info("   âœ… Macro result enhanced with all required attributes")
        
        # ä¸­é–“ä¿å­˜
        if save_intermediate:
            with open(output_path / 'macro_result.pkl', 'wb') as f:
                pickle.dump(macro_result, f)
            logger.info("   Saved: macro_result.pkl")
        
    except Exception as e:
        logger.error(f"Macro analysis failed: {e}")
        raise

    # ========================================
    # Step 3: 2æ®µéšã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è§£æï¼ˆå¼·åŒ–ç‰ˆï¼‰
    # ========================================
    two_stage_result = None
    sorted_events = []
    
    if enable_two_stage and len(material_events) > 0:
        logger.info("\nğŸ”¬ Running Two-Stage Cluster Analysis...")
        
        try:
            # ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚¹ã‚³ã‚¢ä»˜ã‘ã¨ã‚½ãƒ¼ãƒˆ
            event_scores = []
            for event in material_events:
                if isinstance(event, tuple) and len(event) >= 3:
                    start, end, event_type = event[:3]
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    type_scores = {
                        'crack_initiation': 10.0,
                        'crack_propagation': 9.0,
                        'plastic_deformation': 7.0,
                        'dislocation_nucleation': 6.0,
                        'dislocation_avalanche': 6.5,
                        'phase_transition': 8.0,
                        'elastic_deformation': 3.0,
                        'uniform_deformation': 3.5,
                        'defect_migration': 5.5,
                        'transition_state': 4.0,
                        'fatigue_damage': 5.0
                    }
                    base_score = type_scores.get(event_type, 1.0)
                    
                    # ç•°å¸¸ã‚¹ã‚³ã‚¢ã§é‡ã¿ä»˜ã‘
                    if macro_result.anomaly_scores and 'combined' in macro_result.anomaly_scores:
                        scores = macro_result.anomaly_scores['combined']
                        if len(scores) > start:
                            score_range = scores[start:min(end+1, len(scores))]
                            if len(score_range) > 0:
                                base_score *= (1 + np.max(score_range) * 0.5)
                    
                    event_scores.append((start, end, base_score, event_type))
            
            # ã‚½ãƒ¼ãƒˆ
            sorted_events = sorted(event_scores, key=lambda x: x[2], reverse=True)
            
            # TOP50é¸æŠ
            selected_events = sorted_events[:min(50, len(sorted_events))]
            logger.info(f"   Selected TOP {len(selected_events)} events for analysis")
            
            # Two-Stageç”¨ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆä½œæˆ
            detected_events = []
            for i, (start, end, score, event_type) in enumerate(selected_events):
                event_name = f"{event_type}_{i:02d}_score_{score:.2f}"
                detected_events.append((start, end, event_name))
            
            # ClusterAnalysisConfigè¨­å®š
            cluster_config = ClusterAnalysisConfig()
            cluster_config.detect_dislocations = True
            cluster_config.detect_cracks = True
            cluster_config.detect_phase_transitions = True
            cluster_config.use_confidence = True
            cluster_config.use_physics_prediction = True  # ç‰©ç†äºˆæ¸¬ã‚’æœ‰åŠ¹åŒ–
            
            # TwoStageAnalyzeråˆæœŸåŒ–ã¨å®Ÿè¡Œ
            two_stage_analyzer = MaterialTwoStageAnalyzerGPU(
                config=cluster_config,
                material_type=material_type
            )
            
            two_stage_result = two_stage_analyzer.analyze_trajectory(
                trajectory=trajectory,
                macro_result=macro_result,
                detected_events=detected_events,
                cluster_atoms=cluster_atoms,
                atom_types=atom_types,
                stress_history=macro_result.stress_strain.get('stress') if macro_result.stress_strain else None
            )
            
            logger.info(f"   âœ… Two-stage analysis complete")
            
            # çµæœã®æ¤œè¨¼ã¨è¡¨ç¤º
            if hasattr(two_stage_result, 'material_state'):
                state = two_stage_result.material_state
                logger.info(f"   Material state: {state.get('state', 'unknown')}")
                logger.info(f"   Health index: {state.get('health_index', 1.0):.1%}")
            
            if hasattr(two_stage_result, 'critical_clusters'):
                n_critical = len(two_stage_result.critical_clusters)
                logger.info(f"   Critical clusters: {n_critical}")
            
            # ä¸­é–“ä¿å­˜
            if save_intermediate:
                with open(output_path / 'two_stage_result.pkl', 'wb') as f:
                    pickle.dump(two_stage_result, f)
                logger.info("   Saved: two_stage_result.pkl")
            
        except Exception as e:
            logger.error(f"Two-stage analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            two_stage_result = None
    
    # ========================================
    # Step 4: åŸå­ãƒ¬ãƒ™ãƒ«æ¬ é™¥è§£æï¼ˆå¼·åŒ–ç‰ˆï¼‰
    # ========================================
    impact_results = None
    
    if enable_impact and two_stage_result is not None:
        logger.info("\nâš›ï¸ Running Atomic-Level Defect Analysis...")
        
        try:
            # MaterialImpactAnalyzeråˆæœŸåŒ–
            impact_analyzer = MaterialImpactAnalyzer(
                cluster_mapping=cluster_atoms,
                sigma_threshold=2.5,
                use_network_analysis=True,
                use_gpu=True,
                material_type=material_type
            )
            
            # TOP Nã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è§£æ
            top_n = min(10, len(two_stage_result.critical_clusters)) if hasattr(two_stage_result, 'critical_clusters') else 5
            
            impact_results = impact_analyzer.analyze_critical_clusters(
                macro_result=macro_result,
                two_stage_result=two_stage_result,
                trajectory=trajectory,
                atom_types=atom_types,
                strain_field=strain_field,
                top_n=top_n
            )
            
            if impact_results:
                # çµ±è¨ˆè¨ˆç®—
                total_defects = sum(
                    getattr(r, 'n_defect_atoms', 0) 
                    for r in impact_results.values()
                )
                total_links = sum(
                    getattr(r, 'n_network_links', 0)
                    for r in impact_results.values()
                )
                
                logger.info(f"   âœ… Defect analysis complete")
                logger.info(f"   Events analyzed: {len(impact_results)}")
                logger.info(f"   Total defect atoms: {total_defects}")
                logger.info(f"   Network links: {total_links}")
                
                # æ¬ é™¥ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
                defect_types = Counter()
                for result in impact_results.values():
                    if hasattr(result, 'dominant_defect') and result.dominant_defect:
                        defect_types[result.dominant_defect] += 1
                
                if defect_types:
                    logger.info("   Defect types:")
                    for dtype, count in defect_types.most_common():
                        logger.info(f"     - {dtype}: {count}")
                
                # æœ€å¤§å¿œåŠ›é›†ä¸­
                max_stress = max(
                    (getattr(r, 'max_stress_concentration', 0) 
                     for r in impact_results.values()),
                    default=0
                )
                if max_stress > 0:
                    logger.info(f"   Max stress concentration: {max_stress:.2f} GPa")
                
                # ä¸­é–“ä¿å­˜
                if save_intermediate:
                    with open(output_path / 'impact_results.pkl', 'wb') as f:
                        pickle.dump(impact_results, f)
                    logger.info("   Saved: impact_results.pkl")
            else:
                logger.warning("   No impact results generated")
            
        except Exception as e:
            logger.error(f"Impact analysis failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            impact_results = None
    
    # ========================================
    # Step 5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå®Œå…¨ç‰ˆï¼‰
    # ========================================
    logger.info("\nğŸ“ Generating comprehensive material report...")
    
    try:
        # sorted_eventsã‚’3è¦ç´ ã‚¿ãƒ—ãƒ«ã«å¤‰æ›ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
        sorted_events_for_report = []
        for item in sorted_events:
            if len(item) >= 3:
                start, end, score = item[0], item[1], item[2]
                sorted_events_for_report.append((start, end, score))
        
        # ãƒ‡ãƒãƒƒã‚°: æ¸¡ã™ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        logger.info("   Report generation inputs:")
        logger.info(f"     - macro_result: {macro_result is not None}")
        if macro_result:
            logger.info(f"       - material_events: {len(macro_result.material_events) if macro_result.material_events else 0}")
            logger.info(f"       - stress_strain: {macro_result.stress_strain is not None}")
            logger.info(f"       - anomaly_scores: {macro_result.anomaly_scores is not None}")
            logger.info(f"       - failure_prediction: {macro_result.failure_prediction is not None}")
        logger.info(f"     - two_stage_result: {two_stage_result is not None}")
        logger.info(f"     - impact_results: {len(impact_results) if impact_results else 0}")
        logger.info(f"     - sorted_events: {len(sorted_events_for_report)}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = generate_material_report_from_results(
            macro_result=macro_result,
            two_stage_result=two_stage_result,
            impact_results=impact_results,
            sorted_events=sorted_events_for_report,
            metadata=metadata,
            material_type=material_type,
            output_dir=str(output_path),
            verbose=verbose,
            debug=True  # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ON
        )
        
        logger.info(f"   âœ… Report generated successfully")
        logger.info(f"   Report length: {len(report):,} characters")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ç¢ºèª
        report_path = output_path / 'material_report.md'
        if report_path.exists():
            logger.info(f"   Report saved to: {report_path}")
        
        # çµ±åˆçµæœã®JSONä¿å­˜
        summary_data = {
            'material_type': material_type,
            'metadata': metadata,
            'n_frames': n_frames,
            'n_atoms': n_atoms,
            'n_events': len(material_events),
            'event_types': dict(Counter(e[2] for e in material_events if len(e) >= 3)),
            'analysis_complete': True,
            'report_generated': True,
            'report_length': len(report)
        }
        
        # è§£æçµæœã®è¿½åŠ 
        if macro_result:
            if macro_result.failure_prediction:
                summary_data['failure_prediction'] = {
                    'probability': macro_result.failure_prediction.get('failure_probability', 0),
                    'mode': macro_result.failure_prediction.get('failure_mode', 'unknown')
                }
            if macro_result.stress_strain:
                summary_data['max_stress'] = macro_result.stress_strain.get('max_stress', 0)
        
        if two_stage_result:
            if hasattr(two_stage_result, 'material_state'):
                summary_data['material_state'] = two_stage_result.material_state.get('state', 'unknown')
            if hasattr(two_stage_result, 'critical_clusters'):
                summary_data['n_critical_clusters'] = len(two_stage_result.critical_clusters)
        
        if impact_results:
            summary_data['n_defect_analyses'] = len(impact_results)
        
        with open(output_path / 'analysis_summary.json', 'w') as f:
            json.dump(summary_data, f, indent=2)
        logger.info("   Saved: analysis_summary.json")
        
    except Exception as e:
        logger.error(f"Report generation failed: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        report = None
    
    # ========================================
    # Step 6: å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # ========================================
    if enable_visualization:
        logger.info("\nğŸ“Š Creating visualizations...")
        
        try:
            # å„ç¨®å¯è¦–åŒ–é–¢æ•°ã®å‘¼ã³å‡ºã—
            # (æ—¢å­˜ã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã¯ãã®ã¾ã¾ä½¿ç”¨)
            
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
            if material_events:
                fig = visualize_event_timeline(
                    material_events,
                    save_path=str(output_path / 'event_timeline.png')
                )
                logger.info("   Event timeline visualized")
            
            # å¿œåŠ›-æ­ªã¿æ›²ç·š
            if macro_result and macro_result.stress_strain:
                fig = visualize_stress_strain(
                    macro_result.stress_strain,
                    material_type,
                    save_path=str(output_path / 'stress_strain.png')
                )
                logger.info("   Stress-strain curve visualized")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ€ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—
            if two_stage_result:
                fig = visualize_cluster_damage(
                    two_stage_result,
                    save_path=str(output_path / 'cluster_damage.png')
                )
                logger.info("   Cluster damage map visualized")
            
            # æ¬ é™¥ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            if impact_results:
                fig = visualize_defect_network(
                    impact_results,
                    save_path=str(output_path / 'defect_network.png')
                )
                logger.info("   Defect network visualized")
            
            logger.info("   âœ… All visualizations completed")
            
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
    
    # ========================================
    # å®Œäº†
    # ========================================
    logger.info("\n" + "="*70)
    logger.info("âœ… MATERIAL ANALYSIS PIPELINE COMPLETE!")
    logger.info(f"   Material: {material_type}")
    logger.info(f"   Output directory: {output_path}")
    logger.info("   Summary:")
    
    if macro_result:
        logger.info(f"     âœ“ {len(material_events)} material events detected")
        if macro_result.failure_prediction:
            fp = macro_result.failure_prediction.get('failure_probability', 0)
            logger.info(f"     âœ“ Failure probability: {fp:.1%}")
    
    if two_stage_result:
        if hasattr(two_stage_result, 'material_state'):
            state = two_stage_result.material_state.get('state', 'unknown')
            logger.info(f"     âœ“ Material state: {state}")
        if hasattr(two_stage_result, 'critical_clusters'):
            n_crit = len(two_stage_result.critical_clusters)
            logger.info(f"     âœ“ {n_crit} critical clusters identified")
    
    if impact_results:
        logger.info(f"     âœ“ {len(impact_results)} atomic defect analyses completed")
    
    if report:
        logger.info(f"     âœ“ Report generated ({len(report):,} characters)")
    
    logger.info("="*70)
    
    return {
        'macro_result': macro_result,
        'two_stage_result': two_stage_result,
        'impact_results': impact_results,
        'sorted_events': sorted_events_for_report,
        'material_events': material_events,
        'report': report,
        'output_dir': output_path,
        'material_type': material_type,
        'metadata': metadata,
        'success': True
    }

# ============================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆæ—¢å­˜ã®ã‚‚ã®ã‚’ç¶­æŒï¼‰
# ============================================

def compute_strain_field_from_trajectory(
    trajectory: np.ndarray,
    material_events: List[Tuple[int, int, str]],
    lattice_constant: float = 2.87  # BCCé‰„ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
) -> np.ndarray:
    """
    ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã‹ã‚‰æ­ªã¿å ´ã‚’ç°¡æ˜“è¨ˆç®—
    
    Parameters
    ----------
    trajectory : np.ndarray
        åŸå­ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒª (n_frames, n_atoms, 3)
    material_events : List[Tuple[int, int, str]]
        ææ–™ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆï¼ˆæ­ªã¿æ¨å®šã®å‚è€ƒï¼‰
    lattice_constant : float
        æ ¼å­å®šæ•°ï¼ˆÃ…ï¼‰- æ­£è¦åŒ–ç”¨
    
    Returns
    -------
    np.ndarray
        å„åŸå­ã®å¹³å‡æ­ªã¿å ´ (n_atoms,)
    """
    n_frames, n_atoms = trajectory.shape[:2]
    strain_field = np.zeros(n_atoms)
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ç´¯ç©æ­ªã¿è¨ˆç®—
    for i in range(1, n_frames):
        # å„åŸå­ã®ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¤‰ä½
        displacement = trajectory[i] - trajectory[i-1]
        displacement_norm = np.linalg.norm(displacement, axis=1)
        
        # æ ¼å­å®šæ•°ã§æ­£è¦åŒ–ã—ã¦æ­ªã¿ã«å¤‰æ›
        frame_strain = displacement_norm / lattice_constant
        
        # ç´¯ç©
        strain_field += frame_strain
    
    # å¹³å‡åŒ–
    strain_field = strain_field / (n_frames - 1) if n_frames > 1 else strain_field
    
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®è£œæ­£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if material_events:
        # å¡‘æ€§ãƒ»è»¢ä½ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚‹é ˜åŸŸã‚’å¼·èª¿
        event_mask = np.zeros(n_frames, dtype=bool)
        for start, end, event_type in material_events:
            if any(key in event_type for key in ['plastic', 'dislocation', 'crack']):
                event_mask[start:min(end+1, n_frames)] = True
        
        # ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿãƒ•ãƒ¬ãƒ¼ãƒ ã§ã®è¿½åŠ æ­ªã¿
        if np.any(event_mask):
            event_frames = np.where(event_mask)[0]
            for frame in event_frames:
                if frame > 0 and frame < n_frames:
                    event_displacement = np.linalg.norm(
                        trajectory[frame] - trajectory[frame-1], axis=1
                    )
                    # ã‚¤ãƒ™ãƒ³ãƒˆé ˜åŸŸã¯æ­ªã¿ã‚’å¢—å¹…
                    strain_field += event_displacement / lattice_constant * 0.5
    
    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆç‰©ç†çš„ã«å¦¥å½“ãªç¯„å›²ï¼‰
    strain_field = np.clip(strain_field, 0, 0.5)  # æœ€å¤§50%æ­ªã¿
    
    return strain_field

# å¯è¦–åŒ–é–¢æ•°ï¼ˆæ—¢å­˜ã®ã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
def visualize_stress_strain(curve_data: Dict, material_type: str,
                          save_path: Optional[str] = None) -> plt.Figure:
    """å¿œåŠ›-æ­ªã¿æ›²ç·šã®å¯è¦–åŒ–"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    if 'strain' in curve_data and 'stress' in curve_data:
        strain = curve_data['strain']
        stress = curve_data['stress']
        
        ax.plot(strain, stress, 'b-', linewidth=2, label=material_type)
        
        if 'yield_point' in curve_data:
            yield_idx = curve_data['yield_point']
            ax.plot(strain[yield_idx], stress[yield_idx], 'ro',
                   markersize=10, label='Yield Point')
        
        if 'fracture_point' in curve_data:
            frac_idx = curve_data['fracture_point']
            ax.plot(strain[frac_idx], stress[frac_idx], 'rx',
                   markersize=12, label='Fracture')
        
        ax.set_xlabel('Strain', fontsize=12)
        ax.set_ylabel('Stress (GPa)', fontsize=12)
        ax.set_title(f'Stress-Strain Curve - {material_type}', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

def visualize_event_timeline(events: List, save_path: Optional[str] = None) -> plt.Figure:
    """ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®å¯è¦–åŒ–"""
    fig, ax = plt.subplots(figsize=(14, 6))
    
    colors = {
        'elastic_deformation': 'green',
        'uniform_deformation': 'lightgreen',
        'plastic_deformation': 'yellow',
        'dislocation_nucleation': 'orange',
        'dislocation_avalanche': 'darkorange',
        'defect_migration': 'coral',
        'crack_initiation': 'red',
        'crack_propagation': 'darkred',
        'phase_transition': 'purple',
        'transition_state': 'mediumpurple',
        'fatigue_damage': 'brown'
    }
    
    y_positions = {}
    y_counter = 0
    
    for event in events:
        if isinstance(event, tuple) and len(event) >= 3:
            start, end, event_type = event[:3]
            
            if event_type not in y_positions:
                y_positions[event_type] = y_counter
                y_counter += 1
            
            y = y_positions[event_type]
            color = colors.get(event_type, 'gray')
            
            ax.barh(y, end - start, left=start, height=0.8,
                   color=color, alpha=0.7, edgecolor='black')
    
    ax.set_yticks(list(y_positions.values()))
    ax.set_yticklabels(list(y_positions.keys()))
    ax.set_xlabel('Frame', fontsize=12)
    ax.set_title('Material Event Timeline', fontsize=14)
    ax.grid(True, alpha=0.3, axis='x')
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

def visualize_cluster_damage(two_stage_result: Any,
                            save_path: Optional[str] = None) -> plt.Figure:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ€ãƒ¡ãƒ¼ã‚¸ãƒãƒƒãƒ—ã®å¯è¦–åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    ax1 = axes[0]
    if hasattr(two_stage_result, 'critical_clusters'):
        critical = two_stage_result.critical_clusters[:20]
        if critical:
            ax1.bar(range(len(critical)), [1]*len(critical), color='red', alpha=0.7)
            ax1.set_xticks(range(len(critical)))
            ax1.set_xticklabels([f"C{c}" for c in critical], rotation=45)
            ax1.set_ylabel('Criticality', fontsize=12)
            ax1.set_title('Critical Clusters', fontsize=12)
    
    ax2 = axes[1]
    if hasattr(two_stage_result, 'global_cluster_importance'):
        top_clusters = sorted(
            two_stage_result.global_cluster_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        if top_clusters:
            cluster_ids = [f"C{c[0]}" for c in top_clusters]
            scores = [c[1] for c in top_clusters]
            
            ax2.barh(cluster_ids, scores, color='steelblue', alpha=0.7)
            ax2.set_xlabel('Importance Score', fontsize=12)
            ax2.set_title('Cluster Importance Ranking', fontsize=12)
            ax2.invert_yaxis()
    
    plt.suptitle('Cluster Damage Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

def visualize_defect_network(impact_results: Dict,
                            save_path: Optional[str] = None) -> plt.Figure:
    """æ¬ é™¥ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯è¦–åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    ax1 = axes[0, 0]
    defect_types = Counter()
    for result in impact_results.values():
        if hasattr(result, 'dominant_defect') and result.dominant_defect:
            defect_types[result.dominant_defect] += 1
    
    if defect_types:
        ax1.pie(defect_types.values(), labels=defect_types.keys(),
               autopct='%1.1f%%', startangle=90)
        ax1.set_title('Defect Type Distribution')
    
    ax2 = axes[0, 1]
    stress_concs = []
    for r in impact_results.values():
        if hasattr(r, 'max_stress_concentration'):
            stress_concs.append(r.max_stress_concentration)
    
    if stress_concs:
        ax2.hist(stress_concs, bins=20, color='red', alpha=0.7, edgecolor='black')
        ax2.set_xlabel('Stress Concentration (GPa)')
        ax2.set_ylabel('Count')
        ax2.set_title('Stress Concentration Distribution')
        ax2.grid(True, alpha=0.3)
    
    ax3 = axes[1, 0]
    patterns = []
    for result in impact_results.values():
        if hasattr(result, 'defect_network') and result.defect_network:
            if hasattr(result.defect_network, 'network_pattern'):
                patterns.append(result.defect_network.network_pattern)
    
    if patterns:
        pattern_counts = Counter(patterns)
        ax3.bar(pattern_counts.keys(), pattern_counts.values(),
               color='green', alpha=0.7)
        ax3.set_xlabel('Network Pattern')
        ax3.set_ylabel('Count')
        ax3.set_title('Defect Network Patterns')
    
    ax4 = axes[1, 1]
    plastic_zones = []
    for r in impact_results.values():
        if hasattr(r, 'plastic_zone_size'):
            plastic_zones.append(r.plastic_zone_size)
    
    if plastic_zones:
        ax4.hist(plastic_zones, bins=20, color='orange', alpha=0.7, edgecolor='black')
        ax4.set_xlabel('Plastic Zone Size (Ã…)')
        ax4.set_ylabel('Count')
        ax4.set_title('Plastic Zone Distribution')
        ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Atomic Defect Network Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    return fig

# CLI Interfaceï¼ˆæ—¢å­˜ã®ã‚‚ã®ã‚’ç¶­æŒï¼‰
def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(
        description='Material Full Analysis Pipeline - LambdaÂ³ GPU (REFACTORED v3.1)'
    )
    
    parser.add_argument('trajectory', help='Path to trajectory file (.npy)')
    parser.add_argument('atom_types', help='Path to atom types file (.npy)')
    parser.add_argument('--material', '-m', default='SUJ2',
                       choices=['SUJ2', 'AL7075', 'Ti6Al4V', 'SS316L'],
                       help='Material type')
    parser.add_argument('--metadata', help='Path to metadata file (.json)')
    parser.add_argument('--clusters', required=True,  # â† å¿…é ˆã«ï¼
                   help='Path to cluster definition file (REQUIRED). '
                        'Must define Cluster 0 as healthy region and others as defects.')
    parser.add_argument('--backbone', '-b',
                       help='Path to backbone_indices file (.npy) - pre-computed defect atoms')
    parser.add_argument('--strain', help='Path to strain field data (.npy)')
    parser.add_argument('--loading', '-l', default='tensile',
                       choices=['tensile', 'compression', 'shear', 'fatigue'])
    parser.add_argument(
        '--subdivide-defects',
        action='store_true',
        help='Subdivide defect clusters for detailed network analysis'
    )
    parser.add_argument(
        '--subdivision-size',
        type=int,
        default=100,
        help='Number of atoms per subdivision (default: 100)'
    )
    parser.add_argument(
        '--sort-by-position',
        action='store_true',
        help='Sort atoms by spatial position before subdivision'
    )
    parser.add_argument('--strain-rate', type=float, default=1e-3)
    parser.add_argument('--temperature', '-T', type=float, default=300.0)
    parser.add_argument('--output', '-o', default='./material_results')
    parser.add_argument('--no-two-stage', action='store_true')
    parser.add_argument('--no-impact', action='store_true')
    parser.add_argument('--no-viz', action='store_true')
    parser.add_argument('--verbose', '-v', action='store_true')
    parser.add_argument('--save-intermediate', action='store_true',
                       help='Save intermediate results as pickle files')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    metadata_path = args.metadata if args.metadata else 'metadata_auto.json'
    
    try:
        results = run_material_analysis_pipeline(
            trajectory_path=args.trajectory,
            metadata_path=metadata_path,
            atom_types_path=args.atom_types,
            material_type=args.material,
            cluster_definition_path=args.clusters,
            backbone_indices_path=args.backbone, 
            strain_field_path=args.strain,
            enable_two_stage=not args.no_two_stage,
            enable_impact=not args.no_impact,
            enable_visualization=not args.no_viz,
            output_dir=args.output,
            loading_type=args.loading,
            strain_rate=args.strain_rate,
            temperature=args.temperature,
            verbose=args.verbose,
            save_intermediate=args.save_intermediate,
            subdivide_defects=args.subdivide_defects,
            subdivision_size=args.subdivision_size,
            sort_by_position=args.sort_by_position
        )
        
        if results and results.get('success'):
            print(f"\nâœ… Success! Results saved to: {results['output_dir']}")
            return 0
        else:
            print("\nâŒ Pipeline failed. Check logs for details.")
            return 1
            
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == '__main__':
    exit(main())
