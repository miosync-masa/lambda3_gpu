#!/usr/bin/env python3
"""
CUDA Kernels for Material LambdaÂ³ Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ææ–™è§£æç”¨ã®é«˜é€ŸCUDAã‚«ãƒ¼ãƒãƒ«é›†
æ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«ã€æå‚·åº¦ã€é…ä½æ•°ã‚’å…¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒæ™‚è¨ˆç®—ï¼ğŸ’

RTX 4070 Ti SUPERã®åŠ›ã‚’100%å¼•ãå‡ºã™æœ¬æ°—å®Ÿè£…

by ç’°ã¡ã‚ƒã‚“ - CUDA Edition v1.0
"""

import numpy as np
import logging

logger = logging.getLogger('lambda3_gpu.material_analysis.cuda')

try:
    import cupy as cp
    HAS_CUDA = True
except ImportError:
    cp = None
    HAS_CUDA = False

# ===============================
# CUDA Kernel Codes
# ===============================

STRAIN_TENSOR_KERNEL_CODE = r'''
extern "C" __global__
void compute_strain_tensor_kernel(
    const float* __restrict__ ref_positions,    // (n_atoms, 3)
    const float* __restrict__ curr_positions,   // (n_atoms, 3)
    const int* __restrict__ cluster_atoms,      // (n_clusters, max_atoms_per_cluster)
    const int* __restrict__ cluster_sizes,      // (n_clusters,)
    float* __restrict__ strain_tensor,          // (n_clusters, 6) Voigtè¨˜æ³•
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size < 4) return;
    
    // å…±æœ‰ãƒ¡ãƒ¢ãƒªã§é«˜é€ŸåŒ–
    __shared__ float ref_centroid[3];
    __shared__ float curr_centroid[3];
    __shared__ float covariance[9];  // 3x3è¡Œåˆ—
    __shared__ float ref_matrix[9];  // ref^T * ref
    
    // åˆæœŸåŒ–
    if (tid < 3) {
        ref_centroid[tid] = 0.0f;
        curr_centroid[tid] = 0.0f;
    }
    if (tid < 9) {
        covariance[tid] = 0.0f;
        ref_matrix[tid] = 0.0f;
    }
    __syncthreads();
    
    // é‡å¿ƒè¨ˆç®—ï¼ˆä¸¦åˆ—ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
    int atoms_per_thread = (cluster_size + blockDim.x - 1) / blockDim.x;
    int start_atom = tid * atoms_per_thread;
    int end_atom = min(start_atom + atoms_per_thread, cluster_size);
    
    float local_ref[3] = {0.0f, 0.0f, 0.0f};
    float local_curr[3] = {0.0f, 0.0f, 0.0f};
    
    for (int i = start_atom; i < end_atom; i++) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + i];
        if (atom_idx < 0 || atom_idx >= n_atoms) continue;
        
        for (int d = 0; d < 3; d++) {
            local_ref[d] += ref_positions[atom_idx * 3 + d];
            local_curr[d] += curr_positions[atom_idx * 3 + d];
        }
    }
    
    // å…±æœ‰ãƒ¡ãƒ¢ãƒªã«é›†ç´„
    for (int d = 0; d < 3; d++) {
        atomicAdd(&ref_centroid[d], local_ref[d]);
        atomicAdd(&curr_centroid[d], local_curr[d]);
    }
    __syncthreads();
    
    if (tid < 3) {
        ref_centroid[tid] /= cluster_size;
        curr_centroid[tid] /= cluster_size;
    }
    __syncthreads();
    
    // å¤‰å½¢å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«Fè¨ˆç®—
    for (int i = start_atom; i < end_atom; i++) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + i];
        if (atom_idx < 0 || atom_idx >= n_atoms) continue;
        
        float ref[3], curr[3];
        for (int d = 0; d < 3; d++) {
            ref[d] = ref_positions[atom_idx * 3 + d] - ref_centroid[d];
            curr[d] = curr_positions[atom_idx * 3 + d] - curr_centroid[d];
        }
        
        // F = curr * ref^T ã®è¦ç´ ã‚’è¨ˆç®—
        for (int i = 0; i < 3; i++) {
            for (int j = 0; j < 3; j++) {
                atomicAdd(&covariance[i * 3 + j], curr[i] * ref[j]);
                atomicAdd(&ref_matrix[j * 3 + i], ref[j] * ref[i]);
            }
        }
    }
    __syncthreads();
    
    // Green-Lagrangeæ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«è¨ˆç®—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰0ãŒä»£è¡¨ï¼‰
    if (tid == 0) {
        // ç°¡æ˜“é€†è¡Œåˆ—ï¼ˆ3x3ã®å ´åˆã¯è§£æçš„ã«è¨ˆç®—å¯èƒ½ï¼‰
        float det = ref_matrix[0] * (ref_matrix[4] * ref_matrix[8] - ref_matrix[5] * ref_matrix[7])
                  - ref_matrix[1] * (ref_matrix[3] * ref_matrix[8] - ref_matrix[5] * ref_matrix[6])
                  + ref_matrix[2] * (ref_matrix[3] * ref_matrix[7] - ref_matrix[4] * ref_matrix[6]);
        
        if (fabsf(det) < 1e-10f) {
            // ç‰¹ç•°è¡Œåˆ—ã®å ´åˆã¯å˜ä½è¡Œåˆ—ã¨ã—ã¦æ‰±ã†
            for (int i = 0; i < 6; i++) {
                strain_tensor[cluster_id * 6 + i] = 0.0f;
            }
            return;
        }
        
        // F = covariance * (ref_matrix)^-1
        float F[9];
        float inv_det = 1.0f / det;
        
        // é€†è¡Œåˆ—è¨ˆç®—ã¨å¤‰å½¢å‹¾é…ã®åŒæ™‚è¨ˆç®—ï¼ˆçœç•¥ç‰ˆï¼‰
        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯æ­£ç¢ºãªé€†è¡Œåˆ—è¨ˆç®—ãŒå¿…è¦
        for (int i = 0; i < 9; i++) {
            F[i] = covariance[i] * inv_det;
        }
        
        // Green-Lagrangeæ­ªã¿ E = 0.5 * (F^T * F - I)
        float E[6];  // Voigtè¨˜æ³•
        
        // æ­£è¦æ­ªã¿ï¼ˆå¯¾è§’æˆåˆ†ï¼‰
        E[0] = 0.5f * (F[0]*F[0] + F[3]*F[3] + F[6]*F[6] - 1.0f);  // E_xx
        E[1] = 0.5f * (F[1]*F[1] + F[4]*F[4] + F[7]*F[7] - 1.0f);  // E_yy
        E[2] = 0.5f * (F[2]*F[2] + F[5]*F[5] + F[8]*F[8] - 1.0f);  // E_zz
        
        // ã›ã‚“æ–­æ­ªã¿ï¼ˆéå¯¾è§’æˆåˆ†ï¼‰
        E[3] = F[0]*F[1] + F[3]*F[4] + F[6]*F[7];  // E_xy
        E[4] = F[0]*F[2] + F[3]*F[5] + F[6]*F[8];  // E_xz
        E[5] = F[1]*F[2] + F[4]*F[5] + F[7]*F[8];  // E_yz
        
        // å‡ºåŠ›
        for (int i = 0; i < 6; i++) {
            strain_tensor[cluster_id * 6 + i] = E[i];
        }
    }
}
'''

COORDINATION_NUMBER_KERNEL_CODE = r'''
extern "C" __global__
void compute_coordination_kernel(
    const float* __restrict__ positions,        // (n_atoms, 3)
    const int* __restrict__ cluster_atoms,      // (n_clusters, max_atoms)
    const int* __restrict__ cluster_sizes,
    float* __restrict__ coordination_numbers,   // (n_clusters,)
    const float cutoff_squared,
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size == 0) return;
    
    __shared__ float total_coordination;
    if (tid == 0) total_coordination = 0.0f;
    __syncthreads();
    
    // å„ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚µãƒ³ãƒ—ãƒ«åŸå­ã‚’å‡¦ç†
    int n_samples = min(cluster_size, 50);  // æœ€å¤§50åŸå­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    int atoms_per_thread = (n_samples + blockDim.x - 1) / blockDim.x;
    int start_sample = tid * atoms_per_thread;
    int end_sample = min(start_sample + atoms_per_thread, n_samples);
    
    float local_coord = 0.0f;
    
    for (int s = start_sample; s < end_sample; s++) {
        int atom_i = cluster_atoms[cluster_id * max_atoms_per_cluster + s];
        if (atom_i < 0 || atom_i >= n_atoms) continue;
        
        float xi = positions[atom_i * 3 + 0];
        float yi = positions[atom_i * 3 + 1];
        float zi = positions[atom_i * 3 + 2];
        
        int neighbors = 0;
        
        // å…¨åŸå­ã¨ã®è·é›¢ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é©åŒ–å¯èƒ½ï¼‰
        for (int j = 0; j < n_atoms; j++) {
            if (j == atom_i) continue;
            
            float dx = positions[j * 3 + 0] - xi;
            float dy = positions[j * 3 + 1] - yi;
            float dz = positions[j * 3 + 2] - zi;
            
            float dist_sq = dx*dx + dy*dy + dz*dz;
            if (dist_sq < cutoff_squared) {
                neighbors++;
            }
        }
        
        local_coord += neighbors;
    }
    
    // é›†ç´„
    atomicAdd(&total_coordination, local_coord);
    __syncthreads();
    
    // å¹³å‡ã‚’å‡ºåŠ›
    if (tid == 0) {
        coordination_numbers[cluster_id] = total_coordination / n_samples;
    }
}
'''

DAMAGE_SCORE_KERNEL_CODE = r'''
extern "C" __global__
void compute_damage_kernel(
    const float* __restrict__ ref_positions,
    const float* __restrict__ curr_positions,
    const int* __restrict__ cluster_atoms,
    const int* __restrict__ cluster_sizes,
    float* __restrict__ damage_scores,          // (n_clusters,)
    const int n_atoms,
    const int n_clusters,
    const int max_atoms_per_cluster
) {
    const int cluster_id = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (cluster_id >= n_clusters) return;
    
    const int cluster_size = cluster_sizes[cluster_id];
    if (cluster_size == 0) return;
    
    __shared__ float total_rmsd;
    if (tid == 0) total_rmsd = 0.0f;
    __syncthreads();
    
    // å„ã‚¹ãƒ¬ãƒƒãƒ‰ãŒè¤‡æ•°åŸå­ã®RMSDã‚’è¨ˆç®—
    int atoms_per_thread = (cluster_size + blockDim.x - 1) / blockDim.x;
    int start_atom = tid * atoms_per_thread;
    int end_atom = min(start_atom + atoms_per_thread, cluster_size);
    
    float local_rmsd = 0.0f;
    
    for (int i = start_atom; i < end_atom; i++) {
        int atom_idx = cluster_atoms[cluster_id * max_atoms_per_cluster + i];
        if (atom_idx < 0 || atom_idx >= n_atoms) continue;
        
        float dx = curr_positions[atom_idx * 3 + 0] - ref_positions[atom_idx * 3 + 0];
        float dy = curr_positions[atom_idx * 3 + 1] - ref_positions[atom_idx * 3 + 1];
        float dz = curr_positions[atom_idx * 3 + 2] - ref_positions[atom_idx * 3 + 2];
        
        local_rmsd += dx*dx + dy*dy + dz*dz;
    }
    
    // é›†ç´„
    atomicAdd(&total_rmsd, local_rmsd);
    __syncthreads();
    
    // æå‚·åº¦è¨ˆç®—ï¼ˆtanhæ­£è¦åŒ–ï¼‰
    if (tid == 0) {
        float rmsd = sqrtf(total_rmsd / cluster_size);
        damage_scores[cluster_id] = tanhf(rmsd / 2.0f);  // 0-1ã«æ­£è¦åŒ–
    }
}
'''

# ===============================
# Kernel Manager Class
# ===============================

class MaterialCUDAKernels:
    """ææ–™è§£æç”¨CUDAã‚«ãƒ¼ãƒãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.strain_kernel = None
        self.coord_kernel = None
        self.damage_kernel = None
        self.compiled = False
        
        if HAS_CUDA:
            self._compile_kernels()
    
    def _compile_kernels(self):
        """CUDAã‚«ãƒ¼ãƒãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        try:
            # æ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«ã‚«ãƒ¼ãƒãƒ«
            self.strain_kernel = cp.RawKernel(
                STRAIN_TENSOR_KERNEL_CODE,
                'compute_strain_tensor_kernel',
                options=('--use_fast_math', '-O3')
            )
            
            # é…ä½æ•°ã‚«ãƒ¼ãƒãƒ«
            self.coord_kernel = cp.RawKernel(
                COORDINATION_NUMBER_KERNEL_CODE,
                'compute_coordination_kernel',
                options=('--use_fast_math', '-O3')
            )
            
            # æå‚·åº¦ã‚«ãƒ¼ãƒãƒ«
            self.damage_kernel = cp.RawKernel(
                DAMAGE_SCORE_KERNEL_CODE,
                'compute_damage_kernel',
                options=('--use_fast_math', '-O3')
            )
            
            self.compiled = True
            logger.info("âœ… All CUDA kernels compiled successfully!")
            
        except Exception as e:
            logger.warning(f"âš ï¸ CUDA kernel compilation failed: {e}")
            logger.warning("   Falling back to standard GPU computation")
            self.compiled = False
    
    def compute_strain_tensors_cuda(self,
                                   ref_positions: cp.ndarray,
                                   curr_positions: cp.ndarray,
                                   cluster_atoms: Dict,
                                   n_frames: int) -> cp.ndarray:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§æ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«è¨ˆç®—
        
        Returns
        -------
        cp.ndarray
            (n_frames, n_clusters, 6) ã®æ­ªã¿ãƒ†ãƒ³ã‚½ãƒ«
        """
        if not self.compiled or self.strain_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = ref_positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        ref_pos_flat = ref_positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        strain_result = cp.zeros((n_clusters, 6), dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        threads_per_block = 256
        blocks = n_clusters
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        self.strain_kernel(
            (blocks,), (threads_per_block,),
            (ref_pos_flat, curr_positions.flatten().astype(cp.float32),
             cluster_array_gpu, cluster_sizes_gpu,
             strain_result, n_atoms, n_clusters, max_atoms)
        )
        
        return strain_result
    
    def compute_coordination_cuda(self,
                                 positions: cp.ndarray,
                                 cluster_atoms: Dict,
                                 cutoff: float) -> cp.ndarray:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§é…ä½æ•°è¨ˆç®—
        
        Returns
        -------
        cp.ndarray
            (n_clusters,) ã®é…ä½æ•°
        """
        if not self.compiled or self.coord_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        positions_flat = positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        coord_result = cp.zeros(n_clusters, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.coord_kernel(
            (blocks,), (threads_per_block,),
            (positions_flat, cluster_array_gpu, cluster_sizes_gpu,
             coord_result, cutoff * cutoff, n_atoms, n_clusters, max_atoms)
        )
        
        return coord_result
    
    def compute_damage_cuda(self,
                          ref_positions: cp.ndarray,
                          curr_positions: cp.ndarray,
                          cluster_atoms: Dict) -> cp.ndarray:
        """
        CUDAã‚«ãƒ¼ãƒãƒ«ã§æå‚·åº¦è¨ˆç®—
        
        Returns
        -------
        cp.ndarray
            (n_clusters,) ã®æå‚·åº¦
        """
        if not self.compiled or self.damage_kernel is None:
            raise RuntimeError("CUDA kernels not compiled")
        
        n_atoms = ref_positions.shape[0]
        n_clusters = len(cluster_atoms)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é…åˆ—ã®æº–å‚™
        max_atoms = max(len(atoms) for atoms in cluster_atoms.values())
        cluster_array = np.full((n_clusters, max_atoms), -1, dtype=np.int32)
        cluster_sizes = np.zeros(n_clusters, dtype=np.int32)
        
        for cid, atoms in cluster_atoms.items():
            cluster_array[cid, :len(atoms)] = atoms
            cluster_sizes[cid] = len(atoms)
        
        # GPUè»¢é€
        cluster_array_gpu = cp.asarray(cluster_array)
        cluster_sizes_gpu = cp.asarray(cluster_sizes)
        ref_pos_flat = ref_positions.flatten().astype(cp.float32)
        curr_pos_flat = curr_positions.flatten().astype(cp.float32)
        
        # çµæœé…åˆ—
        damage_result = cp.zeros(n_clusters, dtype=cp.float32)
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
        threads_per_block = 256
        blocks = n_clusters
        
        self.damage_kernel(
            (blocks,), (threads_per_block,),
            (ref_pos_flat, curr_pos_flat, cluster_array_gpu, cluster_sizes_gpu,
             damage_result, n_atoms, n_clusters, max_atoms)
        )
        
        return damage_result

# ===============================
# Export
# ===============================

__all__ = [
    'MaterialCUDAKernels',
    'STRAIN_TENSOR_KERNEL_CODE',
    'COORDINATION_NUMBER_KERNEL_CODE',
    'DAMAGE_SCORE_KERNEL_CODE'
]

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    print("ğŸ’ Material CUDA Kernels Module")
    print("   RTX 4070 Ti SUPER optimized!")
    
    if HAS_CUDA:
        kernels = MaterialCUDAKernels()
        if kernels.compiled:
            print("   âœ… All kernels ready!")
        else:
            print("   âš ï¸ Compilation failed")
    else:
        print("   âŒ CUDA not available")
